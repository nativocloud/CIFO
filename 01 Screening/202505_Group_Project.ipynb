{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccabf80",
   "metadata": {},
   "source": [
    "# Sports League Optimization: Comparative Analysis of Algorithms\n",
    "\n",
    "This notebook presents a comprehensive analysis of different optimization algorithms applied to the Sports League problem. We compare Hill Climbing, Simulated Annealing, and Genetic Algorithm approaches, analyzing their performance across multiple metrics.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Problem Definition](#1-problem-definition)\n",
    "2. [Experimental Setup](#2-experimental-setup)\n",
    "3. [Algorithm Implementations](#3-algorithm-implementations)\n",
    "4. [Performance Comparison](#4-performance-comparison)\n",
    "5. [Statistical Analysis](#5-statistical-analysis)\n",
    "6. [Conclusion](#6-conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy import stats\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Import our custom modules\n",
    "from solution import LeagueSolution, LeagueHillClimbingSolution, LeagueSASolution\n",
    "from evolution import (\n",
    "    hill_climbing, \n",
    "    simulated_annealing, \n",
    "    genetic_algorithm,\n",
    "    # Mutation operators\n",
    "    mutate_swap,\n",
    "    mutate_swap_constrained,\n",
    "    mutate_team_shift,\n",
    "    mutate_targeted_player_exchange,\n",
    "    mutate_shuffle_within_team_constrained,\n",
    "    # Crossover operators\n",
    "    crossover_one_point,\n",
    "    crossover_one_point_prefer_valid,\n",
    "    crossover_uniform,\n",
    "    crossover_uniform_prefer_valid,\n",
    "    # Selection operators\n",
    "    selection_tournament,\n",
    "    selection_tournament_variable_k,\n",
    "    selection_ranking,\n",
    "    selection_boltzmann\n",
    ")\n",
    "# Import the fitness counter module\n",
    "from fitness_counter import fitness_counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d7e46",
   "metadata": {},
   "source": [
    "## 1. Problem Definition\n",
    "\n",
    "### 1.1 Sports League Problem\n",
    "\n",
    "The Sports League problem involves assigning players to teams while satisfying specific constraints and optimizing for team balance. The goal is to create teams with similar average skill levels.\n",
    "\n",
    "**Formal Definition:**\n",
    "- We have 35 players with different positions (GK, DEF, MID, FWD) and skill levels\n",
    "- We need to assign these players to 5 teams (7 players per team)\n",
    "- Each team must have exactly 1 GK, 2 DEF, 2 MID, and 2 FWD\n",
    "- Each team's total salary must not exceed 750M €\n",
    "- The objective is to minimize the standard deviation of average team skills\n",
    "\n",
    "### 1.2 Solution Representation\n",
    "\n",
    "We represent a solution as a list of team assignments for each player. For example, if `solution.repr[0] = 2`, it means player 0 is assigned to team 2.\n",
    "\n",
    "**Search Space Size:**\n",
    "- For 35 players and 5 teams, the theoretical search space is 5^35\n",
    "- With constraints, the actual feasible search space is much smaller, but still extremely large\n",
    "\n",
    "### 1.3 Fitness Function\n",
    "\n",
    "The fitness function calculates the standard deviation of the average skill levels across all teams. A lower value indicates more balanced teams, which is our optimization goal.\n",
    "\n",
    "For invalid solutions (those violating constraints), we return infinity to ensure they are never selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load player data\n",
    "players_df = pd.read_csv(\"players.csv\", sep=\";\")\n",
    "# Rename the salary column to match the code expectations\n",
    "players_df = players_df.rename(columns={'Salary (€M)': 'Salary'})\n",
    "players_data = players_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Display the player data\n",
    "players_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5e515",
   "metadata": {},
   "source": [
    "### 1.4 Data Analysis\n",
    "\n",
    "Let's analyze the player data to understand the distribution of skills, positions, and salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c11eaf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Analyze player positions\n",
    "position_counts = players_df['Position'].value_counts()\n",
    "print(\"Position distribution:\")\n",
    "print(position_counts)\n",
    "\n",
    "# Analyze skill distribution by position\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Position', y='Skill', data=players_df)\n",
    "plt.title('Skill Distribution by Position')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Analyze salary distribution by position\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Position', y='Salary', data=players_df)\n",
    "plt.title('Salary Distribution by Position')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Correlation between skill and salary\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Skill', y='Salary', hue='Position', data=players_df)\n",
    "plt.title('Correlation between Skill and Salary')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fd779a",
   "metadata": {},
   "source": [
    "## 2. Experimental Setup\n",
    "\n",
    "### 2.1 Metrics for Comparison\n",
    "\n",
    "To ensure a fair comparison between different algorithms, we'll track the following metrics:\n",
    "\n",
    "1. **Solution Quality**: The fitness value (standard deviation of average team skills)\n",
    "2. **Function Evaluations**: Number of fitness function calls\n",
    "3. **Iterations**: Number of algorithm iterations\n",
    "4. **Runtime**: Actual execution time in seconds\n",
    "\n",
    "### 2.2 Algorithm Configurations\n",
    "\n",
    "We'll test the following algorithm configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b65784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate valid initial solutions using a robust heuristic approach\n",
    "def generate_valid_solution(solution_class, players_data, max_attempts=500000, verbose=False):\n",
    "    \"\"\"\n",
    "    Generate a valid initial solution for any solution class using a robust heuristic approach.\n",
    "    \n",
    "    Args:\n",
    "        solution_class: The solution class to instantiate\n",
    "        players_data: List of player dictionaries\n",
    "        max_attempts: Maximum number of attempts to generate a valid solution\n",
    "        verbose: Whether to print progress information\n",
    "        \n",
    "    Returns:\n",
    "        A valid solution instance or None if no valid solution could be found\n",
    "    \"\"\"\n",
    "    # Track attempts for diagnostics\n",
    "    random_attempts = 0\n",
    "    heuristic_attempts = 0\n",
    "    swap_attempts = 0\n",
    "    \n",
    "    # First try the standard random approach with increased attempts\n",
    "    for _ in range(5000):  # Try more random solutions first\n",
    "        random_attempts += 1\n",
    "        solution = solution_class(players=players_data)\n",
    "        if solution.is_valid():\n",
    "            if verbose:\n",
    "                print(f\"Found valid solution with random approach after {random_attempts} attempts\")\n",
    "            return solution\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Using heuristic construction approach for valid solution generation...\")\n",
    "    \n",
    "    # Try multiple heuristic construction attempts with different team orders\n",
    "    for heuristic_try in range(20):  # Try multiple heuristic constructions with different randomization\n",
    "        heuristic_attempts += 1\n",
    "        \n",
    "        # Group players by position\n",
    "        players_by_position = {}\n",
    "        for i, player in enumerate(players_data):\n",
    "            pos = player['Position']\n",
    "            if pos not in players_by_position:\n",
    "                players_by_position[pos] = []\n",
    "            players_by_position[pos].append((i, player))\n",
    "        \n",
    "        # Shuffle players within each position to increase diversity\n",
    "        for pos in players_by_position:\n",
    "            random.shuffle(players_by_position[pos])\n",
    "        \n",
    "        # Create a new solution instance\n",
    "        solution = solution_class(players=players_data)\n",
    "        \n",
    "        # Reset all assignments\n",
    "        for i in range(len(solution.repr)):\n",
    "            solution.repr[i] = -1  # -1 means unassigned\n",
    "        \n",
    "        # Number of teams\n",
    "        num_teams = 5\n",
    "        \n",
    "        # Create a randomized team order for assignments to increase diversity\n",
    "        team_order = list(range(num_teams))\n",
    "        random.shuffle(team_order)\n",
    "        \n",
    "        # Assign players to teams using a balanced approach with randomized team order\n",
    "        # First, assign goalkeepers (1 per team)\n",
    "        for team_idx, team_id in enumerate(team_order):\n",
    "            if team_idx < len(players_by_position.get('GK', [])):\n",
    "                player_idx = players_by_position['GK'][team_idx][0]\n",
    "                solution.repr[player_idx] = team_id\n",
    "        \n",
    "        # Assign defenders (2 per team)\n",
    "        for team_idx, team_id in enumerate(team_order):\n",
    "            defenders_assigned = 0\n",
    "            for player_tuple in players_by_position.get('DEF', []):\n",
    "                player_idx = player_tuple[0]\n",
    "                if solution.repr[player_idx] == -1:  # If player not assigned yet\n",
    "                    solution.repr[player_idx] = team_id\n",
    "                    defenders_assigned += 1\n",
    "                    if defenders_assigned == 2:  # 2 defenders per team\n",
    "                        break\n",
    "        \n",
    "        # Assign midfielders (2 per team)\n",
    "        for team_idx, team_id in enumerate(team_order):\n",
    "            midfielders_assigned = 0\n",
    "            for player_tuple in players_by_position.get('MID', []):\n",
    "                player_idx = player_tuple[0]\n",
    "                if solution.repr[player_idx] == -1:  # If player not assigned yet\n",
    "                    solution.repr[player_idx] = team_id\n",
    "                    midfielders_assigned += 1\n",
    "                    if midfielders_assigned == 2:  # 2 midfielders per team\n",
    "                        break\n",
    "        \n",
    "        # Assign forwards (2 per team)\n",
    "        for team_idx, team_id in enumerate(team_order):\n",
    "            forwards_assigned = 0\n",
    "            for player_tuple in players_by_position.get('FWD', []):\n",
    "                player_idx = player_tuple[0]\n",
    "                if solution.repr[player_idx] == -1:  # If player not assigned yet\n",
    "                    solution.repr[player_idx] = team_id\n",
    "                    forwards_assigned += 1\n",
    "                    if forwards_assigned == 2:  # 2 forwards per team\n",
    "                        break\n",
    "        \n",
    "        # Check if all players are assigned\n",
    "        if -1 in solution.repr:\n",
    "            # Assign remaining players to balance teams\n",
    "            for i in range(len(solution.repr)):\n",
    "                if solution.repr[i] == -1:\n",
    "                    # Find team with fewest players\n",
    "                    team_counts = [0] * num_teams\n",
    "                    for team_id in solution.repr:\n",
    "                        if team_id != -1:\n",
    "                            team_counts[team_id] += 1\n",
    "                    solution.repr[i] = team_counts.index(min(team_counts))\n",
    "        \n",
    "        # Check if solution is valid\n",
    "        if solution.is_valid():\n",
    "            if verbose:\n",
    "                print(f\"Found valid solution with heuristic approach after {heuristic_attempts} attempts\")\n",
    "            return solution\n",
    "        \n",
    "        # If heuristic approach fails, try to fix salary constraints\n",
    "        # Calculate team salaries\n",
    "        team_salaries = [0] * num_teams\n",
    "        team_positions = [{pos: 0 for pos in ['GK', 'DEF', 'MID', 'FWD']} for _ in range(num_teams)]\n",
    "        \n",
    "        for i, team_id in enumerate(solution.repr):\n",
    "            if team_id >= 0:  # Skip unassigned players\n",
    "                team_salaries[team_id] += players_data[i]['Salary']\n",
    "                team_positions[team_id][players_data[i]['Position']] += 1\n",
    "        \n",
    "        # Try to swap players to satisfy salary constraints\n",
    "        max_salary = 750  # Maximum salary per team\n",
    "        \n",
    "        # More aggressive swapping strategy with multiple passes\n",
    "        for swap_pass in range(3):  # Try multiple passes of swaps\n",
    "            made_improvement = False\n",
    "            \n",
    "            # Try to fix position constraints first\n",
    "            for team_id in range(num_teams):\n",
    "                for pos, count in team_positions[team_id].items():\n",
    "                    required = {'GK': 1, 'DEF': 2, 'MID': 2, 'FWD': 2}[pos]\n",
    "                    \n",
    "                    # If we have too many of this position\n",
    "                    while count > required:\n",
    "                        # Find another team that needs this position\n",
    "                        for other_team in range(num_teams):\n",
    "                            if other_team != team_id and team_positions[other_team][pos] < required:\n",
    "                                # Find a player of this position in our team\n",
    "                                for i, player_team in enumerate(solution.repr):\n",
    "                                    if player_team == team_id and players_data[i]['Position'] == pos:\n",
    "                                        # Swap this player to the other team\n",
    "                                        solution.repr[i] = other_team\n",
    "                                        team_positions[team_id][pos] -= 1\n",
    "                                        team_positions[other_team][pos] += 1\n",
    "                                        team_salaries[team_id] -= players_data[i]['Salary']\n",
    "                                        team_salaries[other_team] += players_data[i]['Salary']\n",
    "                                        count -= 1\n",
    "                                        made_improvement = True\n",
    "                                        break\n",
    "                                if count <= required:\n",
    "                                    break\n",
    "            \n",
    "            # Try to fix salary constraints\n",
    "            for attempt in range(1000):  # More attempts per pass\n",
    "                swap_attempts += 1\n",
    "                \n",
    "                # Find teams that exceed salary cap\n",
    "                over_budget_teams = [i for i, salary in enumerate(team_salaries) if salary > max_salary]\n",
    "                if not over_budget_teams:  # All teams within budget\n",
    "                    break\n",
    "                    \n",
    "                # Find teams under budget\n",
    "                under_budget_teams = [i for i, salary in enumerate(team_salaries) if salary <= max_salary]\n",
    "                if not under_budget_teams:  # All teams over budget\n",
    "                    break\n",
    "                    \n",
    "                # Select a team over budget and a team under budget\n",
    "                over_team = random.choice(over_budget_teams)\n",
    "                under_team = random.choice(under_budget_teams)\n",
    "                \n",
    "                # Find players in these teams\n",
    "                over_team_players = [(i, players_data[i]) for i in range(len(solution.repr)) if solution.repr[i] == over_team]\n",
    "                under_team_players = [(i, players_data[i]) for i in range(len(solution.repr)) if solution.repr[i] == under_team]\n",
    "                \n",
    "                # Sort players by salary (descending) to prioritize high-salary swaps\n",
    "                over_team_players.sort(key=lambda x: x[1]['Salary'], reverse=True)\n",
    "                \n",
    "                # Try to find a pair of players to swap that would improve salary balance\n",
    "                for over_idx, over_player in over_team_players:\n",
    "                    over_pos = over_player['Position']\n",
    "                    over_salary = over_player['Salary']\n",
    "                    \n",
    "                    # Filter under-team players by position\n",
    "                    matching_under_players = [(idx, p) for idx, p in under_team_players if p['Position'] == over_pos]\n",
    "                    \n",
    "                    for under_idx, under_player in matching_under_players:\n",
    "                        under_salary = under_player['Salary']\n",
    "                        \n",
    "                        # Calculate new salaries after swap\n",
    "                        new_over_salary = team_salaries[over_team] - over_salary + under_salary\n",
    "                        new_under_salary = team_salaries[under_team] - under_salary + over_salary\n",
    "                        \n",
    "                        # If swap improves situation, do it\n",
    "                        if new_over_salary <= max_salary or new_over_salary < team_salaries[over_team]:\n",
    "                            # Swap players\n",
    "                            solution.repr[over_idx] = under_team\n",
    "                            solution.repr[under_idx] = over_team\n",
    "                            \n",
    "                            # Update team salaries\n",
    "                            team_salaries[over_team] = new_over_salary\n",
    "                            team_salaries[under_team] = new_under_salary\n",
    "                            made_improvement = True\n",
    "                            break\n",
    "                    \n",
    "                    if made_improvement:\n",
    "                        break\n",
    "                \n",
    "                # Check if we've made a swap and solution is valid\n",
    "                if solution.is_valid():\n",
    "                    if verbose:\n",
    "                        print(f\"Found valid solution after {random_attempts} random attempts, \"\n",
    "                              f\"{heuristic_attempts} heuristic attempts, and {swap_attempts} swap attempts\")\n",
    "                    return solution\n",
    "                \n",
    "                # If no improvement was made in this pass, move to next pass\n",
    "                if not made_improvement:\n",
    "                    break\n",
    "    \n",
    "    # If we still don't have a valid solution, try one more random approach with remaining attempts\n",
    "    remaining_attempts = max_attempts - random_attempts - swap_attempts\n",
    "    for _ in range(remaining_attempts):\n",
    "        solution = solution_class(players=players_data)\n",
    "        if solution.is_valid():\n",
    "            if verbose:\n",
    "                print(f\"Found valid solution with final random approach after {random_attempts + _ + 1} total random attempts\")\n",
    "            return solution\n",
    "    \n",
    "    # If all attempts fail, return None\n",
    "    if verbose:\n",
    "        print(f\"Failed to find valid solution after {max_attempts} total attempts\")\n",
    "    return None\n",
    "\n",
    "# Define algorithm configurations\n",
    "configs = {\n",
    "    # Hill Climbing configurations\n",
    "    'HC_Standard': {\n",
    "        'algorithm': 'Hill Climbing',\n",
    "        'params': {\n",
    "            'max_iterations': 500,\n",
    "            'max_no_improvement': 100,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Hill Climbing with valid initial solution\n",
    "    'HC_Valid_Initial': {\n",
    "        'algorithm': 'Hill Climbing Valid',\n",
    "        'params': {\n",
    "            'max_iterations': 500,\n",
    "            'max_no_improvement': 100,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Simulated Annealing configurations\n",
    "    'SA_Standard': {\n",
    "        'algorithm': 'Simulated Annealing',\n",
    "        'params': {\n",
    "            'initial_temperature': 200.0,\n",
    "            'cooling_rate': 0.95,\n",
    "            'min_temperature': 1e-5,\n",
    "            'iterations_per_temp': 20,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Genetic Algorithm configurations\n",
    "    'GA_Tournament_OnePoint': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'params': {\n",
    "            'population_size': 100,\n",
    "            'max_generations': 50,\n",
    "            'selection_operator': selection_tournament,\n",
    "            'crossover_operator': crossover_one_point_prefer_valid,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_operator': mutate_swap_constrained,\n",
    "            'mutation_rate': 0.1,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 2,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'GA_Ranking_Uniform': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'params': {\n",
    "            'population_size': 100,\n",
    "            'max_generations': 50,\n",
    "            'selection_operator': selection_ranking,\n",
    "            'crossover_operator': crossover_uniform_prefer_valid,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_operator': mutate_swap_constrained,\n",
    "            'mutation_rate': 0.1,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 2,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'GA_Boltzmann_TeamShift': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'params': {\n",
    "            'population_size': 100,\n",
    "            'max_generations': 50,\n",
    "            'selection_operator': selection_boltzmann,\n",
    "            'selection_params': {'temperature': 1.0},\n",
    "            'crossover_operator': crossover_uniform_prefer_valid,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_operator': mutate_team_shift,\n",
    "            'mutation_rate': 0.1,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 2,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'GA_Hybrid': {\n",
    "        'algorithm': 'Hybrid GA',\n",
    "        'params': {\n",
    "            'population_size': 75,\n",
    "            'max_generations': 40,\n",
    "            'selection_operator': selection_tournament,\n",
    "            'selection_params': {'k': 3},\n",
    "            'crossover_operator': crossover_one_point_prefer_valid,\n",
    "            'crossover_rate': 0.85,\n",
    "            'mutation_operator': mutate_targeted_player_exchange,\n",
    "            'mutation_rate': 0.15,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 1,\n",
    "            'local_search': {'algorithm': 'hill_climbing', 'frequency': 5, 'iterations': 50},\n",
    "            'verbose': False\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525eda15",
   "metadata": {},
   "source": [
    "### 2.3 Experiment Setup\n",
    "\n",
    "We'll run each algorithm configuration multiple times to account for randomness and collect statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8da1a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Import solution classes at the top level to ensure they're available in all scopes\n",
    "from solution import LeagueHillClimbingSolution, LeagueSASolution\n",
    "\n",
    "# Function to run a single experiment\n",
    "def run_single_experiment(config_name, run_number, configs, players_data):\n",
    "    \"\"\"\n",
    "    Run a single experiment for a specific configuration and run number.\n",
    "    \n",
    "    Args:\n",
    "        config_name: Name of the configuration to run\n",
    "        run_number: Run number for this experiment\n",
    "        configs: Dictionary of all configurations\n",
    "        players_data: Player data for the experiment\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with experiment results or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config = configs[config_name]\n",
    "        algorithm_name = config['algorithm']\n",
    "        params = config['params'].copy()\n",
    "        \n",
    "        # Reset fitness counter for this experiment\n",
    "        from fitness_counter import fitness_counter\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize solution and run algorithm based on configuration\n",
    "        if algorithm_name == 'Hill Climbing':\n",
    "            # Standard Hill Climbing with potentially invalid initial solution\n",
    "            solution = LeagueHillClimbingSolution(players=players_data)\n",
    "            \n",
    "            # Start counting fitness evaluations\n",
    "            fitness_counter.start_counting(LeagueHillClimbingSolution)\n",
    "            \n",
    "            # Run Hill Climbing\n",
    "            best_solution, best_fitness, history = hill_climbing(solution, **params)\n",
    "            \n",
    "            # Stop counting fitness evaluations\n",
    "            evaluations = fitness_counter.stop_counting(LeagueHillClimbingSolution)\n",
    "            \n",
    "        elif algorithm_name == 'Hill Climbing Valid':\n",
    "            # Hill Climbing with guaranteed valid initial solution\n",
    "            solution = generate_valid_solution(LeagueHillClimbingSolution, players_data, verbose=False)\n",
    "            \n",
    "            # If we couldn't generate a valid solution, return error\n",
    "            if solution is None:\n",
    "                return {\n",
    "                    'Configuration': config_name,\n",
    "                    'Run': run_number,\n",
    "                    'Fitness': float('inf'),\n",
    "                    'Evaluations': 0,\n",
    "                    'Runtime': 0.0,\n",
    "                    'History': [],\n",
    "                    'Error': 'Could not generate valid initial solution'\n",
    "                }\n",
    "            \n",
    "            # Start counting fitness evaluations\n",
    "            fitness_counter.start_counting(LeagueHillClimbingSolution)\n",
    "            \n",
    "            # Run Hill Climbing\n",
    "            best_solution, best_fitness, history = hill_climbing(solution, **params)\n",
    "            \n",
    "            # Stop counting fitness evaluations\n",
    "            evaluations = fitness_counter.stop_counting(LeagueHillClimbingSolution)\n",
    "            \n",
    "        elif algorithm_name == 'Simulated Annealing':\n",
    "            solution = LeagueSASolution(players=players_data)\n",
    "            \n",
    "            # Start counting fitness evaluations\n",
    "            fitness_counter.start_counting(LeagueSASolution)\n",
    "            \n",
    "            # Run Simulated Annealing\n",
    "            best_solution, best_fitness, history = simulated_annealing(solution, **params)\n",
    "            \n",
    "            # Stop counting fitness evaluations\n",
    "            evaluations = fitness_counter.stop_counting(LeagueSASolution)\n",
    "            \n",
    "        elif algorithm_name == 'Genetic Algorithm':\n",
    "            # Start counting fitness evaluations\n",
    "            fitness_counter.start_counting(LeagueSolution)\n",
    "            \n",
    "            # Run Genetic Algorithm\n",
    "            best_solution, best_fitness, history = genetic_algorithm(\n",
    "                players_data,\n",
    "                **params\n",
    "            )\n",
    "            \n",
    "            # Stop counting fitness evaluations\n",
    "            evaluations = fitness_counter.stop_counting(LeagueSolution)\n",
    "            \n",
    "        elif algorithm_name == 'Hybrid GA':\n",
    "            # Start counting fitness evaluations\n",
    "            fitness_counter.start_counting(LeagueSolution)\n",
    "            \n",
    "            # Extract local search parameters\n",
    "            local_search = params.pop('local_search', None)\n",
    "            \n",
    "            # Run Genetic Algorithm with local search\n",
    "            best_solution, best_fitness, history = genetic_algorithm(\n",
    "                players_data,\n",
    "                local_search=local_search,\n",
    "                **params\n",
    "            )\n",
    "            \n",
    "            # Stop counting fitness evaluations\n",
    "            evaluations = fitness_counter.stop_counting(LeagueSolution)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown algorithm: {algorithm_name}\")\n",
    "        \n",
    "        # Calculate runtime\n",
    "        runtime = time.time() - start_time\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            'Configuration': config_name,\n",
    "            'Run': run_number,\n",
    "            'Fitness': best_fitness,\n",
    "            'Evaluations': evaluations,\n",
    "            'Runtime': runtime,\n",
    "            'History': history\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Return error information\n",
    "        return {\n",
    "            'Configuration': config_name,\n",
    "            'Run': run_number,\n",
    "            'Fitness': float('inf'),\n",
    "            'Evaluations': 0,\n",
    "            'Runtime': 0.0,\n",
    "            'History': [],\n",
    "            'Error': str(e)\n",
    "        }\n",
    "\n",
    "# Function to run experiments in parallel or sequentially\n",
    "def run_experiments(configs, players_data, num_runs=5, parallel=True, max_workers=None, save_csv=True):\n",
    "    \"\"\"\n",
    "    Run experiments for all configurations with multiple runs.\n",
    "    \n",
    "    Args:\n",
    "        configs: Dictionary of configurations to test\n",
    "        players_data: Player data for the experiments\n",
    "        num_runs: Number of runs per configuration\n",
    "        parallel: Whether to run experiments in parallel\n",
    "        max_workers: Maximum number of parallel workers (None = auto)\n",
    "        save_csv: Whether to save results to CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with experiment results\n",
    "    \"\"\"\n",
    "    # Create a list of all experiments to run\n",
    "    experiments = []\n",
    "    for config_name in configs:\n",
    "        for run in range(1, num_runs + 1):\n",
    "            experiments.append((config_name, run))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if parallel:\n",
    "        # Run experiments in parallel\n",
    "        print(f\"Running {len(experiments)} experiments in parallel mode...\")\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all experiments\n",
    "            future_to_exp = {\n",
    "                executor.submit(run_single_experiment, config_name, run, configs, players_data): \n",
    "                (config_name, run) for config_name, run in experiments\n",
    "            }\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future in concurrent.futures.as_completed(future_to_exp):\n",
    "                config_name, run = future_to_exp[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Check for errors\n",
    "                    if 'Error' in result:\n",
    "                        print(f\"Error in {config_name} - Run {run}: {result['Error']}\")\n",
    "                    else:\n",
    "                        print(f\"Completed {config_name} - Run {run}: Fitness = {result['Fitness']:.6f}, \"\n",
    "                              f\"Evaluations = {result['Evaluations']}, Runtime = {result['Runtime']:.2f}s\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Exception in {config_name} - Run {run}: {str(e)}\")\n",
    "    else:\n",
    "        # Run experiments sequentially\n",
    "        print(f\"Running {len(experiments)} experiments in sequential mode...\")\n",
    "        for config_name, run in experiments:\n",
    "            result = run_single_experiment(config_name, run, configs, players_data)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Check for errors\n",
    "            if 'Error' in result:\n",
    "                print(f\"Error in {config_name} - Run {run}: {result['Error']}\")\n",
    "            else:\n",
    "                print(f\"Completed {config_name} - Run {run}: Fitness = {result['Fitness']:.6f}, \"\n",
    "                      f\"Evaluations = {result['Evaluations']}, Runtime = {result['Runtime']:.2f}s\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results to CSV if requested\n",
    "    if save_csv:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"experiment_results_{timestamp}.csv\"\n",
    "        \n",
    "        # Create a copy of the DataFrame without the History column for CSV export\n",
    "        export_df = results_df.drop(columns=['History'], errors='ignore')\n",
    "        export_df.to_csv(filename, index=False)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef981e",
   "metadata": {},
   "source": [
    "## 3. Algorithm Implementations\n",
    "\n",
    "Let's run the experiments and compare the performance of different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "results_df = run_experiments(configs, players_data, num_runs=5, parallel=True, save_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfcf64",
   "metadata": {},
   "source": [
    "## 4. Performance Comparison\n",
    "\n",
    "### 4.1 Solution Quality Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa796eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare solution quality (fitness values)\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.boxplot(x='Configuration', y='Fitness', \n",
    "                data=results_df[results_df['Fitness'] < float('inf')],\n",
    "                palette='viridis', width=0.6)\n",
    "plt.title('Solution Quality Comparison Across Algorithms', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Algorithm Configuration', fontsize=14)\n",
    "plt.ylabel('Fitness Value (Lower is Better)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add mean markers\n",
    "means = results_df[results_df['Fitness'] < float('inf')].groupby('Configuration')['Fitness'].mean()\n",
    "for i, mean_val in enumerate(means):\n",
    "    ax.plot(i, mean_val, marker='o', color='red', markersize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display summary statistics with improved formatting\n",
    "fitness_stats = results_df.groupby('Configuration')['Fitness'].agg(['mean', 'std', 'min', 'max'])\n",
    "print(\"Fitness Statistics (Lower is Better):\")\n",
    "print(\"=\" * 80)\n",
    "formatted_stats = fitness_stats.copy()\n",
    "for col in formatted_stats.columns:\n",
    "    formatted_stats[col] = formatted_stats[col].apply(lambda x: f\"{x:.6f}\")\n",
    "print(formatted_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98b00d",
   "metadata": {},
   "source": [
    "### 4.2 Computational Efficiency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5274386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare runtime\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.barplot(x='Configuration', y='Runtime', data=results_df, \n",
    "                palette='Blues_d', errorbar=('ci', 95))\n",
    "plt.title('Runtime Comparison of Optimization Algorithms', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Algorithm Configuration', fontsize=14)\n",
    "plt.ylabel('Runtime (seconds)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, p in enumerate(ax.patches):\n",
    "    ax.annotate(f'{p.get_height():.2f}s', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare function evaluations\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.barplot(x='Configuration', y='Evaluations', hue='Configuration', data=results_df,\n",
    "                palette='Greens_d', errorbar=('ci', 95), legend=False)\n",
    "plt.title('Function Evaluations Comparison (Computational Efficiency)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Algorithm Configuration', fontsize=14)\n",
    "plt.ylabel('Number of Function Evaluations', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, p in enumerate(ax.patches):\n",
    "    ax.annotate(f'{int(p.get_height())}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14c8f2",
   "metadata": {},
   "source": [
    "### 4.3 Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curves for each algorithm and each run\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Use a consistent color palette for configurations\n",
    "config_colors = plt.cm.tab10(np.linspace(0, 1, len(configs)))\n",
    "config_color_map = {config: config_colors[i] for i, config in enumerate(configs.keys())}\n",
    "\n",
    "# Line styles for different runs\n",
    "line_styles = ['-', '--', ':', '-.', (0, (3, 1, 1, 1))]\n",
    "\n",
    "# Plot each configuration and run\n",
    "for config_name in configs.keys():\n",
    "    # Get all runs for this configuration\n",
    "    config_runs = results_df[(results_df['Configuration'] == config_name)]\n",
    "    \n",
    "    if len(config_runs) > 0 and 'History' in config_runs.columns:\n",
    "        # Plot each run separately\n",
    "        for i, (_, run_data) in enumerate(config_runs.iterrows()):\n",
    "            if isinstance(run_data['History'], list) and len(run_data['History']) > 0:\n",
    "                run_num = run_data['Run']\n",
    "                line_style = line_styles[i % len(line_styles)]\n",
    "                \n",
    "                # Plot the convergence curve with consistent color by config and line style by run\n",
    "                plt.plot(run_data['History'], \n",
    "                         label=f\"{config_name} (Run {run_num})\", \n",
    "                         color=config_color_map[config_name],\n",
    "                         linestyle=line_style,\n",
    "                         linewidth=2,\n",
    "                         alpha=0.8)\n",
    "\n",
    "plt.title('Convergence Curves por Execução de Cada Algoritmo', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Iterações', fontsize=14)\n",
    "plt.ylabel('Valor de Fitness (Menor é Melhor)', fontsize=14)\n",
    "plt.legend(title='Configuração e Execução', fontsize=10, title_fontsize=12, loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized convergence curves (by function evaluations) for each run\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Use a consistent color palette for configurations\n",
    "config_colors = plt.cm.tab10(np.linspace(0, 1, len(configs)))\n",
    "config_color_map = {config: config_colors[i] for i, config in enumerate(configs.keys())}\n",
    "\n",
    "# Line styles for different runs\n",
    "line_styles = ['-', '--', ':', '-.', (0, (3, 1, 1, 1))]\n",
    "\n",
    "# Plot each configuration and run\n",
    "for config_name in configs.keys():\n",
    "    # Get all runs for this configuration\n",
    "    config_runs = results_df[(results_df['Configuration'] == config_name)]\n",
    "    \n",
    "    if len(config_runs) > 0 and 'History' in config_runs.columns:\n",
    "        # Plot each run separately\n",
    "        for i, (_, run_data) in enumerate(config_runs.iterrows()):\n",
    "            if isinstance(run_data['History'], list) and len(run_data['History']) > 0:\n",
    "                run_num = run_data['Run']\n",
    "                line_style = line_styles[i % len(line_styles)]\n",
    "                \n",
    "                # Calculate evaluations per iteration (approximately)\n",
    "                evals_per_iter = run_data['Evaluations'] / len(run_data['History'])\n",
    "                \n",
    "                # Create x-axis values representing function evaluations\n",
    "                x_values = [i * evals_per_iter for i in range(len(run_data['History']))]\n",
    "                \n",
    "                # Plot the normalized convergence curve\n",
    "                plt.plot(x_values, run_data['History'], \n",
    "                         label=f\"{config_name} (Run {run_num})\",\n",
    "                         color=config_color_map[config_name],\n",
    "                         linestyle=line_style,\n",
    "                         linewidth=2,\n",
    "                         alpha=0.8)\n",
    "\n",
    "plt.title('Curvas de Convergência Normalizadas por Avaliações de Função', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Número de Avaliações de Função', fontsize=14)\n",
    "plt.ylabel('Valor de Fitness (Menor é Melhor)', fontsize=14)\n",
    "plt.legend(title='Configuração e Execução', fontsize=10, title_fontsize=12, loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c563bc",
   "metadata": {},
   "source": [
    "### 4.4 Comparação de Algoritmos Genéticos por Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e96ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curves for GA algorithms only, comparing all runs\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Filter only GA configurations\n",
    "ga_configs = {k: v for k, v in configs.items() if 'Genetic Algorithm' in v['algorithm']}\n",
    "\n",
    "# Use a consistent color palette for configurations\n",
    "ga_colors = plt.cm.viridis(np.linspace(0, 1, len(ga_configs)))\n",
    "ga_color_map = {config: ga_colors[i] for i, config in enumerate(ga_configs.keys())}\n",
    "\n",
    "# Line styles for different runs\n",
    "line_styles = ['-', '--', ':', '-.', (0, (3, 1, 1, 1))]\n",
    "markers = ['o', 's', '^', 'D', 'v']\n",
    "\n",
    "# Plot each GA configuration and run\n",
    "for config_name in ga_configs.keys():\n",
    "    # Get all runs for this configuration\n",
    "    config_runs = results_df[(results_df['Configuration'] == config_name)]\n",
    "    \n",
    "    if len(config_runs) > 0 and 'History' in config_runs.columns:\n",
    "        # Plot each run separately\n",
    "        for i, (_, run_data) in enumerate(config_runs.iterrows()):\n",
    "            if isinstance(run_data['History'], list) and len(run_data['History']) > 0:\n",
    "                run_num = run_data['Run']\n",
    "                line_style = line_styles[i % len(line_styles)]\n",
    "                marker = markers[i % len(markers)]\n",
    "                \n",
    "                # Plot with markers at regular intervals for better visibility\n",
    "                plt.plot(run_data['History'], \n",
    "                         label=f\"{config_name} (Run {run_num})\", \n",
    "                         color=ga_color_map[config_name],\n",
    "                         linestyle=line_style,\n",
    "                         marker=marker,\n",
    "                         markevery=max(1, len(run_data['History'])//10),  # Show markers at regular intervals\n",
    "                         markersize=6,\n",
    "                         linewidth=2,\n",
    "                         alpha=0.8)\n",
    "\n",
    "plt.title('Comparação de Algoritmos Genéticos por Execução', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Iterações', fontsize=14)\n",
    "plt.ylabel('Valor de Fitness (Menor é Melhor)', fontsize=14)\n",
    "plt.legend(title='Configuração e Execução', fontsize=10, title_fontsize=12, loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized convergence curves for GA algorithms only\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot each GA configuration and run with normalized x-axis\n",
    "for config_name in ga_configs.keys():\n",
    "    # Get all runs for this configuration\n",
    "    config_runs = results_df[(results_df['Configuration'] == config_name)]\n",
    "    \n",
    "    if len(config_runs) > 0 and 'History' in config_runs.columns:\n",
    "        # Plot each run separately\n",
    "        for i, (_, run_data) in enumerate(config_runs.iterrows()):\n",
    "            if isinstance(run_data['History'], list) and len(run_data['History']) > 0:\n",
    "                run_num = run_data['Run']\n",
    "                line_style = line_styles[i % len(line_styles)]\n",
    "                marker = markers[i % len(markers)]\n",
    "                \n",
    "                # Calculate evaluations per iteration (approximately)\n",
    "                evals_per_iter = run_data['Evaluations'] / len(run_data['History'])\n",
    "                \n",
    "                # Create x-axis values representing function evaluations\n",
    "                x_values = [i * evals_per_iter for i in range(len(run_data['History']))]\n",
    "                \n",
    "                # Plot the normalized convergence curve\n",
    "                plt.plot(x_values, run_data['History'], \n",
    "                         label=f\"{config_name} (Run {run_num})\",\n",
    "                         color=ga_color_map[config_name],\n",
    "                         linestyle=line_style,\n",
    "                         marker=marker,\n",
    "                         markevery=max(1, len(run_data['History'])//10),\n",
    "                         markersize=6,\n",
    "                         linewidth=2,\n",
    "                         alpha=0.8)\n",
    "\n",
    "plt.title('Curvas de Convergência Normalizadas - Algoritmos Genéticos', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Número de Avaliações de Função', fontsize=14)\n",
    "plt.ylabel('Valor de Fitness (Menor é Melhor)', fontsize=14)\n",
    "plt.legend(title='Configuração e Execução', fontsize=10, title_fontsize=12, loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a2a99",
   "metadata": {},
   "source": [
    "### 4.5 Comparação Empilhada de Algoritmos Genéticos por Geração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf61e29",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create a stacked comparison of GA algorithms showing average fitness by generation\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Filter only GA configurations\n",
    "ga_configs = {k: v for k, v in configs.items() if 'Genetic Algorithm' in v['algorithm']}\n",
    "\n",
    "# Use a consistent color palette for configurations\n",
    "ga_colors = plt.cm.plasma(np.linspace(0, 1, len(ga_configs)))\n",
    "ga_color_map = {config: ga_colors[i] for i, config in enumerate(ga_configs.keys())}\n",
    "\n",
    "# Find the maximum number of generations across all GA runs\n",
    "max_generations = 0\n",
    "for config_name in ga_configs.keys():\n",
    "    config_runs = results_df[(results_df['Configuration'] == config_name)]\n",
    "    for _, run_data in config_runs.iterrows():\n",
    "        if isinstance(run_data['History'], list) and len(run_data['History']) > max_generations:\n",
    "            max_generations = len(run_data['History'])\n",
    "\n",
    "# Prepare data structure to hold average fitness by generation for each GA type\n",
    "ga_avg_fitness = {}\n",
    "for config_name in ga_configs.keys():\n",
    "    ga_avg_fitness[config_name] = np.zeros(max_generations)\n",
    "    count = np.zeros(max_generations)\n",
    "    \n",
    "    # Get all runs for this configuration\n",
    "    config_runs = results_df[(results_df['Configuration'] == config_name)]\n",
    "    \n",
    "    # Accumulate fitness values for each generation\n",
    "    for _, run_data in config_runs.iterrows():\n",
    "        if isinstance(run_data['History'], list) and len(run_data['History']) > 0:\n",
    "            history = run_data['History']\n",
    "            for i, fitness in enumerate(history):\n",
    "                if i < max_generations:\n",
    "                    ga_avg_fitness[config_name][i] += fitness\n",
    "                    count[i] += 1\n",
    "    \n",
    "    # Calculate average, avoiding division by zero\n",
    "    for i in range(max_generations):\n",
    "        if count[i] > 0:\n",
    "            ga_avg_fitness[config_name][i] /= count[i]\n",
    "        else:\n",
    "            # If no data for this generation, use the last known value or infinity\n",
    "            if i > 0 and count[i-1] > 0:\n",
    "                ga_avg_fitness[config_name][i] = ga_avg_fitness[config_name][i-1]\n",
    "            else:\n",
    "                ga_avg_fitness[config_name][i] = float('inf')\n",
    "\n",
    "# Plot average fitness by generation for each GA type\n",
    "for config_name, avg_fitness in ga_avg_fitness.items():\n",
    "    plt.plot(range(max_generations), avg_fitness, \n",
    "             label=config_name,\n",
    "             color=ga_color_map[config_name],\n",
    "             linewidth=3)\n",
    "\n",
    "plt.title('Comparação de Fitness Médio por Geração - Algoritmos Genéticos', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Geração', fontsize=14)\n",
    "plt.ylabel('Fitness Médio (Menor é Melhor)', fontsize=14)\n",
    "plt.legend(title='Configuração GA', fontsize=12, title_fontsize=13, loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized by function evaluations\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Calculate average evaluations per generation for each GA type\n",
    "ga_avg_evals_per_gen = {}\n",
    "for config_name in ga_configs.keys():\n",
    "    config_runs = results_df[(results_df['Configuration'] == config_name)]\n",
    "    total_evals = 0\n",
    "    total_gens = 0\n",
    "    \n",
    "    for _, run_data in config_runs.iterrows():\n",
    "        if isinstance(run_data['History'], list) and len(run_data['History']) > 0:\n",
    "            total_evals += run_data['Evaluations']\n",
    "            total_gens += len(run_data['History'])\n",
    "    \n",
    "    if total_gens > 0:\n",
    "        ga_avg_evals_per_gen[config_name] = total_evals / total_gens\n",
    "    else:\n",
    "        ga_avg_evals_per_gen[config_name] = 1  # Default if no data\n",
    "\n",
    "# Plot average fitness by function evaluations for each GA type\n",
    "for config_name, avg_fitness in ga_avg_fitness.items():\n",
    "    # Create x-axis values representing function evaluations\n",
    "    evals_per_gen = ga_avg_evals_per_gen[config_name]\n",
    "    x_values = [i * evals_per_gen for i in range(len(avg_fitness))]\n",
    "    \n",
    "    plt.plot(x_values, avg_fitness, \n",
    "             label=config_name,\n",
    "             color=ga_color_map[config_name],\n",
    "             linewidth=3)\n",
    "\n",
    "plt.title('Comparação de Fitness Médio por Avaliações de Função - Algoritmos Genéticos', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Número de Avaliações de Função', fontsize=14)\n",
    "plt.ylabel('Fitness Médio (Menor é Melhor)', fontsize=14)\n",
    "plt.legend(title='Configuração GA', fontsize=12, title_fontsize=13, loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b8d29",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n",
    "\n",
    "Let's perform statistical tests to determine if there are significant differences between algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform statistical analysis\n",
    "def perform_statistical_analysis(results_df):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis on experiment results.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with experiment results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistical analysis results\n",
    "    \"\"\"\n",
    "    # Filter out invalid results (inf fitness)\n",
    "    valid_results = results_df[results_df['Fitness'] < float('inf')]\n",
    "    \n",
    "    # Get unique configurations\n",
    "    configurations = valid_results['Configuration'].unique()\n",
    "    \n",
    "    # If we have only one configuration, no comparison needed\n",
    "    if len(configurations) <= 1:\n",
    "        print(\"Not enough valid configurations for statistical comparison\")\n",
    "        return None\n",
    "    elif len(configurations) == 2:\n",
    "        print(\"\\n=== Two-Group Comparison ===\")\n",
    "        return two_group_comparison(valid_results, configurations)\n",
    "    else:\n",
    "        print(\"\\n=== Multiple-Group Comparison ===\")\n",
    "        return multiple_group_comparison(valid_results, configurations)\n",
    "\n",
    "# Function to perform two-group comparison\n",
    "def two_group_comparison(results_df, configurations):\n",
    "    \"\"\"\n",
    "    Perform statistical comparison between two groups.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with experiment results\n",
    "        configurations: List of two configurations to compare\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistical analysis results\n",
    "    \"\"\"\n",
    "    # Extract data for each configuration\n",
    "    group1 = results_df[results_df['Configuration'] == configurations[0]]['Fitness']\n",
    "    group2 = results_df[results_df['Configuration'] == configurations[1]]['Fitness']\n",
    "    \n",
    "    # Check for normality using Shapiro-Wilk test\n",
    "    _, p_norm1 = stats.shapiro(group1)\n",
    "    _, p_norm2 = stats.shapiro(group2)\n",
    "    \n",
    "    print(f\"Shapiro-Wilk normality test p-values: {configurations[0]}: {p_norm1:.4f}, {configurations[1]}: {p_norm2:.4f}\")\n",
    "    \n",
    "    # Determine if data is normally distributed\n",
    "    alpha = 0.05\n",
    "    is_normal = (p_norm1 > alpha) and (p_norm2 > alpha)\n",
    "    \n",
    "    if is_normal:\n",
    "        # Use t-test for normally distributed data\n",
    "        print(\"Data appears normally distributed, using t-test\")\n",
    "        t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)\n",
    "        test_name = \"Welch's t-test\"\n",
    "        \n",
    "        # Calculate effect size (Cohen's d)\n",
    "        mean1, mean2 = group1.mean(), group2.mean()\n",
    "        std1, std2 = group1.std(), group2.std()\n",
    "        pooled_std = np.sqrt(((len(group1) - 1) * std1**2 + (len(group2) - 1) * std2**2) / \n",
    "                             (len(group1) + len(group2) - 2))\n",
    "        effect_size = abs(mean1 - mean2) / pooled_std\n",
    "        effect_size_name = \"Cohen's d\"\n",
    "        \n",
    "    else:\n",
    "        # Use Mann-Whitney U test for non-normally distributed data\n",
    "        print(\"Data does not appear normally distributed, using Mann-Whitney U test\")\n",
    "        u_stat, p_value = stats.mannwhitneyu(group1, group2)\n",
    "        test_name = \"Mann-Whitney U test\"\n",
    "        \n",
    "        # Calculate effect size (r = Z / sqrt(N))\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        z_score = stats.norm.ppf(1 - p_value/2)  # Two-tailed p-value to z-score\n",
    "        effect_size = abs(z_score) / np.sqrt(n1 + n2)\n",
    "        effect_size_name = \"r (rank-biserial correlation)\"\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if effect_size < 0.2:\n",
    "        effect_interpretation = \"Small\"\n",
    "    elif effect_size < 0.5:\n",
    "        effect_interpretation = \"Medium\"\n",
    "    elif effect_size < 0.8:\n",
    "        effect_interpretation = \"Large\"\n",
    "    else:\n",
    "        effect_interpretation = \"Very Large\"\n",
    "    \n",
    "    # Determine significance\n",
    "    is_significant = p_value < alpha\n",
    "    \n",
    "    print(f\"{test_name} p-value: {p_value:.4f}\")\n",
    "    print(f\"Effect size ({effect_size_name}): {effect_size:.4f} ({effect_interpretation})\")\n",
    "    print(f\"Significant difference: {is_significant}\")\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'test_name': test_name,\n",
    "        'p_value': p_value,\n",
    "        'effect_size': effect_size,\n",
    "        'effect_size_name': effect_size_name,\n",
    "        'effect_interpretation': effect_interpretation,\n",
    "        'is_significant': is_significant,\n",
    "        'better_configuration': configurations[0] if group1.mean() < group2.mean() else configurations[1]\n",
    "    }\n",
    "\n",
    "# Function to perform multiple-group comparison\n",
    "def multiple_group_comparison(results_df, configurations):\n",
    "    \"\"\"\n",
    "    Perform statistical comparison between multiple groups.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with experiment results\n",
    "        configurations: List of configurations to compare\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistical analysis results\n",
    "    \"\"\"\n",
    "    # Check if we have enough data for each configuration\n",
    "    groups = []\n",
    "    valid_configs = []\n",
    "    for config in configurations:\n",
    "        group = results_df[results_df['Configuration'] == config]['Fitness']\n",
    "        if len(group) >= 3:  # Need at least 3 samples for statistical tests\n",
    "            groups.append(group)\n",
    "            valid_configs.append(config)\n",
    "        else:\n",
    "            print(f\"Warning: Configuration {config} has fewer than 3 valid results, excluding from analysis\")\n",
    "    \n",
    "    if len(groups) < 2:\n",
    "        print(\"Not enough valid configurations with sufficient data for statistical comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Check for normality using Shapiro-Wilk test\n",
    "    normal_data = True\n",
    "    for i, config in enumerate(valid_configs):\n",
    "        if i < len(groups):\n",
    "            # Handle potential errors in Shapiro-Wilk test\n",
    "            try:\n",
    "                _, p_norm = stats.shapiro(groups[i])\n",
    "                print(f\"Shapiro-Wilk normality test p-value for {config}: {p_norm:.4f}\")\n",
    "                if p_norm <= 0.05:\n",
    "                    normal_data = False\n",
    "            except Exception as e:\n",
    "                print(f\"Error in normality test for {config}: {str(e)}\")\n",
    "                normal_data = False  # Assume non-normal if test fails\n",
    "    \n",
    "    # Determine which test to use based on normality\n",
    "    alpha = 0.05\n",
    "    if normal_data:\n",
    "        # Use one-way ANOVA for normally distributed data\n",
    "        print(\"Data appears normally distributed, using one-way ANOVA\")\n",
    "        try:\n",
    "            f_stat, p_value = stats.f_oneway(*groups)\n",
    "            test_name = \"One-way ANOVA\"\n",
    "            \n",
    "            # Calculate effect size (Eta-squared)\n",
    "            # Convert groups to a single array and create a group label array\n",
    "            all_data = np.concatenate(groups)\n",
    "            group_labels = np.concatenate([[i] * len(group) for i, group in enumerate(groups)])\n",
    "            \n",
    "            # Calculate grand mean and group means\n",
    "            grand_mean = np.mean(all_data)\n",
    "            group_means = [np.mean(group) for group in groups]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in ANOVA test: {str(e)}\")\n",
    "            print(\"Falling back to non-parametric Kruskal-Wallis test\")\n",
    "            normal_data = False  # Fall back to non-parametric test\n",
    "        \n",
    "        # Calculate sum of squares\n",
    "        ss_total = np.sum((all_data - grand_mean) ** 2)\n",
    "        ss_between = np.sum([len(group) * (mean - grand_mean) ** 2 for group, mean in zip(groups, group_means)])\n",
    "        \n",
    "        # Calculate Eta-squared\n",
    "        effect_size = ss_between / ss_total\n",
    "        effect_size_name = \"Eta-squared\"\n",
    "        \n",
    "    else:\n",
    "        # Use Kruskal-Wallis test for non-normally distributed data\n",
    "        print(\"Data does not appear normally distributed, using Kruskal-Wallis test\")\n",
    "        h_stat, p_value = stats.kruskal(*groups)\n",
    "        test_name = \"Kruskal-Wallis test\"\n",
    "        \n",
    "        # Calculate effect size (Eta-squared for Kruskal-Wallis)\n",
    "        # This is an approximation based on the H statistic\n",
    "        n_total = sum(len(group) for group in groups)\n",
    "        effect_size = (h_stat - len(groups) + 1) / (n_total - len(groups))\n",
    "        effect_size_name = \"Eta-squared (H)\"\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if effect_size < 0.01:\n",
    "        effect_interpretation = \"Very Small\"\n",
    "    elif effect_size < 0.06:\n",
    "        effect_interpretation = \"Small\"\n",
    "    elif effect_size < 0.14:\n",
    "        effect_interpretation = \"Medium\"\n",
    "    else:\n",
    "        effect_interpretation = \"Large\"\n",
    "    \n",
    "    # Determine significance\n",
    "    is_significant = p_value < alpha\n",
    "    \n",
    "    print(f\"{test_name} p-value: {p_value:.4f}\")\n",
    "    print(f\"Effect size ({effect_size_name}): {effect_size:.4f} ({effect_interpretation})\")\n",
    "    print(f\"Significant difference: {is_significant}\")\n",
    "    \n",
    "    # If significant, perform post-hoc tests\n",
    "    significant_pairs = []\n",
    "    if is_significant:\n",
    "        print(\"\\n=== Post-hoc Tests ===\")\n",
    "        \n",
    "        if normal_data:\n",
    "            # Use Tukey HSD for normally distributed data\n",
    "            from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "            \n",
    "            # Prepare data for Tukey HSD\n",
    "            all_data = np.concatenate(groups)\n",
    "            group_labels = np.concatenate([[str(configurations[i])] * len(group) for i, group in enumerate(groups) if i < len(configurations)])\n",
    "            \n",
    "            # Perform Tukey HSD test\n",
    "            tukey_results = pairwise_tukeyhsd(all_data, group_labels, alpha=alpha)\n",
    "            print(tukey_results)\n",
    "            \n",
    "            # Extract significant pairs\n",
    "            for i in range(len(tukey_results.reject)):\n",
    "                if tukey_results.reject[i]:\n",
    "                    pair = (tukey_results.groupsunique[tukey_results.data[i, 0]], \n",
    "                            tukey_results.groupsunique[tukey_results.data[i, 1]])\n",
    "                    significant_pairs.append(pair)\n",
    "            \n",
    "            post_hoc_name = \"Tukey HSD\"\n",
    "            \n",
    "        else:\n",
    "            # Use Dunn's test with Bonferroni correction for non-normally distributed data\n",
    "            try:\n",
    "                # Prepare data for Dunn's test using scikit-posthocs\n",
    "                dunn_data = {}\n",
    "                for i, config in enumerate(configurations):\n",
    "                    if i < len(groups):\n",
    "                        dunn_data[config] = groups[i]\n",
    "                \n",
    "                # Perform Dunn's test\n",
    "                dunn_results = sp.posthoc_dunn(dunn_data, p_adjust='bonferroni')\n",
    "                print(dunn_results)\n",
    "                \n",
    "                # Extract significant pairs\n",
    "                for i in range(len(dunn_results.columns)):\n",
    "                    for j in range(i+1, len(dunn_results.columns)):\n",
    "                        if dunn_results.iloc[i, j] < alpha:\n",
    "                            pair = (dunn_results.columns[i], dunn_results.columns[j])\n",
    "                            significant_pairs.append(pair)\n",
    "                \n",
    "                post_hoc_name = \"Dunn's test with Bonferroni correction\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error performing Dunn's test: {str(e)}\")\n",
    "                print(\"Falling back to pairwise Mann-Whitney U tests with Bonferroni correction\")\n",
    "                \n",
    "                # Perform pairwise Mann-Whitney U tests with Bonferroni correction\n",
    "                num_comparisons = len(groups) * (len(groups) - 1) // 2\n",
    "                adjusted_alpha = alpha / num_comparisons\n",
    "                \n",
    "                for i in range(len(groups)):\n",
    "                    for j in range(i+1, len(groups)):\n",
    "                        u_stat, p_value = stats.mannwhitneyu(groups[i], groups[j])\n",
    "                        print(f\"{configurations[i]} vs {configurations[j]}: p-value = {p_value:.4f}\")\n",
    "                        \n",
    "                        if p_value < adjusted_alpha:\n",
    "                            pair = (configurations[i], configurations[j])\n",
    "                            significant_pairs.append(pair)\n",
    "                            print(f\"  Significant difference (p < {adjusted_alpha:.4f})\")\n",
    "                \n",
    "                post_hoc_name = \"Pairwise Mann-Whitney U tests with Bonferroni correction\"\n",
    "    \n",
    "    # Find the best configuration\n",
    "    mean_fitness = [group.mean() for group in groups]\n",
    "    best_idx = np.argmin(mean_fitness)\n",
    "    best_configuration = configurations[best_idx]\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'test_name': test_name,\n",
    "        'p_value': p_value,\n",
    "        'effect_size': effect_size,\n",
    "        'effect_size_name': effect_size_name,\n",
    "        'effect_interpretation': effect_interpretation,\n",
    "        'is_significant': is_significant,\n",
    "        'post_hoc_name': post_hoc_name if is_significant else None,\n",
    "        'significant_pairs': significant_pairs,\n",
    "        'best_configuration': best_configuration\n",
    "    }\n",
    "\n",
    "# Function to plot results with significance annotations\n",
    "def plot_with_significance(results_df, statistical_results):\n",
    "    \"\"\"\n",
    "    Plot results with significance annotations.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with experiment results\n",
    "        statistical_results: Results from statistical analysis\n",
    "    \"\"\"\n",
    "    if statistical_results is None:\n",
    "        print(\"No statistical results available for significance plotting\")\n",
    "        return\n",
    "    \n",
    "    # Filter out invalid results (inf fitness)\n",
    "    valid_results = results_df[results_df['Fitness'] < float('inf')]\n",
    "    \n",
    "    if len(valid_results) == 0:\n",
    "        print(\"No valid results available for significance plotting\")\n",
    "        return\n",
    "    \n",
    "    # Create boxplot with improved styling\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    ax = sns.boxplot(x='Configuration', y='Fitness', data=valid_results, \n",
    "                    palette='viridis', width=0.6)\n",
    "    plt.title('Solution Quality Comparison with Statistical Significance', \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Algorithm Configuration', fontsize=14)\n",
    "    plt.ylabel('Fitness Value (Lower is Better)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add significance annotations if there are significant differences\n",
    "    if statistical_results.get('is_significant', False) and 'significant_pairs' in statistical_results:\n",
    "        # Get the positions of the boxes\n",
    "        box_positions = {config: i for i, config in enumerate(valid_results['Configuration'].unique())}\n",
    "        \n",
    "        # Add annotations for significant pairs\n",
    "        y_max = valid_results['Fitness'].max()\n",
    "        y_range = valid_results['Fitness'].max() - valid_results['Fitness'].min()\n",
    "        \n",
    "        # Add a legend for significance levels\n",
    "        plt.figtext(0.01, 0.01, \"* p < 0.05, ** p < 0.01, *** p < 0.001\", \n",
    "                   ha=\"left\", fontsize=12, bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "        \n",
    "        for i, (config1, config2) in enumerate(statistical_results['significant_pairs']):\n",
    "            if config1 in box_positions and config2 in box_positions:\n",
    "                x1, x2 = box_positions[config1], box_positions[config2]\n",
    "                y = y_max + (i + 1) * y_range * 0.05\n",
    "                \n",
    "                # Draw the line\n",
    "                plt.plot([x1, x2], [y, y], 'k-', linewidth=1.5)\n",
    "                plt.plot([x1, x1], [y - y_range * 0.01, y], 'k-', linewidth=1.5)\n",
    "                plt.plot([x2, x2], [y - y_range * 0.01, y], 'k-', linewidth=1.5)\n",
    "                \n",
    "                # Determine significance level symbol\n",
    "                if 'p_values' in statistical_results and len(statistical_results['p_values']) > i:\n",
    "                    p_value = statistical_results['p_values'][i]\n",
    "                    if p_value < 0.001:\n",
    "                        sig_symbol = '***'\n",
    "                    elif p_value < 0.01:\n",
    "                        sig_symbol = '**'\n",
    "                    else:\n",
    "                        sig_symbol = '*'\n",
    "                else:\n",
    "                    sig_symbol = '*'\n",
    "                \n",
    "                # Add significance symbol\n",
    "                plt.text((x1 + x2) / 2, y, sig_symbol, ha='center', va='bottom', fontsize=14)\n",
    "    \n",
    "    # Add a note about the best configuration if available\n",
    "    if 'best_configuration' in statistical_results:\n",
    "        best_config = statistical_results['best_configuration']\n",
    "        plt.figtext(0.5, 0.01, f\"Best configuration: {best_config}\", \n",
    "                   ha=\"center\", fontsize=12, bbox={\"facecolor\":\"lightgreen\", \"alpha\":0.8, \"pad\":5})\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Perform statistical analysis\n",
    "statistical_results = perform_statistical_analysis(results_df)\n",
    "\n",
    "# Plot with significance annotations\n",
    "if statistical_results:\n",
    "    plot_with_significance(results_df, statistical_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f1b8af",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Based on our experiments and statistical analysis, we can draw the following conclusions:\n",
    "\n",
    "1. **Solution Quality**: [Fill in based on results]\n",
    "2. **Computational Efficiency**: [Fill in based on results]\n",
    "3. **Convergence Speed**: [Fill in based on results]\n",
    "4. **Statistical Significance**: [Fill in based on results]\n",
    "\n",
    "Overall, the [best algorithm] appears to be the most effective approach for the Sports League optimization problem, providing the best balance between solution quality and computational efficiency."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
