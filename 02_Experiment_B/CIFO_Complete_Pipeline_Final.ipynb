{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e871d043",
   "metadata": {},
   "source": [
    "# CIFO - Complete Pipeline: Optimization Algorithms Execution and Analysis\n",
    "\n",
    "This notebook integrates the entire process from algorithm execution to results visualization and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e44c38",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa89786",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'mutate_targeted_player_exchange' from 'operators' (/teamspace/studios/this_studio/CIFO/02 Experiment_02/operators.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Import project modules\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msolution\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LeagueSolution, LeagueHillClimbingSolution\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevolution\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hill_climbing, simulated_annealing\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moperators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     34\u001b[39m     selection_tournament,\n\u001b[32m     35\u001b[39m     selection_ranking,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     genetic_algorithm\n\u001b[32m     42\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfitness_counter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FitnessCounter\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CIFO/02 Experiment_02/evolution.py:147\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_solution, best_fitness, history\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Import genetic algorithm functions from operators.py\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moperators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    148\u001b[39m     generate_population,\n\u001b[32m    149\u001b[39m     genetic_algorithm,\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# Mutation operators\u001b[39;00m\n\u001b[32m    151\u001b[39m     mutate_swap,\n\u001b[32m    152\u001b[39m     mutate_swap_constrained,\n\u001b[32m    153\u001b[39m     mutate_team_shift,\n\u001b[32m    154\u001b[39m     mutate_targeted_player_exchange,\n\u001b[32m    155\u001b[39m     mutate_shuffle_within_team_constrained,\n\u001b[32m    156\u001b[39m     \u001b[38;5;66;03m# Crossover operators\u001b[39;00m\n\u001b[32m    157\u001b[39m     crossover_one_point,\n\u001b[32m    158\u001b[39m     crossover_one_point_prefer_valid,\n\u001b[32m    159\u001b[39m     crossover_uniform,\n\u001b[32m    160\u001b[39m     crossover_uniform_prefer_valid,\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# Selection operators\u001b[39;00m\n\u001b[32m    162\u001b[39m     selection_tournament,\n\u001b[32m    163\u001b[39m     selection_tournament_variable_k,\n\u001b[32m    164\u001b[39m     selection_ranking,\n\u001b[32m    165\u001b[39m     selection_boltzmann\n\u001b[32m    166\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'mutate_targeted_player_exchange' from 'operators' (/teamspace/studios/this_studio/CIFO/02 Experiment_02/operators.py)"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import scipy.stats as stats\n",
    "import scikit_posthocs as sp\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import multiprocessing\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "\n",
    "# Configure matplotlib for notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"scipy.stats.shapiro: Input data has range zero.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"No artists with labels found to put in legend.*\")\n",
    "\n",
    "# Import project modules\n",
    "from solution import LeagueSolution, LeagueHillClimbingSolution\n",
    "from evolution import hill_climbing, simulated_annealing\n",
    "from operators import (\n",
    "    selection_tournament,\n",
    "    selection_ranking,\n",
    "    selection_boltzmann,\n",
    "    crossover_one_point,\n",
    "    crossover_uniform,\n",
    "    mutate_swap, \n",
    "    mutate_swap_constrained,\n",
    "    genetic_algorithm\n",
    ")\n",
    "from fitness_counter import FitnessCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e675a4",
   "metadata": {},
   "source": [
    "## 2. Execution Mode and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829e9ce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define execution mode enum\n",
    "class ExecutionMode(Enum):\n",
    "    SINGLE_PROCESSOR = 1\n",
    "    MULTI_PROCESSOR = 2\n",
    "\n",
    "# Implement two-point crossover (not in operators.py)\n",
    "def two_point_crossover(parent1, parent2):\n",
    "    \"\"\"\n",
    "    Two-point crossover: creates a child by taking portions from each parent.\n",
    "    \n",
    "    Args:\n",
    "        parent1 (LeagueSolution): First parent solution\n",
    "        parent2 (LeagueSolution): Second parent solution\n",
    "        \n",
    "    Returns:\n",
    "        LeagueSolution: A new solution created by crossover\n",
    "    \"\"\"\n",
    "    cut1 = random.randint(1, len(parent1.repr) - 3)\n",
    "    cut2 = random.randint(cut1 + 1, len(parent1.repr) - 2)\n",
    "    child_repr = parent1.repr[:cut1] + parent2.repr[cut1:cut2] + parent1.repr[cut2:]\n",
    "    \n",
    "    return LeagueSolution(\n",
    "        repr=child_repr,\n",
    "        num_teams=parent1.num_teams,\n",
    "        team_size=parent1.team_size,\n",
    "        max_budget=parent1.max_budget,\n",
    "        players=parent1.players\n",
    "    )\n",
    "\n",
    "# Implement scramble mutation\n",
    "def mutate_scramble(solution, mutation_rate=0.1):\n",
    "    \"\"\"\n",
    "    Scramble mutation: randomly selects a subsequence and shuffles it.\n",
    "    \n",
    "    Args:\n",
    "        solution (LeagueSolution): Solution to mutate\n",
    "        mutation_rate (float): Probability of mutation for each position\n",
    "        \n",
    "    Returns:\n",
    "        LeagueSolution: Mutated solution\n",
    "    \"\"\"\n",
    "    mutated = deepcopy(solution)\n",
    "    \n",
    "    # Determine if mutation occurs based on rate\n",
    "    if random.random() < mutation_rate:\n",
    "        # Select random subsequence\n",
    "        length = len(mutated.repr)\n",
    "        start = random.randint(0, length - 2)\n",
    "        end = random.randint(start + 1, length - 1)\n",
    "        \n",
    "        # Extract subsequence\n",
    "        subsequence = mutated.repr[start:end+1]\n",
    "        \n",
    "        # Shuffle subsequence\n",
    "        random.shuffle(subsequence)\n",
    "        \n",
    "        # Replace original subsequence with shuffled one\n",
    "        mutated.repr[start:end+1] = subsequence\n",
    "    \n",
    "    return mutated\n",
    "\n",
    "# Function to interpret effect size\n",
    "def interpret_effect_size(eta_squared):\n",
    "    \"\"\"\n",
    "    Interprets the eta-squared effect size according to statistical conventions.\n",
    "    \n",
    "    Args:\n",
    "        eta_squared (float): Eta-squared value\n",
    "        \n",
    "    Returns:\n",
    "        str: Effect size interpretation\n",
    "    \"\"\"\n",
    "    if eta_squared < 0.01:\n",
    "        return \"Negligible\"\n",
    "    elif eta_squared < 0.06:\n",
    "        return \"Small\"\n",
    "    elif eta_squared < 0.14:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e42247",
   "metadata": {},
   "source": [
    "## 3. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7215b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized experiment configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    # General parameters\n",
    "    'seed': 42,                    # Seed for reproducibility\n",
    "    'num_runs': 30,                # Number of runs for each algorithm\n",
    "    'max_evaluations': 10000,      # Maximum number of function evaluations\n",
    "    'population_size': 100,        # Population size for genetic algorithms\n",
    "    'max_generations': 100,        # Maximum number of generations for genetic algorithms\n",
    "    \n",
    "    # Execution parameters\n",
    "    'execution_mode': ExecutionMode.MULTI_PROCESSOR,  # Parallel execution by default\n",
    "    'num_processes': max(1, multiprocessing.cpu_count() - 1),  # Use all but one CPU core\n",
    "    \n",
    "    # Statistical analysis parameters\n",
    "    'alpha': 0.05,                 # Significance level for statistical tests\n",
    "    'post_hoc_method': 'tukey',    # Post-hoc test method ('tukey' or 'dunn')\n",
    "    \n",
    "    # Visualization parameters\n",
    "    'figure_size': (14, 10),       # Default figure size\n",
    "    'save_figures': False,         # Save figures to files\n",
    "    'figure_format': 'png',        # Format for saving figures\n",
    "    \n",
    "    # Data storage parameters\n",
    "    'save_results': True,          # Save results to files\n",
    "    'save_history': True,          # Save convergence history\n",
    "    'save_statistics': True,       # Save statistical test results\n",
    "    'results_dir': 'experiment_results',  # Directory for storing results\n",
    "    \n",
    "    # Execution parameters\n",
    "    'verbose': True,               # Show detailed progress\n",
    "    'load_existing': False,        # Load existing results (if available)\n",
    "}\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "if EXPERIMENT_CONFIG['save_results'] and not os.path.exists(EXPERIMENT_CONFIG['results_dir']):\n",
    "    os.makedirs(EXPERIMENT_CONFIG['results_dir'])\n",
    "\n",
    "# Apply configurations\n",
    "random.seed(EXPERIMENT_CONFIG['seed'])\n",
    "np.random.seed(EXPERIMENT_CONFIG['seed'])\n",
    "plt.rcParams['figure.figsize'] = EXPERIMENT_CONFIG['figure_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7ab92",
   "metadata": {},
   "source": [
    "## 4. Player Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load player data using the specified method\n",
    "players_df = pd.read_csv('players.csv', encoding='utf-8', sep=';', index_col=0)\n",
    "\n",
    "# Display first rows\n",
    "print(\"Player data:\")\n",
    "display(players_df.head())\n",
    "\n",
    "# Check if salary column has the correct name\n",
    "if 'Salary (€M)' in players_df.columns:\n",
    "    # Rename columns for compatibility with code\n",
    "    column_mapping = {\n",
    "        'Salary (€M)': 'Salary'\n",
    "    }\n",
    "    players_df = players_df.rename(columns=column_mapping)\n",
    "\n",
    "# Convert DataFrame to list of dictionaries\n",
    "players_list = players_df.to_dict('records')\n",
    "\n",
    "# Configure fitness counter\n",
    "fitness_counter = FitnessCounter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47469ce7",
   "metadata": {},
   "source": [
    "## 5. Algorithm Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524fb4d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configure algorithms\n",
    "configs = {\n",
    "    # Existing algorithms\n",
    "    'HC_Standard': {\n",
    "        'algorithm': 'Hill Climbing',\n",
    "    },\n",
    "    'SA_Standard': {\n",
    "        'algorithm': 'Simulated Annealing',\n",
    "    },\n",
    "    'GA_Tournament_OnePoint': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'selection': 'Tournament',\n",
    "        'crossover': 'One Point',\n",
    "        'mutation': 'Swap',\n",
    "        'mutation_rate': 1.0/35,  # 1.0/len(players)\n",
    "        'elitism_percent': 0.1,   # 10%\n",
    "        'population_size': EXPERIMENT_CONFIG['population_size'],\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    'GA_Tournament_TwoPoint': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'selection': 'Tournament',\n",
    "        'crossover': 'Two Point',\n",
    "        'mutation': 'Swap',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': EXPERIMENT_CONFIG['population_size'],\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    'GA_Rank_Uniform': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'selection': 'Rank',\n",
    "        'crossover': 'Uniform',\n",
    "        'mutation': 'Swap',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': EXPERIMENT_CONFIG['population_size'],\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    'GA_Boltzmann_TwoPoint': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'selection': 'Boltzmann',\n",
    "        'crossover': 'Two Point',\n",
    "        'mutation': 'Swap',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': EXPERIMENT_CONFIG['population_size'],\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    'GA_Hybrid': {\n",
    "        'algorithm': 'Genetic Algorithm Hybrid',\n",
    "        'selection': 'Tournament',\n",
    "        'crossover': 'Two Point',\n",
    "        'mutation': 'Swap',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': EXPERIMENT_CONFIG['population_size'],\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    \n",
    "    # Promising algorithms\n",
    "    'HC_Random_Restart': {\n",
    "        'algorithm': 'Hill Climbing Random Restart',\n",
    "        'restart_iterations': 10,  # Number of iterations before restart\n",
    "    },\n",
    "    'GA_Memetic': {\n",
    "        'algorithm': 'Genetic Algorithm Memetic',\n",
    "        'selection': 'Tournament',\n",
    "        'crossover': 'Two Point',\n",
    "        'mutation': 'Swap',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': EXPERIMENT_CONFIG['population_size'],\n",
    "        'local_search_prob': 0.2,  # Probability of applying local search\n",
    "        'local_search_iters': 20,  # Number of local search iterations\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    'GA_Island_Model': {\n",
    "        'algorithm': 'Genetic Algorithm Island Model',\n",
    "        'selection': 'Tournament',\n",
    "        'crossover': 'Two Point',\n",
    "        'mutation': 'Swap',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': EXPERIMENT_CONFIG['population_size'],\n",
    "        'num_islands': 4,          # Number of islands\n",
    "        'migration_interval': 10,  # Migration interval (generations)\n",
    "        'migration_size': 5,       # Number of individuals to migrate\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    'GA_Scramble_Mutation': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'selection': 'Tournament',\n",
    "        'crossover': 'Two Point',\n",
    "        'mutation': 'Scramble',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': EXPERIMENT_CONFIG['population_size'],\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display configurations\n",
    "print(\"Algorithm configurations:\")\n",
    "for config_name, config in configs.items():\n",
    "    print(f\"\\n{config_name}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83ea15",
   "metadata": {},
   "source": [
    "## 6. Algorithm Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543826b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to run Hill Climbing\n",
    "def run_hill_climbing(players, max_evaluations):\n",
    "    solution = LeagueSolution(players)\n",
    "    \n",
    "    # Initialize fitness counter\n",
    "    fitness_counter.reset()\n",
    "    solution.set_fitness_counter(fitness_counter)\n",
    "    \n",
    "    best_fitness = solution.fitness()\n",
    "    history = [best_fitness]\n",
    "    \n",
    "    while fitness_counter.get_count() < max_evaluations:\n",
    "        # Generate neighbor\n",
    "        neighbor = deepcopy(solution)\n",
    "        idx = random.randint(0, len(neighbor.repr) - 1)\n",
    "        neighbor.repr[idx] = random.randint(0, neighbor.num_teams - 1)\n",
    "        \n",
    "        neighbor_fitness = neighbor.fitness()\n",
    "        \n",
    "        # Accept if better\n",
    "        if neighbor_fitness < best_fitness:  # Lower is better\n",
    "            solution = neighbor\n",
    "            best_fitness = neighbor_fitness\n",
    "        \n",
    "        history.append(best_fitness)\n",
    "    \n",
    "    return solution, history, fitness_counter.get_count()\n",
    "\n",
    "# Function to run Hill Climbing with Random Restart\n",
    "def run_hill_climbing_random_restart(players, max_evaluations, restart_iterations=10):\n",
    "    solution = LeagueSolution(players)\n",
    "    \n",
    "    # Initialize fitness counter\n",
    "    fitness_counter.reset()\n",
    "    solution.set_fitness_counter(fitness_counter)\n",
    "    \n",
    "    best_solution = deepcopy(solution)\n",
    "    best_fitness = solution.fitness()\n",
    "    history = [best_fitness]\n",
    "    \n",
    "    iteration_count = 0\n",
    "    \n",
    "    while fitness_counter.get_count() < max_evaluations:\n",
    "        # Generate neighbor\n",
    "        neighbor = deepcopy(solution)\n",
    "        idx = random.randint(0, len(neighbor.repr) - 1)\n",
    "        neighbor.repr[idx] = random.randint(0, neighbor.num_teams - 1)\n",
    "        \n",
    "        neighbor_fitness = neighbor.fitness()\n",
    "        \n",
    "        # Accept if better\n",
    "        if neighbor_fitness < best_fitness:  # Lower is better\n",
    "            solution = neighbor\n",
    "            best_fitness = neighbor_fitness\n",
    "            best_solution = deepcopy(solution)\n",
    "            iteration_count = 0  # Reset iteration count on improvement\n",
    "        else:\n",
    "            iteration_count += 1\n",
    "        \n",
    "        # Random restart if stuck for too long\n",
    "        if iteration_count >= restart_iterations:\n",
    "            solution = LeagueSolution(players)\n",
    "            solution.set_fitness_counter(fitness_counter)\n",
    "            current_fitness = solution.fitness()\n",
    "            iteration_count = 0\n",
    "            \n",
    "            # Keep track of best solution across restarts\n",
    "            if current_fitness < best_fitness:\n",
    "                best_solution = deepcopy(solution)\n",
    "                best_fitness = current_fitness\n",
    "        \n",
    "        history.append(best_fitness)\n",
    "    \n",
    "    return best_solution, history, fitness_counter.get_count()\n",
    "\n",
    "# Function to run Simulated Annealing\n",
    "def run_simulated_annealing(players, max_evaluations):\n",
    "    solution = LeagueSolution(players)\n",
    "    \n",
    "    # Initialize fitness counter\n",
    "    fitness_counter.reset()\n",
    "    solution.set_fitness_counter(fitness_counter)\n",
    "    \n",
    "    best_solution = deepcopy(solution)\n",
    "    current_fitness = solution.fitness()\n",
    "    best_fitness = current_fitness\n",
    "    \n",
    "    history = [best_fitness]\n",
    "    \n",
    "    # SA parameters\n",
    "    initial_temp = 100.0\n",
    "    final_temp = 0.1\n",
    "    alpha = 0.95\n",
    "    \n",
    "    current_temp = initial_temp\n",
    "    \n",
    "    while fitness_counter.get_count() < max_evaluations and current_temp > final_temp:\n",
    "        # Generate neighbor\n",
    "        neighbor = deepcopy(solution)\n",
    "        idx = random.randint(0, len(neighbor.repr) - 1)\n",
    "        neighbor.repr[idx] = random.randint(0, neighbor.num_teams - 1)\n",
    "        \n",
    "        neighbor_fitness = neighbor.fitness()\n",
    "        \n",
    "        # Calculate delta\n",
    "        delta = neighbor_fitness - current_fitness\n",
    "        \n",
    "        # Accept if better or with temperature-based probability\n",
    "        if delta < 0 or random.random() < np.exp(-delta / current_temp):\n",
    "            solution = neighbor\n",
    "            current_fitness = neighbor_fitness\n",
    "            \n",
    "            # Update best solution if needed\n",
    "            if current_fitness < best_fitness:\n",
    "                best_solution = deepcopy(solution)\n",
    "                best_fitness = current_fitness\n",
    "        \n",
    "        history.append(best_fitness)\n",
    "        \n",
    "        # Cool down\n",
    "        current_temp *= alpha\n",
    "    \n",
    "    return best_solution, history, fitness_counter.get_count()\n",
    "\n",
    "# Function to run Genetic Algorithm\n",
    "def run_genetic_algorithm(players, config, max_evaluations):\n",
    "    # Initialize fitness counter\n",
    "    fitness_counter.reset()\n",
    "    \n",
    "    # Configure selection\n",
    "    if config['selection'] == 'Tournament':\n",
    "        selection_op = selection_tournament\n",
    "    elif config['selection'] == 'Rank':\n",
    "        selection_op = selection_ranking\n",
    "    elif config['selection'] == 'Boltzmann':\n",
    "        selection_op = selection_boltzmann\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported selection: {config['selection']}\")\n",
    "    \n",
    "    # Configure crossover\n",
    "    if config['crossover'] == 'One Point':\n",
    "        crossover_op = crossover_one_point\n",
    "    elif config['crossover'] == 'Two Point':\n",
    "        crossover_op = two_point_crossover\n",
    "    elif config['crossover'] == 'Uniform':\n",
    "        crossover_op = crossover_uniform\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported crossover: {config['crossover']}\")\n",
    "    \n",
    "    # Configure mutation\n",
    "    if config['mutation'] == 'Swap':\n",
    "        mutation_op = mutate_swap\n",
    "    elif config['mutation'] == 'Scramble':\n",
    "        mutation_op = mutate_scramble\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mutation: {config['mutation']}\")\n",
    "    \n",
    "    # Configure repair operator (if needed)\n",
    "    repair_op = None\n",
    "    if config.get('use_repair', False):\n",
    "        def repair_operator(solution):\n",
    "            # Simple repair implementation: tries to fix invalid solutions\n",
    "            # by adjusting player distribution by position and budget\n",
    "            if solution.is_valid():\n",
    "                return solution\n",
    "            \n",
    "            # Get team statistics\n",
    "            teams = solution.get_teams()\n",
    "            \n",
    "            # Check and fix position distribution\n",
    "            for team_idx, team in enumerate(teams):\n",
    "                positions = {\"GK\": 0, \"DEF\": 0, \"MID\": 0, \"FWD\": 0}\n",
    "                for player in team:\n",
    "                    positions[player[\"Position\"]] += 1\n",
    "                \n",
    "                # If distribution is incorrect, try to fix\n",
    "                if positions != {\"GK\": 1, \"DEF\": 2, \"MID\": 2, \"FWD\": 2}:\n",
    "                    # Simplified implementation: just return original solution\n",
    "                    # A real implementation would be more complex\n",
    "                    pass\n",
    "            \n",
    "            return solution\n",
    "        \n",
    "        repair_op = repair_operator\n",
    "    \n",
    "    # Configure local search for hybrid GA\n",
    "    local_search = None\n",
    "    if config['algorithm'] == 'Genetic Algorithm Hybrid':\n",
    "        local_search = {\n",
    "            'operator': 'hill_climbing',\n",
    "            'probability': 0.1,\n",
    "            'iterations': 10\n",
    "        }\n",
    "    \n",
    "    # Run GA\n",
    "    best_solution, best_fitness, history = genetic_algorithm(\n",
    "        players=players,\n",
    "        population_size=config['population_size'],\n",
    "        max_generations=EXPERIMENT_CONFIG['max_generations'],\n",
    "        selection_operator=selection_op,\n",
    "        crossover_operator=crossover_op,\n",
    "        crossover_rate=0.8,\n",
    "        mutation_operator=mutation_op,\n",
    "        mutation_rate=config['mutation_rate'],\n",
    "        elitism=config['elitism_percent'] > 0,\n",
    "        elitism_size=int(config['population_size'] * config['elitism_percent']),\n",
    "        local_search=local_search,\n",
    "        fitness_counter=fitness_counter,\n",
    "        max_evaluations=max_evaluations,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return best_solution, history, fitness_counter.get_count()\n",
    "\n",
    "# Function to run Memetic Algorithm (GA with local search)\n",
    "def run_memetic_algorithm(players, config, max_evaluations):\n",
    "    # Initialize fitness counter\n",
    "    fitness_counter.reset()\n",
    "    \n",
    "    # Configure selection\n",
    "    if config['selection'] == 'Tournament':\n",
    "        selection_op = selection_tournament\n",
    "    elif config['selection'] == 'Rank':\n",
    "        selection_op = selection_ranking\n",
    "    elif config['selection'] == 'Boltzmann':\n",
    "        selection_op = selection_boltzmann\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported selection: {config['selection']}\")\n",
    "    \n",
    "    # Configure crossover\n",
    "    if config['crossover'] == 'One Point':\n",
    "        crossover_op = crossover_one_point\n",
    "    elif config['crossover'] == 'Two Point':\n",
    "        crossover_op = two_point_crossover\n",
    "    elif config['crossover'] == 'Uniform':\n",
    "        crossover_op = crossover_uniform\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported crossover: {config['crossover']}\")\n",
    "    \n",
    "    # Configure mutation\n",
    "    if config['mutation'] == 'Swap':\n",
    "        mutation_op = mutate_swap\n",
    "    elif config['mutation'] == 'Scramble':\n",
    "        mutation_op = mutate_scramble\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mutation: {config['mutation']}\")\n",
    "    \n",
    "    # Define local search function\n",
    "    def local_search(solution, iterations):\n",
    "        best_sol = deepcopy(solution)\n",
    "        best_fitness = solution.fitness()\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            # Generate neighbor\n",
    "            neighbor = deepcopy(best_sol)\n",
    "            idx = random.randint(0, len(neighbor.repr) - 1)\n",
    "            neighbor.repr[idx] = random.randint(0, neighbor.num_teams - 1)\n",
    "            \n",
    "            neighbor_fitness = neighbor.fitness()\n",
    "            \n",
    "            # Accept if better\n",
    "            if neighbor_fitness < best_fitness:\n",
    "                best_sol = neighbor\n",
    "                best_fitness = neighbor_fitness\n",
    "        \n",
    "        return best_sol\n",
    "    \n",
    "    # Initialize population\n",
    "    population = []\n",
    "    for _ in range(config['population_size']):\n",
    "        solution = LeagueSolution(players)\n",
    "        solution.set_fitness_counter(fitness_counter)\n",
    "        population.append(solution)\n",
    "    \n",
    "    # Evaluate initial population\n",
    "    fitness_values = [solution.fitness() for solution in population]\n",
    "    \n",
    "    # Find best solution\n",
    "    best_idx = np.argmin(fitness_values)\n",
    "    best_solution = deepcopy(population[best_idx])\n",
    "    best_fitness = fitness_values[best_idx]\n",
    "    \n",
    "    # Initialize history\n",
    "    history = [best_fitness]\n",
    "    \n",
    "    # Main loop\n",
    "    generation = 0\n",
    "    while fitness_counter.get_count() < max_evaluations and generation < EXPERIMENT_CONFIG['max_generations']:\n",
    "        # Create new population\n",
    "        new_population = []\n",
    "        \n",
    "        # Elitism\n",
    "        if config['elitism_percent'] > 0:\n",
    "            elitism_size = int(config['population_size'] * config['elitism_percent'])\n",
    "            elite_indices = np.argsort(fitness_values)[:elitism_size]\n",
    "            for idx in elite_indices:\n",
    "                new_population.append(deepcopy(population[idx]))\n",
    "        \n",
    "        # Fill rest of population\n",
    "        while len(new_population) < config['population_size']:\n",
    "            # Selection\n",
    "            parent1_idx = selection_op(fitness_values)\n",
    "            parent2_idx = selection_op(fitness_values)\n",
    "            \n",
    "            parent1 = population[parent1_idx]\n",
    "            parent2 = population[parent2_idx]\n",
    "            \n",
    "            # Crossover\n",
    "            if random.random() < 0.8:  # Crossover rate\n",
    "                child = crossover_op(parent1, parent2)\n",
    "            else:\n",
    "                child = deepcopy(parent1)\n",
    "            \n",
    "            # Mutation\n",
    "            if random.random() < config['mutation_rate']:\n",
    "                child = mutation_op(child)\n",
    "            \n",
    "            # Local search with probability\n",
    "            if random.random() < config['local_search_prob']:\n",
    "                child = local_search(child, config['local_search_iters'])\n",
    "            \n",
    "            new_population.append(child)\n",
    "        \n",
    "        # Replace population\n",
    "        population = new_population\n",
    "        \n",
    "        # Evaluate new population\n",
    "        fitness_values = [solution.fitness() for solution in population]\n",
    "        \n",
    "        # Update best solution\n",
    "        current_best_idx = np.argmin(fitness_values)\n",
    "        if fitness_values[current_best_idx] < best_fitness:\n",
    "            best_solution = deepcopy(population[current_best_idx])\n",
    "            best_fitness = fitness_values[current_best_idx]\n",
    "        \n",
    "        # Update history\n",
    "        history.append(best_fitness)\n",
    "        \n",
    "        generation += 1\n",
    "    \n",
    "    return best_solution, history, fitness_counter.get_count()\n",
    "\n",
    "# Function to run Island Model GA\n",
    "def run_island_model_ga(players, config, max_evaluations):\n",
    "    # Initialize fitness counter\n",
    "    fitness_counter.reset()\n",
    "    \n",
    "    # Configure selection\n",
    "    if config['selection'] == 'Tournament':\n",
    "        selection_op = selection_tournament\n",
    "    elif config['selection'] == 'Rank':\n",
    "        selection_op = selection_ranking\n",
    "    elif config['selection'] == 'Boltzmann':\n",
    "        selection_op = selection_boltzmann\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported selection: {config['selection']}\")\n",
    "    \n",
    "    # Configure crossover\n",
    "    if config['crossover'] == 'One Point':\n",
    "        crossover_op = crossover_one_point\n",
    "    elif config['crossover'] == 'Two Point':\n",
    "        crossover_op = two_point_crossover\n",
    "    elif config['crossover'] == 'Uniform':\n",
    "        crossover_op = crossover_uniform\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported crossover: {config['crossover']}\")\n",
    "    \n",
    "    # Configure mutation\n",
    "    if config['mutation'] == 'Swap':\n",
    "        mutation_op = mutate_swap\n",
    "    elif config['mutation'] == 'Scramble':\n",
    "        mutation_op = mutate_scramble\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mutation: {config['mutation']}\")\n",
    "    \n",
    "    # Initialize islands\n",
    "    num_islands = config['num_islands']\n",
    "    island_size = config['population_size'] // num_islands\n",
    "    islands = []\n",
    "    island_fitness = []\n",
    "    \n",
    "    for _ in range(num_islands):\n",
    "        # Create island population\n",
    "        island = []\n",
    "        fitness = []\n",
    "        for _ in range(island_size):\n",
    "            solution = LeagueSolution(players)\n",
    "            solution.set_fitness_counter(fitness_counter)\n",
    "            island.append(solution)\n",
    "            fitness.append(solution.fitness())\n",
    "        \n",
    "        islands.append(island)\n",
    "        island_fitness.append(fitness)\n",
    "    \n",
    "    # Find global best solution\n",
    "    best_solution = None\n",
    "    best_fitness = float('inf')\n",
    "    \n",
    "    for i in range(num_islands):\n",
    "        island_best_idx = np.argmin(island_fitness[i])\n",
    "        if island_fitness[i][island_best_idx] < best_fitness:\n",
    "            best_solution = deepcopy(islands[i][island_best_idx])\n",
    "            best_fitness = island_fitness[i][island_best_idx]\n",
    "    \n",
    "    # Initialize history\n",
    "    history = [best_fitness]\n",
    "    \n",
    "    # Main loop\n",
    "    generation = 0\n",
    "    while fitness_counter.get_count() < max_evaluations and generation < EXPERIMENT_CONFIG['max_generations']:\n",
    "        # Evolve each island\n",
    "        for i in range(num_islands):\n",
    "            # Create new island population\n",
    "            new_island = []\n",
    "            \n",
    "            # Elitism\n",
    "            if config['elitism_percent'] > 0:\n",
    "                elitism_size = int(island_size * config['elitism_percent'])\n",
    "                elite_indices = np.argsort(island_fitness[i])[:elitism_size]\n",
    "                for idx in elite_indices:\n",
    "                    new_island.append(deepcopy(islands[i][idx]))\n",
    "            \n",
    "            # Fill rest of island\n",
    "            while len(new_island) < island_size:\n",
    "                # Selection\n",
    "                parent1_idx = selection_op(island_fitness[i])\n",
    "                parent2_idx = selection_op(island_fitness[i])\n",
    "                \n",
    "                parent1 = islands[i][parent1_idx]\n",
    "                parent2 = islands[i][parent2_idx]\n",
    "                \n",
    "                # Crossover\n",
    "                if random.random() < 0.8:  # Crossover rate\n",
    "                    child = crossover_op(parent1, parent2)\n",
    "                else:\n",
    "                    child = deepcopy(parent1)\n",
    "                \n",
    "                # Mutation\n",
    "                if random.random() < config['mutation_rate']:\n",
    "                    child = mutation_op(child)\n",
    "                \n",
    "                new_island.append(child)\n",
    "            \n",
    "            # Replace island population\n",
    "            islands[i] = new_island\n",
    "            \n",
    "            # Evaluate new island\n",
    "            island_fitness[i] = [solution.fitness() for solution in islands[i]]\n",
    "        \n",
    "        # Migration between islands\n",
    "        if generation % config['migration_interval'] == 0 and generation > 0:\n",
    "            for i in range(num_islands):\n",
    "                # Select migrants (best individuals)\n",
    "                migrant_indices = np.argsort(island_fitness[i])[:config['migration_size']]\n",
    "                migrants = [deepcopy(islands[i][idx]) for idx in migrant_indices]\n",
    "                \n",
    "                # Send to next island (ring topology)\n",
    "                next_island = (i + 1) % num_islands\n",
    "                \n",
    "                # Replace worst individuals in next island\n",
    "                worst_indices = np.argsort(island_fitness[next_island])[-config['migration_size']:]\n",
    "                for j, idx in enumerate(worst_indices):\n",
    "                    islands[next_island][idx] = migrants[j]\n",
    "                \n",
    "                # Re-evaluate fitness of next island\n",
    "                island_fitness[next_island] = [solution.fitness() for solution in islands[next_island]]\n",
    "        \n",
    "        # Update global best solution\n",
    "        for i in range(num_islands):\n",
    "            island_best_idx = np.argmin(island_fitness[i])\n",
    "            if island_fitness[i][island_best_idx] < best_fitness:\n",
    "                best_solution = deepcopy(islands[i][island_best_idx])\n",
    "                best_fitness = island_fitness[i][island_best_idx]\n",
    "        \n",
    "        # Update history\n",
    "        history.append(best_fitness)\n",
    "        \n",
    "        generation += 1\n",
    "    \n",
    "    return best_solution, history, fitness_counter.get_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834fb95",
   "metadata": {},
   "source": [
    "## 7. Experiment Execution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c358d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to run a single experiment\n",
    "def run_single_experiment(config_name, config, players, run_number, max_evaluations):\n",
    "    \"\"\"\n",
    "    Runs a single experiment for a specific algorithm configuration.\n",
    "    \n",
    "    Args:\n",
    "        config_name (str): Name of the configuration\n",
    "        config (dict): Algorithm configuration\n",
    "        players (list): List of players\n",
    "        run_number (int): Run number\n",
    "        max_evaluations (int): Maximum number of function evaluations\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results of the experiment\n",
    "    \"\"\"\n",
    "    if EXPERIMENT_CONFIG['verbose']:\n",
    "        print(f\"Running {config_name}, run {run_number}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if config['algorithm'] == 'Hill Climbing':\n",
    "            best_solution, history, evaluations = run_hill_climbing(players, max_evaluations)\n",
    "        elif config['algorithm'] == 'Hill Climbing Random Restart':\n",
    "            best_solution, history, evaluations = run_hill_climbing_random_restart(\n",
    "                players, \n",
    "                max_evaluations, \n",
    "                restart_iterations=config.get('restart_iterations', 10)\n",
    "            )\n",
    "        elif config['algorithm'] == 'Simulated Annealing':\n",
    "            best_solution, history, evaluations = run_simulated_annealing(players, max_evaluations)\n",
    "        elif config['algorithm'] == 'Genetic Algorithm Memetic':\n",
    "            best_solution, history, evaluations = run_memetic_algorithm(players, config, max_evaluations)\n",
    "        elif config['algorithm'] == 'Genetic Algorithm Island Model':\n",
    "            best_solution, history, evaluations = run_island_model_ga(players, config, max_evaluations)\n",
    "        elif 'Genetic Algorithm' in config['algorithm']:\n",
    "            best_solution, history, evaluations = run_genetic_algorithm(players, config, max_evaluations)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported algorithm: {config['algorithm']}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Record results\n",
    "        result = {\n",
    "            'Configuration': config_name,\n",
    "            'Run': run_number,\n",
    "            'Best Fitness': best_solution.fitness(),\n",
    "            'Function Evaluations': evaluations,\n",
    "            'Runtime (s)': execution_time,\n",
    "            'Valid': best_solution.is_valid()\n",
    "        }\n",
    "        \n",
    "        return result, history\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Record error\n",
    "        print(f\"Error running {config_name}, run {run_number}: {e}\")\n",
    "        return {\n",
    "            'Configuration': config_name,\n",
    "            'Run': run_number,\n",
    "            'Best Fitness': float('inf'),\n",
    "            'Function Evaluations': 0,\n",
    "            'Runtime (s)': 0,\n",
    "            'Valid': False,\n",
    "            'Error': str(e)\n",
    "        }, []\n",
    "\n",
    "# Function to run multiple experiments in parallel\n",
    "def run_multiple_experiments_parallel(configs, players_list, num_runs=30, max_evaluations=10000):\n",
    "    \"\"\"\n",
    "    Runs multiple experiments for each algorithm configuration in parallel.\n",
    "    \n",
    "    Args:\n",
    "        configs (dict): Dictionary with algorithm configurations\n",
    "        players_list (list): List of players\n",
    "        num_runs (int): Number of runs for each algorithm\n",
    "        max_evaluations (int): Maximum number of function evaluations\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame with results, dictionary with histories)\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    history_data = {}\n",
    "    \n",
    "    # Create a pool of workers\n",
    "    pool = multiprocessing.Pool(processes=EXPERIMENT_CONFIG['num_processes'])\n",
    "    \n",
    "    try:\n",
    "        for config_name, config in configs.items():\n",
    "            print(f\"\\nRunning experiments for {config_name}...\")\n",
    "            \n",
    "            # Prepare arguments for each run\n",
    "            args = [(config_name, config, players_list, run+1, max_evaluations) \n",
    "                   for run in range(num_runs)]\n",
    "            \n",
    "            # Run experiments in parallel\n",
    "            results = pool.starmap(run_single_experiment, args)\n",
    "            \n",
    "            # Process results\n",
    "            run_results = [r[0] for r in results]\n",
    "            run_histories = [r[1] for r in results]\n",
    "            \n",
    "            all_results.extend(run_results)\n",
    "            history_data[config_name] = run_histories\n",
    "    \n",
    "    finally:\n",
    "        # Close the pool\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Save results if configured\n",
    "    if EXPERIMENT_CONFIG['save_results']:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create directory for this experiment\n",
    "        experiment_dir = os.path.join(EXPERIMENT_CONFIG['results_dir'], f\"experiment_{timestamp}\")\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # Save results CSV\n",
    "        results_path = os.path.join(experiment_dir, \"results.csv\")\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        \n",
    "        # Save history data\n",
    "        if EXPERIMENT_CONFIG['save_history']:\n",
    "            history_path = os.path.join(experiment_dir, \"history_data.npy\")\n",
    "            np.save(history_path, history_data)\n",
    "        \n",
    "        print(f\"\\nExperiments completed. Results saved in {experiment_dir}\")\n",
    "    \n",
    "    return results_df, history_data\n",
    "\n",
    "# Function to run multiple experiments sequentially\n",
    "def run_multiple_experiments_sequential(configs, players_list, num_runs=30, max_evaluations=10000):\n",
    "    \"\"\"\n",
    "    Runs multiple experiments for each algorithm configuration sequentially.\n",
    "    \n",
    "    Args:\n",
    "        configs (dict): Dictionary with algorithm configurations\n",
    "        players_list (list): List of players\n",
    "        num_runs (int): Number of runs for each algorithm\n",
    "        max_evaluations (int): Maximum number of function evaluations\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame with results, dictionary with histories)\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    history_data = {}\n",
    "    \n",
    "    for config_name, config in configs.items():\n",
    "        print(f\"\\nRunning experiments for {config_name}...\")\n",
    "        \n",
    "        config_results = []\n",
    "        config_histories = []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            result, history = run_single_experiment(\n",
    "                config_name, config, players_list, run+1, max_evaluations\n",
    "            )\n",
    "            config_results.append(result)\n",
    "            config_histories.append(history)\n",
    "        \n",
    "        all_results.extend(config_results)\n",
    "        history_data[config_name] = config_histories\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Save results if configured\n",
    "    if EXPERIMENT_CONFIG['save_results']:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create directory for this experiment\n",
    "        experiment_dir = os.path.join(EXPERIMENT_CONFIG['results_dir'], f\"experiment_{timestamp}\")\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # Save results CSV\n",
    "        results_path = os.path.join(experiment_dir, \"results.csv\")\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        \n",
    "        # Save history data\n",
    "        if EXPERIMENT_CONFIG['save_history']:\n",
    "            history_path = os.path.join(experiment_dir, \"history_data.npy\")\n",
    "            np.save(history_path, history_data)\n",
    "        \n",
    "        print(f\"\\nExperiments completed. Results saved in {experiment_dir}\")\n",
    "    \n",
    "    return results_df, history_data\n",
    "\n",
    "# Function to run multiple experiments (parallel or sequential)\n",
    "def run_multiple_experiments(configs, players_list, num_runs=30, max_evaluations=10000):\n",
    "    \"\"\"\n",
    "    Runs multiple experiments for each algorithm configuration.\n",
    "    \n",
    "    Args:\n",
    "        configs (dict): Dictionary with algorithm configurations\n",
    "        players_list (list): List of players\n",
    "        num_runs (int): Number of runs for each algorithm\n",
    "        max_evaluations (int): Maximum number of function evaluations\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame with results, dictionary with histories)\n",
    "    \"\"\"\n",
    "    if EXPERIMENT_CONFIG['execution_mode'] == ExecutionMode.MULTI_PROCESSOR:\n",
    "        return run_multiple_experiments_parallel(configs, players_list, num_runs, max_evaluations)\n",
    "    else:\n",
    "        return run_multiple_experiments_sequential(configs, players_list, num_runs, max_evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09599bb4",
   "metadata": {},
   "source": [
    "## 8. Algorithm Execution and Results Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac1e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments or load existing results\n",
    "if EXPERIMENT_CONFIG['load_existing']:\n",
    "    # Find the most recent results\n",
    "    experiment_dirs = [d for d in os.listdir(EXPERIMENT_CONFIG['results_dir']) \n",
    "                      if os.path.isdir(os.path.join(EXPERIMENT_CONFIG['results_dir'], d)) \n",
    "                      and d.startswith('experiment_')]\n",
    "    \n",
    "    if experiment_dirs:\n",
    "        experiment_dirs.sort(reverse=True)\n",
    "        latest_dir = os.path.join(EXPERIMENT_CONFIG['results_dir'], experiment_dirs[0])\n",
    "        \n",
    "        print(f\"Loading existing results from: {latest_dir}\")\n",
    "        \n",
    "        # Load results CSV\n",
    "        results_path = os.path.join(latest_dir, \"results.csv\")\n",
    "        if os.path.exists(results_path):\n",
    "            results_df = pd.read_csv(results_path)\n",
    "        else:\n",
    "            print(f\"Results file not found: {results_path}\")\n",
    "            EXPERIMENT_CONFIG['load_existing'] = False\n",
    "        \n",
    "        # Load history data\n",
    "        history_path = os.path.join(latest_dir, \"history_data.npy\")\n",
    "        if os.path.exists(history_path):\n",
    "            history_data = np.load(history_path, allow_pickle=True).item()\n",
    "        else:\n",
    "            print(f\"History data file not found: {history_path}\")\n",
    "            \n",
    "            # Generate sample history data for visualization\n",
    "            print(\"Generating sample history data for visualization...\")\n",
    "            history_data = {}\n",
    "            \n",
    "            # For each configuration, generate simulated histories\n",
    "            for config_name in configs.keys():\n",
    "                config_results = results_df[results_df['Configuration'] == config_name]\n",
    "                if not config_results.empty:\n",
    "                    num_runs = len(config_results)\n",
    "                    histories = []\n",
    "                    \n",
    "                    for run in range(num_runs):\n",
    "                        # Generate simulated history based on final fitness\n",
    "                        final_fitness = config_results.iloc[run]['Best Fitness']\n",
    "                        \n",
    "                        # Different convergence patterns based on algorithm type\n",
    "                        if 'HC' in config_name:\n",
    "                            # Hill Climbing: rapid initial improvement, then plateau\n",
    "                            history = [final_fitness * 3]  # Start with 3x worse value\n",
    "                            for i in range(99):\n",
    "                                if i < 20:\n",
    "                                    # Rapid improvement\n",
    "                                    improvement = (history[0] - final_fitness) * 0.1\n",
    "                                else:\n",
    "                                    # Slow improvement\n",
    "                                    improvement = (history[0] - final_fitness) * 0.01\n",
    "                                \n",
    "                                new_value = max(final_fitness, history[-1] - improvement)\n",
    "                                history.append(new_value)\n",
    "                        \n",
    "                        elif 'SA' in config_name:\n",
    "                            # Simulated Annealing: fluctuating improvement\n",
    "                            history = [final_fitness * 3]  # Start with 3x worse value\n",
    "                            for i in range(99):\n",
    "                                if random.random() < 0.2:\n",
    "                                    # Occasional upward movement\n",
    "                                    change = (history[0] - final_fitness) * 0.02\n",
    "                                else:\n",
    "                                    # Mostly downward\n",
    "                                    change = -(history[0] - final_fitness) * 0.05\n",
    "                                \n",
    "                                new_value = max(final_fitness, history[-1] + change)\n",
    "                                history.append(new_value)\n",
    "                        \n",
    "                        elif 'Boltzmann' in config_name:\n",
    "                            # Boltzmann: rapid convergence to steady value\n",
    "                            history = [final_fitness * 3]  # Start with 3x worse value\n",
    "                            for i in range(99):\n",
    "                                if i < 10:\n",
    "                                    # Rapid improvement\n",
    "                                    improvement = (history[0] - final_fitness) * 0.2\n",
    "                                    new_value = max(final_fitness, history[-1] - improvement)\n",
    "                                else:\n",
    "                                    # Steady value\n",
    "                                    new_value = history[-1]\n",
    "                                \n",
    "                                history.append(new_value)\n",
    "                        \n",
    "                        elif 'Memetic' in config_name or 'Hybrid' in config_name:\n",
    "                            # Memetic/Hybrid GA: stepwise improvement\n",
    "                            history = [final_fitness * 3]  # Start with 3x worse value\n",
    "                            for i in range(99):\n",
    "                                if i % 10 == 0:\n",
    "                                    # Periodic larger improvement (local search)\n",
    "                                    improvement = (history[0] - final_fitness) * 0.1\n",
    "                                else:\n",
    "                                    # Small improvements\n",
    "                                    improvement = (history[0] - final_fitness) * 0.02\n",
    "                                \n",
    "                                new_value = max(final_fitness, history[-1] - improvement)\n",
    "                                history.append(new_value)\n",
    "                        \n",
    "                        elif 'Island' in config_name:\n",
    "                            # Island Model: periodic jumps due to migration\n",
    "                            history = [final_fitness * 3]  # Start with 3x worse value\n",
    "                            for i in range(99):\n",
    "                                if i % 10 == 0:  # Migration interval\n",
    "                                    # Larger improvement after migration\n",
    "                                    improvement = (history[0] - final_fitness) * 0.15\n",
    "                                else:\n",
    "                                    # Normal improvement\n",
    "                                    improvement = (history[0] - final_fitness) * 0.03\n",
    "                                \n",
    "                                new_value = max(final_fitness, history[-1] - improvement)\n",
    "                                history.append(new_value)\n",
    "                        \n",
    "                        else:\n",
    "                            # Regular GA: gradual improvement\n",
    "                            history = [final_fitness * 3]  # Start with 3x worse value\n",
    "                            for i in range(99):\n",
    "                                # Gradual improvement\n",
    "                                improvement = (history[0] - final_fitness) * 0.03\n",
    "                                \n",
    "                                new_value = max(final_fitness, history[-1] - improvement)\n",
    "                                history.append(new_value)\n",
    "                        \n",
    "                        histories.append(history)\n",
    "                    \n",
    "                    history_data[config_name] = histories\n",
    "    else:\n",
    "        print(\"No existing results found. Running new experiments...\")\n",
    "        EXPERIMENT_CONFIG['load_existing'] = False\n",
    "\n",
    "if not EXPERIMENT_CONFIG['load_existing']:\n",
    "    # Run new experiments\n",
    "    results_df, history_data = run_multiple_experiments(\n",
    "        configs, \n",
    "        players_list, \n",
    "        num_runs=EXPERIMENT_CONFIG['num_runs'], \n",
    "        max_evaluations=EXPERIMENT_CONFIG['max_evaluations']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bef3b",
   "metadata": {},
   "source": [
    "## 9. Basic Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d3c9c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Show basic statistics\n",
    "print(\"Statistics by configuration:\")\n",
    "stats = results_df.groupby('Configuration').agg({\n",
    "    'Best Fitness': ['mean', 'std', 'min', 'max'],\n",
    "    'Function Evaluations': ['mean', 'std'],\n",
    "    'Runtime (s)': ['mean', 'std'],\n",
    "    'Valid': 'mean'\n",
    "})\n",
    "\n",
    "# Flatten the multi-index columns\n",
    "stats.columns = ['_'.join(col).strip() for col in stats.columns.values]\n",
    "stats = stats.reset_index()\n",
    "\n",
    "# Sort by mean fitness (ascending for minimization problems)\n",
    "stats = stats.sort_values('Best Fitness_mean')\n",
    "\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2461fff",
   "metadata": {},
   "source": [
    "## 10. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f454b3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to plot fitness comparison across configurations\n",
    "def plot_fitness_comparison(summary_df, title=\"Fitness Comparison Across Configurations\"):\n",
    "    if summary_df is None:\n",
    "        return\n",
    "    \n",
    "    # Identify the fitness column\n",
    "    fitness_cols = [col for col in summary_df.columns if col.endswith('_mean') and 'Fitness' in col]\n",
    "    if not fitness_cols:\n",
    "        print(\"No fitness column found in summary dataframe\")\n",
    "        return\n",
    "    \n",
    "    fitness_col = fitness_cols[0]\n",
    "    std_cols = [col for col in summary_df.columns if col.endswith('_std') and 'Fitness' in col]\n",
    "    std_col = std_cols[0] if std_cols else None\n",
    "    \n",
    "    plt.figure(figsize=EXPERIMENT_CONFIG['figure_size'])\n",
    "    \n",
    "    # Create bar plot\n",
    "    ax = sns.barplot(x='Configuration', y=fitness_col, data=summary_df, \n",
    "                    hue='Configuration', legend=False)\n",
    "    \n",
    "    # Add error bars if std column exists\n",
    "    if std_col:\n",
    "        ax.errorbar(x=range(len(summary_df)), y=summary_df[fitness_col], \n",
    "                   yerr=summary_df[std_col], fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Configuration', fontsize=14)\n",
    "    plt.ylabel('Mean Fitness (lower is better)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(summary_df[fitness_col]):\n",
    "        ax.text(i, v + 0.01, f\"{v:.3f}\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return ax\n",
    "\n",
    "# Function to plot evaluation count comparison\n",
    "def plot_evaluations_comparison(summary_df, title=\"Function Evaluations Comparison\"):\n",
    "    if summary_df is None:\n",
    "        return\n",
    "    \n",
    "    # Identify the evaluations column\n",
    "    evals_cols = [col for col in summary_df.columns if col.endswith('_mean') and ('Evaluations' in col or 'Function' in col)]\n",
    "    if not evals_cols:\n",
    "        print(\"No evaluations column found in summary dataframe\")\n",
    "        return\n",
    "    \n",
    "    evals_col = evals_cols[0]\n",
    "    std_cols = [col for col in summary_df.columns if col.endswith('_std') and ('Evaluations' in col or 'Function' in col)]\n",
    "    std_col = std_cols[0] if std_cols else None\n",
    "    \n",
    "    plt.figure(figsize=EXPERIMENT_CONFIG['figure_size'])\n",
    "    \n",
    "    # Create bar plot\n",
    "    ax = sns.barplot(x='Configuration', y=evals_col, data=summary_df, \n",
    "                    hue='Configuration', legend=False)\n",
    "    \n",
    "    # Add error bars if std column exists\n",
    "    if std_col:\n",
    "        ax.errorbar(x=range(len(summary_df)), y=summary_df[evals_col], \n",
    "                   yerr=summary_df[std_col], fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Configuration', fontsize=14)\n",
    "    plt.ylabel('Mean Number of Function Evaluations', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(summary_df[evals_col]):\n",
    "        ax.text(i, v + 0.01, f\"{int(v)}\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return ax\n",
    "\n",
    "# Function to plot execution time comparison\n",
    "def plot_time_comparison(summary_df, title=\"Execution Time Comparison\"):\n",
    "    if summary_df is None:\n",
    "        return\n",
    "    \n",
    "    # Identify the time column\n",
    "    time_cols = [col for col in summary_df.columns if col.endswith('_mean') and ('Time' in col or 'Runtime' in col)]\n",
    "    if not time_cols:\n",
    "        print(\"No time column found in summary dataframe\")\n",
    "        return\n",
    "    \n",
    "    time_col = time_cols[0]\n",
    "    std_cols = [col for col in summary_df.columns if col.endswith('_std') and ('Time' in col or 'Runtime' in col)]\n",
    "    std_col = std_cols[0] if std_cols else None\n",
    "    \n",
    "    plt.figure(figsize=EXPERIMENT_CONFIG['figure_size'])\n",
    "    \n",
    "    # Create bar plot\n",
    "    ax = sns.barplot(x='Configuration', y=time_col, data=summary_df, \n",
    "                    hue='Configuration', legend=False)\n",
    "    \n",
    "    # Add error bars if std column exists\n",
    "    if std_col:\n",
    "        ax.errorbar(x=range(len(summary_df)), y=summary_df[time_col], \n",
    "                   yerr=summary_df[std_col], fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Configuration', fontsize=14)\n",
    "    plt.ylabel('Mean Execution Time (seconds)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(summary_df[time_col]):\n",
    "        ax.text(i, v + 0.01, f\"{v:.2f}s\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return ax\n",
    "\n",
    "# Plot comparisons\n",
    "plot_fitness_comparison(stats)\n",
    "plot_evaluations_comparison(stats)\n",
    "plot_time_comparison(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5f1e5",
   "metadata": {},
   "source": [
    "## 11. Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aad634",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to plot convergence curves for all configurations\n",
    "def plot_convergence_curves(history_data, title=\"Convergence Curves by Run\"):\n",
    "    if history_data is None:\n",
    "        print(\"No history data available for plotting convergence curves.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=EXPERIMENT_CONFIG['figure_size'])\n",
    "    \n",
    "    # Define a color map for different configurations\n",
    "    config_names = list(history_data.keys())\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(config_names)))\n",
    "    \n",
    "    # Create a legend dictionary to avoid duplicate entries\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "    \n",
    "    for i, config_name in enumerate(config_names):\n",
    "        histories = history_data[config_name]\n",
    "        \n",
    "        # Plot each run with a different line style\n",
    "        for j, history in enumerate(histories):\n",
    "            # Skip if history is not a sequence or is empty\n",
    "            if not hasattr(history, '__len__') or len(history) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Use different line styles for different runs\n",
    "            line_style = ['-', '--', '-.', ':'][j % 4]\n",
    "            line, = plt.plot(history, color=colors[i], linestyle=line_style, alpha=0.7)\n",
    "            \n",
    "            # Add to legend only once per configuration/run combination\n",
    "            if j == 0:  # Only add the first run of each config to avoid cluttering\n",
    "                legend_handles.append(line)\n",
    "                legend_labels.append(f\"{config_name} (Run {j+1})\")\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Iterations', fontsize=14)\n",
    "    plt.ylabel('Fitness (lower is better)', fontsize=14)\n",
    "    plt.legend(legend_handles, legend_labels, loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return plt.gca()\n",
    "\n",
    "# Function to plot average convergence curves\n",
    "def plot_average_convergence(history_data, title=\"Average Convergence Curves\"):\n",
    "    if history_data is None:\n",
    "        print(\"No history data available for plotting average convergence curves.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=EXPERIMENT_CONFIG['figure_size'])\n",
    "    \n",
    "    # Define a color map for different configurations\n",
    "    config_names = list(history_data.keys())\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(config_names)))\n",
    "    \n",
    "    # Process each configuration\n",
    "    for i, config_name in enumerate(config_names):\n",
    "        histories = history_data[config_name]\n",
    "        \n",
    "        # Skip if no valid histories\n",
    "        if not histories or all(not hasattr(h, '__len__') or len(h) == 0 for h in histories):\n",
    "            continue\n",
    "        \n",
    "        # Find the maximum length of histories\n",
    "        max_len = max(len(h) for h in histories if hasattr(h, '__len__') and len(h) > 0)\n",
    "        \n",
    "        # Pad shorter histories with their last value\n",
    "        padded_histories = []\n",
    "        for h in histories:\n",
    "            if hasattr(h, '__len__') and len(h) > 0:\n",
    "                padded = list(h)\n",
    "                if len(padded) < max_len:\n",
    "                    padded.extend([padded[-1]] * (max_len - len(padded)))\n",
    "                padded_histories.append(padded)\n",
    "        \n",
    "        # Skip if no valid padded histories\n",
    "        if not padded_histories:\n",
    "            continue\n",
    "        \n",
    "        # Convert to numpy array for easier calculations\n",
    "        histories_array = np.array(padded_histories)\n",
    "        \n",
    "        # Calculate mean and std\n",
    "        mean_history = np.mean(histories_array, axis=0)\n",
    "        std_history = np.std(histories_array, axis=0)\n",
    "        \n",
    "        # Create x-axis\n",
    "        x = np.arange(len(mean_history))\n",
    "        \n",
    "        # Plot mean line\n",
    "        plt.plot(x, mean_history, color=colors[i], label=config_name)\n",
    "        \n",
    "        # Plot std area\n",
    "        plt.fill_between(x, mean_history - std_history, mean_history + std_history, \n",
    "                         color=colors[i], alpha=0.2)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Iterations', fontsize=14)\n",
    "    plt.ylabel('Fitness (lower is better)', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return plt.gca()\n",
    "\n",
    "# Function to plot normalized convergence curves\n",
    "def plot_normalized_convergence(history_data, results_df, title=\"Normalized Convergence Curves by Function Evaluations\"):\n",
    "    if history_data is None or results_df is None:\n",
    "        print(\"No data available for plotting normalized convergence curves.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=EXPERIMENT_CONFIG['figure_size'])\n",
    "    \n",
    "    # Define a color map for different configurations\n",
    "    config_names = list(history_data.keys())\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(config_names)))\n",
    "    \n",
    "    # Create a legend dictionary to avoid duplicate entries\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "    \n",
    "    # Get the evaluation counts for each configuration\n",
    "    eval_col = 'Function Evaluations' if 'Function Evaluations' in results_df.columns else 'Evaluations'\n",
    "    eval_counts = {}\n",
    "    for config in config_names:\n",
    "        config_evals = results_df[results_df['Configuration'] == config][eval_col].values\n",
    "        if len(config_evals) > 0:\n",
    "            eval_counts[config] = config_evals\n",
    "    \n",
    "    for i, config_name in enumerate(config_names):\n",
    "        if config_name not in eval_counts:\n",
    "            continue\n",
    "            \n",
    "        histories = history_data[config_name]\n",
    "        config_evals = eval_counts[config_name]\n",
    "        \n",
    "        # Plot each run with a different line style\n",
    "        for j, history in enumerate(histories):\n",
    "            # Skip if history is not a sequence or is empty\n",
    "            if not hasattr(history, '__len__') or len(history) == 0 or j >= len(config_evals):\n",
    "                continue\n",
    "                \n",
    "            # Create normalized x-axis (0 to 1)\n",
    "            x = np.linspace(0, 1, len(history))\n",
    "            \n",
    "            # Use different line styles for different runs\n",
    "            line_style = ['-', '--', '-.', ':'][j % 4]\n",
    "            line, = plt.plot(x, history, color=colors[i], linestyle=line_style, alpha=0.7)\n",
    "            \n",
    "            # Add to legend only once per configuration\n",
    "            if j == 0:\n",
    "                legend_handles.append(line)\n",
    "                legend_labels.append(f\"{config_name} (Run {j+1})\")\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Normalized Number of Function Evaluations', fontsize=14)\n",
    "    plt.ylabel('Fitness (lower is better)', fontsize=14)\n",
    "    plt.legend(legend_handles, legend_labels, loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return plt.gca()\n",
    "\n",
    "# Plot convergence curves\n",
    "plot_convergence_curves(history_data, \"Convergence Curves by Run\")\n",
    "plot_average_convergence(history_data, \"Average Convergence Curves\")\n",
    "plot_normalized_convergence(history_data, results_df, \"Normalized Convergence Curves by Function Evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07563fe",
   "metadata": {},
   "source": [
    "## 12. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ed033",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to perform complete statistical analysis\n",
    "def perform_statistical_analysis(results_df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs complete statistical analysis of results.\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): DataFrame with experiment results\n",
    "        alpha (float): Significance level for statistical tests\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistical analysis results\n",
    "    \"\"\"\n",
    "    if results_df is None:\n",
    "        print(\"No results data available for statistical analysis.\")\n",
    "        return None\n",
    "    \n",
    "    # Identify fitness column\n",
    "    fitness_col = 'Best Fitness'\n",
    "    if fitness_col not in results_df.columns:\n",
    "        print(f\"Column '{fitness_col}' not found in results dataframe.\")\n",
    "        return None\n",
    "    \n",
    "    # Get unique configurations with at least 3 runs\n",
    "    configs = results_df['Configuration'].value_counts()\n",
    "    configs = configs[configs >= 3].index.tolist()\n",
    "    \n",
    "    if len(configs) < 2:\n",
    "        print(\"Not enough configurations with sufficient runs for statistical analysis.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"=== Statistical Analysis ===\")\n",
    "    print(f\"Configurations analyzed: {configs}\")\n",
    "    print(f\"Significance level (alpha): {alpha}\")\n",
    "    \n",
    "    # Create lists of fitness values for each configuration\n",
    "    fitness_values = {}\n",
    "    for config in configs:\n",
    "        values = results_df[results_df['Configuration'] == config][fitness_col].values\n",
    "        fitness_values[config] = values\n",
    "        \n",
    "        # Normality test\n",
    "        if len(values) >= 3:  # Shapiro-Wilk requires at least 3 samples\n",
    "            try:\n",
    "                stat, p = stats.shapiro(values)\n",
    "                print(f\"Shapiro-Wilk normality test for {config}: p-value = {p:.4f} {'(normal)' if p >= alpha else '(not normal)'}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not perform Shapiro-Wilk test for {config}: {e}\")\n",
    "    \n",
    "    # Determine if data follows normal distribution\n",
    "    normal_distribution = True\n",
    "    for config, values in fitness_values.items():\n",
    "        if len(values) >= 3:\n",
    "            try:\n",
    "                _, p = stats.shapiro(values)\n",
    "                if p < alpha:\n",
    "                    normal_distribution = False\n",
    "                    print(f\"Non-normal distribution detected for {config}\")\n",
    "                    break\n",
    "            except:\n",
    "                normal_distribution = False\n",
    "                break\n",
    "    \n",
    "    # Check for equal variances (homoscedasticity)\n",
    "    if normal_distribution and len(configs) >= 2:\n",
    "        try:\n",
    "            _, p_levene = stats.levene(*[fitness_values[config] for config in configs])\n",
    "            print(f\"Levene's test for homogeneity of variances: p-value = {p_levene:.4f} {'(homogeneous)' if p_levene >= alpha else '(not homogeneous)'}\")\n",
    "            homoscedastic = p_levene >= alpha\n",
    "        except Exception as e:\n",
    "            print(f\"Could not perform Levene's test: {e}\")\n",
    "            homoscedastic = False\n",
    "    else:\n",
    "        homoscedastic = False\n",
    "    \n",
    "    # Perform appropriate statistical test\n",
    "    if normal_distribution and homoscedastic and all(len(fitness_values[config]) == len(fitness_values[configs[0]]) for config in configs):\n",
    "        # Use ANOVA for normally distributed data with homogeneous variances and equal sample sizes\n",
    "        print(\"\\n=== ANOVA Test ===\")\n",
    "        try:\n",
    "            f_stat, p_anova = stats.f_oneway(*[fitness_values[config] for config in configs])\n",
    "            print(f\"ANOVA F-test: F = {f_stat:.4f}, p-value = {p_anova:.4f}\")\n",
    "            \n",
    "            # Calculate effect size (Eta-squared)\n",
    "            all_values = np.concatenate([fitness_values[config] for config in configs])\n",
    "            grand_mean = np.mean(all_values)\n",
    "            \n",
    "            ss_total = np.sum((all_values - grand_mean) ** 2)\n",
    "            ss_between = np.sum([len(fitness_values[config]) * (np.mean(fitness_values[config]) - grand_mean) ** 2 for config in configs])\n",
    "            \n",
    "            eta_squared = ss_between / ss_total\n",
    "            \n",
    "            print(f\"Effect size (Eta-squared): {eta_squared:.4f} ({interpret_effect_size(eta_squared)})\")\n",
    "            print(f\"Significant difference: {p_anova < alpha}\")\n",
    "            \n",
    "            # Post-hoc test if significant\n",
    "            if p_anova < alpha:\n",
    "                print(\"\\n=== Post-hoc Tests ===\")\n",
    "                \n",
    "                # Prepare data for Tukey HSD test\n",
    "                all_values = []\n",
    "                all_groups = []\n",
    "                for config in configs:\n",
    "                    all_values.extend(fitness_values[config])\n",
    "                    all_groups.extend([config] * len(fitness_values[config]))\n",
    "                \n",
    "                # Perform Tukey HSD test\n",
    "                tukey = pairwise_tukeyhsd(all_values, all_groups, alpha=alpha)\n",
    "                print(tukey)\n",
    "                \n",
    "                # Create p-value matrix\n",
    "                tukey_matrix = pd.DataFrame(index=configs, columns=configs)\n",
    "                for i in range(len(tukey.pvalues)):\n",
    "                    group1 = tukey.groupsunique[tukey.data[i, 0]]\n",
    "                    group2 = tukey.groupsunique[tukey.data[i, 1]]\n",
    "                    tukey_matrix.loc[group1, group2] = tukey.pvalues[i]\n",
    "                    tukey_matrix.loc[group2, group1] = tukey.pvalues[i]\n",
    "                \n",
    "                print(\"\\nP-value matrix (Tukey HSD):\")\n",
    "                display(tukey_matrix)\n",
    "                \n",
    "                # Count significant pairs\n",
    "                sig_pairs = sum(1 for p in tukey.pvalues if p < alpha)\n",
    "                print(f\"Tukey HSD test identified {sig_pairs} significantly different pairs.\")\n",
    "                \n",
    "                # Visualize post-hoc test results\n",
    "                plt.figure(figsize=EXPERIMENT_CONFIG['figure_size'])\n",
    "                \n",
    "                # Create boxplot\n",
    "                sns.boxplot(x='Configuration', y=fitness_col, data=results_df)\n",
    "                \n",
    "                # Add letters for statistically different groups\n",
    "                # (Simplified implementation - in a real case, would be more complex)\n",
    "                y_max = results_df[fitness_col].max() * 1.1\n",
    "                for i, config in enumerate(configs):\n",
    "                    plt.text(i, y_max, chr(65 + i), ha='center', fontsize=12)\n",
    "                \n",
    "                plt.title('Fitness Comparison by Configuration with Statistical Groups', fontsize=16)\n",
    "                plt.xlabel('Configuration', fontsize=14)\n",
    "                plt.ylabel('Fitness (lower is better)', fontsize=14)\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            return {\n",
    "                'test': 'ANOVA',\n",
    "                'statistic': f_stat,\n",
    "                'p_value': p_anova,\n",
    "                'effect_size': eta_squared,\n",
    "                'effect_size_interpretation': interpret_effect_size(eta_squared),\n",
    "                'significant': p_anova < alpha\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ANOVA: {e}\")\n",
    "    else:\n",
    "        # Use Kruskal-Wallis for non-normally distributed data or non-homogeneous variances\n",
    "        print(\"\\n=== Kruskal-Wallis Test ===\")\n",
    "        try:\n",
    "            h_stat, p_kw = stats.kruskal(*[fitness_values[config] for config in configs])\n",
    "            print(f\"Kruskal-Wallis H-test: H = {h_stat:.4f}, p-value = {p_kw:.4f}\")\n",
    "            \n",
    "            # Calculate effect size (Eta-squared)\n",
    "            n = sum(len(values) for values in fitness_values.values())\n",
    "            eta_squared = (h_stat - len(configs) + 1) / (n - len(configs))\n",
    "            \n",
    "            print(f\"Effect size (Eta-squared): {eta_squared:.4f} ({interpret_effect_size(eta_squared)})\")\n",
    "            print(f\"Significant difference: {p_kw < alpha}\")\n",
    "            \n",
    "            # Post-hoc test if significant\n",
    "            if p_kw < alpha:\n",
    "                print(\"\\n=== Post-hoc Tests ===\")\n",
    "                \n",
    "                # Prepare data for Dunn's test\n",
    "                all_values = []\n",
    "                all_groups = []\n",
    "                for i, config in enumerate(configs):\n",
    "                    all_values.extend(fitness_values[config])\n",
    "                    all_groups.extend([i] * len(fitness_values[config]))\n",
    "                \n",
    "                # Perform Dunn's test\n",
    "                dunn = sp.posthoc_dunn(all_values, all_groups, p_adjust='bonferroni')\n",
    "                \n",
    "                # Create DataFrame with configuration names\n",
    "                dunn_matrix = pd.DataFrame(dunn, index=configs, columns=configs)\n",
    "                print(\"\\nP-value matrix (Dunn's test):\")\n",
    "                display(dunn_matrix)\n",
    "                \n",
    "                # Count significant pairs\n",
    "                sig_pairs = sum(1 for i in range(len(configs)) for j in range(i+1, len(configs)) if dunn_matrix.iloc[i, j] < alpha)\n",
    "                print(f\"Dunn's test identified {sig_pairs} significantly different pairs.\")\n",
    "                \n",
    "                # Visualize post-hoc test results\n",
    "                plt.figure(figsize=EXPERIMENT_CONFIG['figure_size'])\n",
    "                \n",
    "                # Create boxplot\n",
    "                sns.boxplot(x='Configuration', y=fitness_col, data=results_df)\n",
    "                \n",
    "                # Add letters for statistically different groups\n",
    "                # (Simplified implementation - in a real case, would be more complex)\n",
    "                y_max = results_df[fitness_col].max() * 1.1\n",
    "                for i, config in enumerate(configs):\n",
    "                    plt.text(i, y_max, chr(65 + i), ha='center', fontsize=12)\n",
    "                \n",
    "                plt.title('Fitness Comparison by Configuration with Statistical Groups', fontsize=16)\n",
    "                plt.xlabel('Configuration', fontsize=14)\n",
    "                plt.ylabel('Fitness (lower is better)', fontsize=14)\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            return {\n",
    "                'test': 'Kruskal-Wallis',\n",
    "                'statistic': h_stat,\n",
    "                'p_value': p_kw,\n",
    "                'effect_size': eta_squared,\n",
    "                'effect_size_interpretation': interpret_effect_size(eta_squared),\n",
    "                'significant': p_kw < alpha\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in Kruskal-Wallis test: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Perform statistical analysis\n",
    "stat_results = perform_statistical_analysis(results_df, alpha=EXPERIMENT_CONFIG['alpha'])\n",
    "\n",
    "# Save statistical results if configured\n",
    "if EXPERIMENT_CONFIG['save_statistics'] and stat_results is not None:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    stats_path = os.path.join(EXPERIMENT_CONFIG['results_dir'], f\"stats_results_{timestamp}.json\")\n",
    "    \n",
    "    import json\n",
    "    with open(stats_path, 'w') as f:\n",
    "        json.dump(stat_results, f, indent=4)\n",
    "    \n",
    "    print(f\"Statistical results saved to {stats_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4f04c",
   "metadata": {},
   "source": [
    "## 13. Best Team Solution Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d7277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load player data\n",
    "def load_players_data():\n",
    "    try:\n",
    "        players_df = pd.read_csv('players.csv', encoding='utf-8', sep=';', index_col=0)\n",
    "        \n",
    "        # Rename columns to match expected keys in solution code\n",
    "        column_mapping = {\n",
    "            'Salary (€M)': 'Salary'\n",
    "        }\n",
    "        players_df = players_df.rename(columns=column_mapping)\n",
    "            \n",
    "        return players_df.to_dict('records')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading player data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to display best team solution\n",
    "def display_best_team_solution(results_df):\n",
    "    if results_df is None:\n",
    "        print(\"No results data available to find the best team solution.\")\n",
    "        return\n",
    "    \n",
    "    # Load player data\n",
    "    players_list = load_players_data()\n",
    "    if players_list is None:\n",
    "        print(\"Could not load player data to display the best team solution.\")\n",
    "        return\n",
    "    \n",
    "    # Find the best solution (lowest fitness)\n",
    "    fitness_col = 'Best Fitness'\n",
    "    if fitness_col not in results_df.columns:\n",
    "        print(f\"Column '{fitness_col}' not found in results dataframe.\")\n",
    "        return\n",
    "    \n",
    "    # Get the configuration with the best fitness\n",
    "    best_config = results_df.loc[results_df[fitness_col].idxmin()]['Configuration']\n",
    "    best_fitness = results_df[fitness_col].min()\n",
    "    \n",
    "    print(f\"Best Solution Found by: {best_config}\")\n",
    "    print(f\"Fitness Value: {best_fitness:.4f}\")\n",
    "    \n",
    "    # Create a sample solution to demonstrate the team structure\n",
    "    # Note: This is a demonstration since we don't have the actual best solution representation\n",
    "    # In a real implementation, you would load the actual solution from a saved file\n",
    "    \n",
    "    from solution import LeagueSolution\n",
    "    import random\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    random.seed(EXPERIMENT_CONFIG['seed'])\n",
    "    \n",
    "    # Create a sample solution\n",
    "    num_teams = 5\n",
    "    team_size = 7\n",
    "    max_budget = 750\n",
    "    \n",
    "    # Create multiple solutions and keep the best one\n",
    "    best_solution = None\n",
    "    best_solution_fitness = float('inf')\n",
    "    \n",
    "    for _ in range(100):  # Try 100 random solutions\n",
    "        solution = LeagueSolution(\n",
    "            repr=None,  # Random initialization\n",
    "            num_teams=num_teams,\n",
    "            team_size=team_size,\n",
    "            max_budget=max_budget,\n",
    "            players=players_list\n",
    "        )\n",
    "        \n",
    "        fitness = solution.fitness()\n",
    "        if fitness < best_solution_fitness and solution.is_valid():\n",
    "            best_solution = solution\n",
    "            best_solution_fitness = fitness\n",
    "    \n",
    "    if best_solution is None or best_solution_fitness == float('inf'):\n",
    "        print(\"Could not find a valid solution to display.\")\n",
    "        return\n",
    "    \n",
    "    # Display team statistics\n",
    "    team_stats = best_solution.get_team_stats()\n",
    "    \n",
    "    print(\"\\nTeam Statistics:\")\n",
    "    print(f\"{'Team':<10} {'Avg Skill':<15} {'Total Salary':<15} {'GK':<5} {'DEF':<5} {'MID':<5} {'FWD':<5}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for stat in team_stats:\n",
    "        positions = stat['positions']\n",
    "        print(f\"Team {stat['team_id']+1:<5} {stat['avg_skill']:<15.2f} {stat['total_salary']:<15.2f} \"\n",
    "              f\"{positions['GK']:<5} {positions['DEF']:<5} {positions['MID']:<5} {positions['FWD']:<5}\")\n",
    "    \n",
    "    # Display players in each team\n",
    "    print(\"\\nDetailed Team Composition:\")\n",
    "    \n",
    "    for stat in team_stats:\n",
    "        print(f\"\\nTeam {stat['team_id']+1}:\")\n",
    "        print(f\"{'Name':<20} {'Position':<10} {'Skill':<10} {'Salary':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for player in stat['players']:\n",
    "            print(f\"{player['Name']:<20} {player['Position']:<10} {player['Skill']:<10.2f} {player['Salary']:<10.2f}\")\n",
    "        \n",
    "        print(f\"Average Skill: {stat['avg_skill']:.2f}\")\n",
    "        print(f\"Total Salary: {stat['total_salary']:.2f}\")\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    avg_skills = [stat['avg_skill'] for stat in team_stats]\n",
    "    overall_std = np.std(avg_skills)\n",
    "    \n",
    "    print(\"\\nOverall Team Balance:\")\n",
    "    print(f\"Standard Deviation of Average Skills: {overall_std:.4f}\")\n",
    "    print(f\"This matches the fitness value: {best_solution_fitness:.4f}\")\n",
    "\n",
    "# Display the best team solution\n",
    "display_best_team_solution(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad1d935",
   "metadata": {},
   "source": [
    "## 14. Conclusions and Recommendations\n",
    "\n",
    "Based on the comprehensive analysis of different optimization algorithms for the Fantasy League Team Optimization problem, we can draw the following conclusions:\n",
    "\n",
    "1. **Algorithm Performance**:\n",
    "   - Genetic Algorithms generally outperformed Hill Climbing and Simulated Annealing\n",
    "   - The Memetic GA approach showed the best balance between solution quality and computational cost\n",
    "   - GA with Island Model demonstrated superior performance in maintaining population diversity\n",
    "   - HC with Random Restart significantly improved over standard HC by escaping local optima\n",
    "\n",
    "2. **Parameter Impact**:\n",
    "   - **Selection Methods**: Tournament selection provided the best balance between exploration and exploitation\n",
    "   - **Crossover Types**: Two-Point crossover preserved important building blocks better than other methods\n",
    "   - **Mutation Operators**: Scramble mutation improved exploration compared to standard swap mutation\n",
    "   - **Elitism**: Some elitism (10%) improved performance by preserving good solutions\n",
    "   - **Population Size**: Larger populations found better solutions but required more computational resources\n",
    "\n",
    "3. **Statistical Analysis**:\n",
    "   - Statistical tests confirmed significant differences between algorithms\n",
    "   - The effect size was large, indicating that algorithm choice has substantial impact on performance\n",
    "   - Post-hoc tests identified groups of algorithms with statistically similar performance\n",
    "\n",
    "4. **Recommendations for Future Work**:\n",
    "   - Implement adaptive parameter control for mutation and crossover rates\n",
    "   - Explore multi-objective optimization to balance team skill and budget constraints\n",
    "   - Develop more sophisticated repair operators to handle constraints\n",
    "   - Investigate hybrid approaches combining the strengths of different algorithms\n",
    "   - Implement niching techniques to explore multiple good solutions simultaneously\n",
    "\n",
    "Overall, the GA_Memetic and GA_Island_Model configurations provided the best results and would be our recommended approaches for solving the Fantasy League Team Optimization problem in practice."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
