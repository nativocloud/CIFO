{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da89bb23",
   "metadata": {},
   "source": [
    "# CIFO - Pipeline Completo: Execução e Análise de Algoritmos de Otimização\n",
    "\n",
    "Este notebook integra todo o processo desde a execução dos algoritmos até à visualização e análise dos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5178eb03",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente e Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a440f70",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configurar matplotlib para notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Suprimir avisos específicos\n",
    "warnings.filterwarnings(\"ignore\", message=\"scipy.stats.shapiro: Input data has range zero.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"No artists with labels found to put in legend.*\")\n",
    "\n",
    "# Importar módulos do projeto\n",
    "from solution import LeagueSolution, LeagueHillClimbingSolution\n",
    "from evolution import hill_climbing, simulated_annealing\n",
    "from operators import (\n",
    "    selection_tournament,\n",
    "    selection_ranking,\n",
    "    selection_boltzmann,\n",
    "    crossover_one_point,\n",
    "    crossover_uniform,\n",
    "    mutate_swap, \n",
    "    mutate_swap_constrained,\n",
    "    genetic_algorithm\n",
    ")\n",
    "from fitness_counter import FitnessCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947221d1",
   "metadata": {},
   "source": [
    "## 2. Implementação de Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0cc7713",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Implementar crossover de dois pontos (não existe em operators.py)\n",
    "def two_point_crossover(parent1, parent2):\n",
    "    \"\"\"\n",
    "    Two-point crossover: creates a child by taking portions from each parent.\n",
    "    \n",
    "    Args:\n",
    "        parent1 (LeagueSolution): First parent solution\n",
    "        parent2 (LeagueSolution): Second parent solution\n",
    "        \n",
    "    Returns:\n",
    "        LeagueSolution: A new solution created by crossover\n",
    "    \"\"\"\n",
    "    cut1 = random.randint(1, len(parent1.repr) - 3)\n",
    "    cut2 = random.randint(cut1 + 1, len(parent1.repr) - 2)\n",
    "    child_repr = parent1.repr[:cut1] + parent2.repr[cut1:cut2] + parent1.repr[cut2:]\n",
    "    \n",
    "    return LeagueSolution(\n",
    "        repr=child_repr,\n",
    "        num_teams=parent1.num_teams,\n",
    "        team_size=parent1.team_size,\n",
    "        max_budget=parent1.max_budget,\n",
    "        players=parent1.players\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801334b",
   "metadata": {},
   "source": [
    "## 3. Configuração dos Parâmetros de Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf9f7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar seed para reprodutibilidade\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configurar parâmetros gerais\n",
    "NUM_RUNS = 30  # Número de execuções para cada algoritmo\n",
    "MAX_EVALUATIONS = 10000  # Número máximo de avaliações de função\n",
    "POPULATION_SIZE = 100  # Tamanho da população para algoritmos genéticos\n",
    "MAX_GENERATIONS = 100  # Número máximo de gerações para algoritmos genéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf9c5b",
   "metadata": {},
   "source": [
    "## 4. Carregamento dos Dados dos Jogadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffa7de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados dos jogadores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Position</th>\n",
       "      <th>Skill</th>\n",
       "      <th>Salary (€M)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alex Carter</td>\n",
       "      <td>GK</td>\n",
       "      <td>85</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jordan Smith</td>\n",
       "      <td>GK</td>\n",
       "      <td>88</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ryan Mitchell</td>\n",
       "      <td>GK</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chris Thompson</td>\n",
       "      <td>GK</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blake Henderson</td>\n",
       "      <td>GK</td>\n",
       "      <td>87</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name Position  Skill  Salary (€M)\n",
       "0      Alex Carter       GK     85           90\n",
       "1     Jordan Smith       GK     88          100\n",
       "2    Ryan Mitchell       GK     83           85\n",
       "3   Chris Thompson       GK     80           80\n",
       "4  Blake Henderson       GK     87           95"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carregar dados dos jogadores\n",
    "players_df = pd.read_csv('players.csv', encoding='utf-8',sep=';', index_col=0)\n",
    "\n",
    "# Mostrar primeiras linhas\n",
    "print(\"Dados dos jogadores:\")\n",
    "display(players_df.head())\n",
    "\n",
    "# Renomear colunas para compatibilidade com o código\n",
    "column_mapping = {\n",
    "    'Salary (€M)': 'Salary'\n",
    "}\n",
    "players_df = players_df.rename(columns=column_mapping)\n",
    "\n",
    "# Converter DataFrame para lista de dicionários\n",
    "players_list = players_df.to_dict('records')\n",
    "\n",
    "# Configurar contador de fitness\n",
    "fitness_counter = FitnessCounter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889d6b6",
   "metadata": {},
   "source": [
    "## 5. Configuração dos Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34337a30",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurações dos algoritmos:\n",
      "\n",
      "HC_Standard:\n",
      "  algorithm: Hill Climbing\n",
      "\n",
      "SA_Standard:\n",
      "  algorithm: Simulated Annealing\n",
      "\n",
      "GA_Tournament_OnePoint:\n",
      "  algorithm: Genetic Algorithm\n",
      "  selection: Tournament\n",
      "  crossover: One Point\n",
      "  mutation_rate: 0.02857142857142857\n",
      "  elitism_percent: 0.1\n",
      "  population_size: 100\n",
      "  use_valid_initial: False\n",
      "  use_repair: False\n",
      "\n",
      "GA_Tournament_TwoPoint:\n",
      "  algorithm: Genetic Algorithm\n",
      "  selection: Tournament\n",
      "  crossover: Two Point\n",
      "  mutation_rate: 0.02857142857142857\n",
      "  elitism_percent: 0.1\n",
      "  population_size: 100\n",
      "  use_valid_initial: False\n",
      "  use_repair: False\n",
      "\n",
      "GA_Rank_Uniform:\n",
      "  algorithm: Genetic Algorithm\n",
      "  selection: Rank\n",
      "  crossover: Uniform\n",
      "  mutation_rate: 0.02857142857142857\n",
      "  elitism_percent: 0.1\n",
      "  population_size: 100\n",
      "  use_valid_initial: False\n",
      "  use_repair: False\n",
      "\n",
      "GA_Boltzmann_TwoPoint:\n",
      "  algorithm: Genetic Algorithm\n",
      "  selection: Boltzmann\n",
      "  crossover: Two Point\n",
      "  mutation_rate: 0.02857142857142857\n",
      "  elitism_percent: 0.1\n",
      "  population_size: 100\n",
      "  use_valid_initial: False\n",
      "  use_repair: False\n",
      "\n",
      "GA_Hybrid:\n",
      "  algorithm: Genetic Algorithm Hybrid\n",
      "  selection: Tournament\n",
      "  crossover: Two Point\n",
      "  mutation_rate: 0.02857142857142857\n",
      "  elitism_percent: 0.1\n",
      "  population_size: 100\n",
      "  use_valid_initial: False\n",
      "  use_repair: False\n"
     ]
    }
   ],
   "source": [
    "# Configurar algoritmos\n",
    "configs = {\n",
    "    # Algoritmos base\n",
    "    'HC_Standard': {\n",
    "        'algorithm': 'Hill Climbing',\n",
    "    },\n",
    "    'SA_Standard': {\n",
    "        'algorithm': 'Simulated Annealing',\n",
    "    },\n",
    "    'GA_Tournament_OnePoint': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'selection': 'Tournament',\n",
    "        'crossover': 'One Point',\n",
    "        'mutation_rate': 1.0/35,  # 1.0/len(players)\n",
    "        'elitism_percent': 0.1,   # 10%\n",
    "        'population_size': 100,\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    'GA_Tournament_TwoPoint': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'selection': 'Tournament',\n",
    "        'crossover': 'Two Point',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': 100,\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    'GA_Rank_Uniform': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'selection': 'Rank',\n",
    "        'crossover': 'Uniform',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': 100,\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    'GA_Boltzmann_TwoPoint': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'selection': 'Boltzmann',\n",
    "        'crossover': 'Two Point',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': 100,\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    },\n",
    "    'GA_Hybrid': {\n",
    "        'algorithm': 'Genetic Algorithm Hybrid',\n",
    "        'selection': 'Tournament',\n",
    "        'crossover': 'Two Point',\n",
    "        'mutation_rate': 1.0/35,\n",
    "        'elitism_percent': 0.1,\n",
    "        'population_size': 100,\n",
    "        'use_valid_initial': False,\n",
    "        'use_repair': False,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Mostrar configurações\n",
    "print(\"Configurações dos algoritmos:\")\n",
    "for config_name, config in configs.items():\n",
    "    print(f\"\\n{config_name}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f10513b",
   "metadata": {},
   "source": [
    "## 6. Implementação dos Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5912596",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função para executar Hill Climbing\n",
    "def run_hill_climbing(players, max_evaluations):\n",
    "    solution = LeagueSolution(players)\n",
    "    \n",
    "    # Iniciar contagem de fitness\n",
    "    fitness_counter.reset()\n",
    "    solution.set_fitness_counter(fitness_counter)\n",
    "    \n",
    "    best_fitness = solution.fitness()\n",
    "    history = [best_fitness]\n",
    "    \n",
    "    while fitness_counter.get_count() < max_evaluations:\n",
    "        # Gerar vizinho\n",
    "        neighbor = deepcopy(solution)\n",
    "        idx = random.randint(0, len(neighbor.repr) - 1)\n",
    "        neighbor.repr[idx] = random.randint(0, neighbor.num_teams - 1)\n",
    "        \n",
    "        neighbor_fitness = neighbor.fitness()\n",
    "        \n",
    "        # Aceitar se melhor\n",
    "        if neighbor_fitness < best_fitness:  # Menor é melhor\n",
    "            solution = neighbor\n",
    "            best_fitness = neighbor_fitness\n",
    "        \n",
    "        history.append(best_fitness)\n",
    "    \n",
    "    return solution, history, fitness_counter.get_count()\n",
    "\n",
    "# Função para executar Simulated Annealing\n",
    "def run_simulated_annealing(players, max_evaluations):\n",
    "    solution = LeagueSolution(players)\n",
    "    \n",
    "    # Iniciar contagem de fitness\n",
    "    fitness_counter.reset()\n",
    "    solution.set_fitness_counter(fitness_counter)\n",
    "    \n",
    "    best_solution = deepcopy(solution)\n",
    "    current_fitness = solution.fitness()\n",
    "    best_fitness = current_fitness\n",
    "    \n",
    "    history = [best_fitness]\n",
    "    \n",
    "    # Parâmetros do SA\n",
    "    initial_temp = 100.0\n",
    "    final_temp = 0.1\n",
    "    alpha = 0.95\n",
    "    \n",
    "    current_temp = initial_temp\n",
    "    \n",
    "    while fitness_counter.get_count() < max_evaluations and current_temp > final_temp:\n",
    "        # Gerar vizinho\n",
    "        neighbor = deepcopy(solution)\n",
    "        idx = random.randint(0, len(neighbor.repr) - 1)\n",
    "        neighbor.repr[idx] = random.randint(0, neighbor.num_teams - 1)\n",
    "        \n",
    "        neighbor_fitness = neighbor.fitness()\n",
    "        \n",
    "        # Calcular delta\n",
    "        delta = neighbor_fitness - current_fitness\n",
    "        \n",
    "        # Aceitar se melhor ou com probabilidade baseada na temperatura\n",
    "        if delta < 0 or random.random() < np.exp(-delta / current_temp):\n",
    "            solution = neighbor\n",
    "            current_fitness = neighbor_fitness\n",
    "            \n",
    "            # Atualizar melhor solução se necessário\n",
    "            if current_fitness < best_fitness:\n",
    "                best_solution = deepcopy(solution)\n",
    "                best_fitness = current_fitness\n",
    "        \n",
    "        history.append(best_fitness)\n",
    "        \n",
    "        # Resfriar\n",
    "        current_temp *= alpha\n",
    "    \n",
    "    return best_solution, history, fitness_counter.get_count()\n",
    "\n",
    "# Função para executar Genetic Algorithm\n",
    "def run_genetic_algorithm(players, config, max_evaluations):\n",
    "    # Iniciar contagem de fitness\n",
    "    fitness_counter.reset()\n",
    "    \n",
    "    # Configurar seleção\n",
    "    if config['selection'] == 'Tournament':\n",
    "        selection_op = selection_tournament\n",
    "    elif config['selection'] == 'Rank':\n",
    "        selection_op = selection_ranking\n",
    "    elif config['selection'] == 'Boltzmann':\n",
    "        selection_op = selection_boltzmann\n",
    "    else:\n",
    "        raise ValueError(f\"Seleção não suportada: {config['selection']}\")\n",
    "    \n",
    "    # Configurar crossover\n",
    "    if config['crossover'] == 'One Point':\n",
    "        crossover_op = crossover_one_point\n",
    "    elif config['crossover'] == 'Two Point':\n",
    "        crossover_op = two_point_crossover\n",
    "    elif config['crossover'] == 'Uniform':\n",
    "        crossover_op = crossover_uniform\n",
    "    else:\n",
    "        raise ValueError(f\"Crossover não suportado: {config['crossover']}\")\n",
    "    \n",
    "    # Configurar mutação\n",
    "    mutation_op = mutate_swap\n",
    "    \n",
    "    # Configurar operador de reparo (se necessário)\n",
    "    repair_op = None\n",
    "    if config.get('use_repair', False):\n",
    "        def repair_operator(solution):\n",
    "            # Implementação simples de reparo: tenta corrigir soluções inválidas\n",
    "            # ajustando a distribuição de jogadores por posição e orçamento\n",
    "            if solution.is_valid():\n",
    "                return solution\n",
    "            \n",
    "            # Obter estatísticas das equipes\n",
    "            teams = solution.get_teams()\n",
    "            \n",
    "            # Verificar e corrigir distribuição de posições\n",
    "            for team_idx, team in enumerate(teams):\n",
    "                positions = {\"GK\": 0, \"DEF\": 0, \"MID\": 0, \"FWD\": 0}\n",
    "                for player in team:\n",
    "                    positions[player[\"Position\"]] += 1\n",
    "                \n",
    "                # Se a distribuição estiver incorreta, tentar corrigir\n",
    "                if positions != {\"GK\": 1, \"DEF\": 2, \"MID\": 2, \"FWD\": 2}:\n",
    "                    # Implementação simplificada: apenas retorna a solução original\n",
    "                    # Uma implementação real seria mais complexa\n",
    "                    pass\n",
    "            \n",
    "            return solution\n",
    "        \n",
    "        repair_op = repair_operator\n",
    "    \n",
    "    # Configurar local search para GA híbrido\n",
    "    local_search = None\n",
    "    if config['algorithm'] == 'Genetic Algorithm Hybrid':\n",
    "        local_search = {\n",
    "            'operator': 'hill_climbing',\n",
    "            'probability': 0.1,\n",
    "            'iterations': 10\n",
    "        }\n",
    "    \n",
    "    # Executar GA\n",
    "    best_solution, best_fitness, history = genetic_algorithm(\n",
    "        players=players,\n",
    "        population_size=config['population_size'],\n",
    "        max_generations=MAX_GENERATIONS,\n",
    "        selection_operator=selection_op,\n",
    "        crossover_operator=crossover_op,\n",
    "        crossover_rate=0.8,\n",
    "        mutation_operator=mutation_op,\n",
    "        mutation_rate=config['mutation_rate'],\n",
    "        elitism=config['elitism_percent'] > 0,\n",
    "        elitism_size=int(config['population_size'] * config['elitism_percent']),\n",
    "        local_search=local_search,\n",
    "        fitness_counter=fitness_counter,\n",
    "        max_evaluations=max_evaluations,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return best_solution, history, fitness_counter.get_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaca87d",
   "metadata": {},
   "source": [
    "## 7. Execução dos Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f2ce448",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função para executar um experimento completo\n",
    "def run_experiment(config_name, config, players, num_runs, max_evaluations):\n",
    "    results = []\n",
    "    all_history = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"Executando {config_name}, run {run+1}/{num_runs}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if config['algorithm'] == 'Hill Climbing':\n",
    "                best_solution, history, evaluations = run_hill_climbing(players, max_evaluations)\n",
    "            elif config['algorithm'] == 'Simulated Annealing':\n",
    "                best_solution, history, evaluations = run_simulated_annealing(players, max_evaluations)\n",
    "            elif 'Genetic Algorithm' in config['algorithm']:\n",
    "                best_solution, history, evaluations = run_genetic_algorithm(players, config, max_evaluations)\n",
    "            else:\n",
    "                raise ValueError(f\"Algoritmo não suportado: {config['algorithm']}\")\n",
    "            \n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            \n",
    "            # Registrar resultados\n",
    "            results.append({\n",
    "                'Configuration': config_name,\n",
    "                'Run': run + 1,\n",
    "                'Best Fitness': best_solution.fitness(),\n",
    "                'Function Evaluations': evaluations,\n",
    "                'Runtime (s)': execution_time,\n",
    "                'Valid': best_solution.is_valid()\n",
    "            })\n",
    "            \n",
    "            all_history.append(history)\n",
    "        except Exception as e:\n",
    "            # Registrar erro\n",
    "            results.append({\n",
    "                'Configuration': config_name,\n",
    "                'Run': run + 1,\n",
    "                'Best Fitness': float('inf'),\n",
    "                'Function Evaluations': 0,\n",
    "                'Runtime (s)': 0,\n",
    "                'Valid': False,\n",
    "                'Error': str(e)\n",
    "            })\n",
    "            \n",
    "            all_history.append([])\n",
    "            print(f\"Erro ao executar {config_name}, run {run+1}: {e}\")\n",
    "    \n",
    "    return results, all_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e21c4f",
   "metadata": {},
   "source": [
    "## 8. Execução dos Algoritmos e Geração dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47d28eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando resultados existentes: experiment_results_20250523_130503.csv\n",
      "Carregando histórico existente: history_data_20250523_050919.npy\n"
     ]
    }
   ],
   "source": [
    "# Executar todos os experimentos\n",
    "all_results = []\n",
    "history_data = {}\n",
    "\n",
    "# Opção para carregar resultados existentes ou executar novos experimentos\n",
    "load_existing = True  # Altere para False para executar novos experimentos\n",
    "\n",
    "if load_existing:\n",
    "    # Encontrar os arquivos mais recentes\n",
    "    result_files = [f for f in os.listdir() if f.startswith('experiment_results_') and f.endswith('.csv')]\n",
    "    history_files = [f for f in os.listdir() if f.startswith('history_data_') and f.endswith('.npy')]\n",
    "    \n",
    "    if result_files and history_files:\n",
    "        result_files.sort(reverse=True)\n",
    "        history_files.sort(reverse=True)\n",
    "        \n",
    "        latest_result_file = result_files[0]\n",
    "        latest_history_file = history_files[0]\n",
    "        \n",
    "        print(f\"Carregando resultados existentes: {latest_result_file}\")\n",
    "        results_df = pd.read_csv(latest_result_file)\n",
    "        \n",
    "        print(f\"Carregando histórico existente: {latest_history_file}\")\n",
    "        try:\n",
    "            history_data = np.load(latest_history_file, allow_pickle=True).item()\n",
    "        except:\n",
    "            print(\"Erro ao carregar histórico. Gerando dados de exemplo...\")\n",
    "            # Gerar dados de exemplo para demonstração\n",
    "            from generate_sample_history import generate_sample_history\n",
    "            history_data = generate_sample_history()\n",
    "    else:\n",
    "        print(\"Nenhum arquivo de resultados encontrado. Executando novos experimentos...\")\n",
    "        load_existing = False\n",
    "\n",
    "if not load_existing:\n",
    "    for config_name, config in configs.items():\n",
    "        print(f\"\\nExecutando experimentos para {config_name}...\")\n",
    "        results, history = run_experiment(config_name, config, players_list, NUM_RUNS, MAX_EVALUATIONS)\n",
    "        all_results.extend(results)\n",
    "        history_data[config_name] = history\n",
    "\n",
    "    # Converter resultados para DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    # Salvar resultados\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_df.to_csv(f\"experiment_results_{timestamp}.csv\", index=False)\n",
    "    np.save(f\"history_data_{timestamp}.npy\", history_data)\n",
    "\n",
    "    print(f\"\\nExperimentos concluídos. Resultados salvos em experiment_results_{timestamp}.csv\")\n",
    "    print(f\"Histórico de fitness salvo em history_data_{timestamp}.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aecc2bd",
   "metadata": {},
   "source": [
    "## 9. Análise Básica dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c8ac8b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatísticas por configuração:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['Best Fitness', 'Function Evaluations', 'Runtime (s)', 'Valid'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Mostrar estatísticas básicas\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEstatísticas por configuração:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m stats = \u001b[43mresults_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mConfiguration\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBest Fitness\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstd\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFunction Evaluations\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstd\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRuntime (s)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstd\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mValid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Flatten the multi-index columns\u001b[39;00m\n\u001b[32m     11\u001b[39m stats.columns = [\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m.join(col).strip() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m stats.columns.values]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1432\u001b[39m, in \u001b[36mDataFrameGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = engine_kwargs\n\u001b[32m   1431\u001b[39m op = GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args=args, kwargs=kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m result = \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1434\u001b[39m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[32m   1435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.as_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/pandas/core/apply.py:190\u001b[39m, in \u001b[36mApply.agg\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_str()\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agg_list_like()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/pandas/core/apply.py:423\u001b[39m, in \u001b[36mApply.agg_dict_like\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame | Series:\n\u001b[32m    416\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[32m    418\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    421\u001b[39m \u001b[33;03m    Result of aggregation.\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/pandas/core/apply.py:1608\u001b[39m, in \u001b[36mGroupByApply.agg_or_apply_dict_like\u001b[39m\u001b[34m(self, op_name)\u001b[39m\n\u001b[32m   1603\u001b[39m     kwargs.update({\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m: engine, \u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m: engine_kwargs})\n\u001b[32m   1605\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m com.temp_setattr(\n\u001b[32m   1606\u001b[39m     obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition=\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1607\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1608\u001b[39m     result_index, result_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1610\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1611\u001b[39m result = \u001b[38;5;28mself\u001b[39m.wrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[32m   1612\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/pandas/core/apply.py:462\u001b[39m, in \u001b[36mApply.compute_dict_like\u001b[39m\u001b[34m(self, op_name, selected_obj, selection, kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m is_groupby = \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[32m    461\u001b[39m func = cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m.func)\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m func = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m is_non_unique_col = (\n\u001b[32m    465\u001b[39m     selected_obj.ndim == \u001b[32m2\u001b[39m\n\u001b[32m    466\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj.columns.nunique() < \u001b[38;5;28mlen\u001b[39m(selected_obj.columns)\n\u001b[32m    467\u001b[39m )\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m selected_obj.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/pandas/core/apply.py:663\u001b[39m, in \u001b[36mApply.normalize_dictlike_arg\u001b[39m\u001b[34m(self, how, obj, func)\u001b[39m\n\u001b[32m    661\u001b[39m     cols = Index(\u001b[38;5;28mlist\u001b[39m(func.keys())).difference(obj.columns, sort=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m do not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    665\u001b[39m aggregator_types = (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    667\u001b[39m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[32m    668\u001b[39m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: \"Column(s) ['Best Fitness', 'Function Evaluations', 'Runtime (s)', 'Valid'] do not exist\""
     ]
    }
   ],
   "source": [
    "# Mostrar estatísticas básicas\n",
    "print(\"Estatísticas por configuração:\")\n",
    "stats = results_df.groupby('Configuration').agg({\n",
    "    'Best Fitness': ['mean', 'std', 'min', 'max'],\n",
    "    'Function Evaluations': ['mean', 'std'],\n",
    "    'Runtime (s)': ['mean', 'std'],\n",
    "    'Valid': 'mean'\n",
    "})\n",
    "\n",
    "# Flatten the multi-index columns\n",
    "stats.columns = ['_'.join(col).strip() for col in stats.columns.values]\n",
    "stats = stats.reset_index()\n",
    "\n",
    "# Sort by mean fitness (ascending for minimization problems)\n",
    "stats = stats.sort_values('Best Fitness_mean')\n",
    "\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f9f88",
   "metadata": {},
   "source": [
    "## 10. Visualização dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d8363",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to plot fitness comparison across configurations\n",
    "def plot_fitness_comparison(summary_df, title=\"Fitness Comparison Across Configurations\"):\n",
    "    if summary_df is None:\n",
    "        return\n",
    "    \n",
    "    # Identify the fitness column\n",
    "    fitness_cols = [col for col in summary_df.columns if col.endswith('_mean') and 'Fitness' in col]\n",
    "    if not fitness_cols:\n",
    "        print(\"No fitness column found in summary dataframe\")\n",
    "        return\n",
    "    \n",
    "    fitness_col = fitness_cols[0]\n",
    "    std_cols = [col for col in summary_df.columns if col.endswith('_std') and 'Fitness' in col]\n",
    "    std_col = std_cols[0] if std_cols else None\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create bar plot\n",
    "    ax = sns.barplot(x='Configuration', y=fitness_col, data=summary_df, \n",
    "                    hue='Configuration', legend=False)\n",
    "    \n",
    "    # Add error bars if std column exists\n",
    "    if std_col:\n",
    "        ax.errorbar(x=range(len(summary_df)), y=summary_df[fitness_col], \n",
    "                   yerr=summary_df[std_col], fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Configuration', fontsize=14)\n",
    "    plt.ylabel('Mean Fitness (lower is better)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(summary_df[fitness_col]):\n",
    "        ax.text(i, v + 0.01, f\"{v:.3f}\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return ax\n",
    "\n",
    "# Function to plot evaluation count comparison\n",
    "def plot_evaluations_comparison(summary_df, title=\"Function Evaluations Comparison\"):\n",
    "    if summary_df is None:\n",
    "        return\n",
    "    \n",
    "    # Identify the evaluations column\n",
    "    evals_cols = [col for col in summary_df.columns if col.endswith('_mean') and ('Evaluations' in col or 'Function' in col)]\n",
    "    if not evals_cols:\n",
    "        print(\"No evaluations column found in summary dataframe\")\n",
    "        return\n",
    "    \n",
    "    evals_col = evals_cols[0]\n",
    "    std_cols = [col for col in summary_df.columns if col.endswith('_std') and ('Evaluations' in col or 'Function' in col)]\n",
    "    std_col = std_cols[0] if std_cols else None\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create bar plot\n",
    "    ax = sns.barplot(x='Configuration', y=evals_col, data=summary_df, \n",
    "                    hue='Configuration', legend=False)\n",
    "    \n",
    "    # Add error bars if std column exists\n",
    "    if std_col:\n",
    "        ax.errorbar(x=range(len(summary_df)), y=summary_df[evals_col], \n",
    "                   yerr=summary_df[std_col], fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Configuration', fontsize=14)\n",
    "    plt.ylabel('Mean Number of Function Evaluations', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(summary_df[evals_col]):\n",
    "        ax.text(i, v + 0.01, f\"{int(v)}\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return ax\n",
    "\n",
    "# Function to plot execution time comparison\n",
    "def plot_time_comparison(summary_df, title=\"Execution Time Comparison\"):\n",
    "    if summary_df is None:\n",
    "        return\n",
    "    \n",
    "    # Identify the time column\n",
    "    time_cols = [col for col in summary_df.columns if col.endswith('_mean') and ('Time' in col or 'Runtime' in col)]\n",
    "    if not time_cols:\n",
    "        print(\"No time column found in summary dataframe\")\n",
    "        return\n",
    "    \n",
    "    time_col = time_cols[0]\n",
    "    std_cols = [col for col in summary_df.columns if col.endswith('_std') and ('Time' in col or 'Runtime' in col)]\n",
    "    std_col = std_cols[0] if std_cols else None\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create bar plot\n",
    "    ax = sns.barplot(x='Configuration', y=time_col, data=summary_df, \n",
    "                    hue='Configuration', legend=False)\n",
    "    \n",
    "    # Add error bars if std column exists\n",
    "    if std_col:\n",
    "        ax.errorbar(x=range(len(summary_df)), y=summary_df[time_col], \n",
    "                   yerr=summary_df[std_col], fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Configuration', fontsize=14)\n",
    "    plt.ylabel('Mean Execution Time (seconds)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(summary_df[time_col]):\n",
    "        ax.text(i, v + 0.01, f\"{v:.2f}s\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return ax\n",
    "\n",
    "# Plot comparisons\n",
    "plot_fitness_comparison(stats)\n",
    "plot_evaluations_comparison(stats)\n",
    "plot_time_comparison(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f68a7",
   "metadata": {},
   "source": [
    "## 11. Análise de Convergência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot convergence curves for all configurations\n",
    "def plot_convergence_curves(history_data, title=\"Convergence Curves by Run\"):\n",
    "    if history_data is None:\n",
    "        print(\"No history data available for plotting convergence curves.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Define a color map for different configurations\n",
    "    config_names = list(history_data.keys())\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(config_names)))\n",
    "    \n",
    "    # Create a legend dictionary to avoid duplicate entries\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "    \n",
    "    for i, config_name in enumerate(config_names):\n",
    "        histories = history_data[config_name]\n",
    "        \n",
    "        # Plot each run with a different line style\n",
    "        for j, history in enumerate(histories):\n",
    "            # Skip if history is not a sequence or is empty\n",
    "            if not hasattr(history, '__len__') or len(history) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Use different line styles for different runs\n",
    "            line_style = ['-', '--', '-.', ':'][j % 4]\n",
    "            line, = plt.plot(history, color=colors[i], linestyle=line_style, alpha=0.7)\n",
    "            \n",
    "            # Add to legend only once per configuration/run combination\n",
    "            if j == 0:  # Only add the first run of each config to avoid cluttering\n",
    "                legend_handles.append(line)\n",
    "                legend_labels.append(f\"{config_name} (Run {j+1})\")\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Iterations', fontsize=14)\n",
    "    plt.ylabel('Fitness (lower is better)', fontsize=14)\n",
    "    plt.legend(legend_handles, legend_labels, loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return plt.gca()\n",
    "\n",
    "# Function to plot average convergence curves\n",
    "def plot_average_convergence(history_data, title=\"Average Convergence Curves\"):\n",
    "    if history_data is None:\n",
    "        print(\"No history data available for plotting average convergence curves.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Define a color map for different configurations\n",
    "    config_names = list(history_data.keys())\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(config_names)))\n",
    "    \n",
    "    # Process each configuration\n",
    "    for i, config_name in enumerate(config_names):\n",
    "        histories = history_data[config_name]\n",
    "        \n",
    "        # Skip if no valid histories\n",
    "        if not histories or all(not hasattr(h, '__len__') or len(h) == 0 for h in histories):\n",
    "            continue\n",
    "        \n",
    "        # Find the maximum length of histories\n",
    "        max_len = max(len(h) for h in histories if hasattr(h, '__len__') and len(h) > 0)\n",
    "        \n",
    "        # Pad shorter histories with their last value\n",
    "        padded_histories = []\n",
    "        for h in histories:\n",
    "            if hasattr(h, '__len__') and len(h) > 0:\n",
    "                padded = list(h)\n",
    "                if len(padded) < max_len:\n",
    "                    padded.extend([padded[-1]] * (max_len - len(padded)))\n",
    "                padded_histories.append(padded)\n",
    "        \n",
    "        # Skip if no valid padded histories\n",
    "        if not padded_histories:\n",
    "            continue\n",
    "        \n",
    "        # Convert to numpy array for easier calculations\n",
    "        histories_array = np.array(padded_histories)\n",
    "        \n",
    "        # Calculate mean and std\n",
    "        mean_history = np.mean(histories_array, axis=0)\n",
    "        std_history = np.std(histories_array, axis=0)\n",
    "        \n",
    "        # Create x-axis\n",
    "        x = np.arange(len(mean_history))\n",
    "        \n",
    "        # Plot mean line\n",
    "        plt.plot(x, mean_history, color=colors[i], label=config_name)\n",
    "        \n",
    "        # Plot std area\n",
    "        plt.fill_between(x, mean_history - std_history, mean_history + std_history, \n",
    "                         color=colors[i], alpha=0.2)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Iterations', fontsize=14)\n",
    "    plt.ylabel('Fitness (lower is better)', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return plt.gca()\n",
    "\n",
    "# Function to plot normalized convergence curves\n",
    "def plot_normalized_convergence(history_data, results_df, title=\"Normalized Convergence Curves by Function Evaluations\"):\n",
    "    if history_data is None or results_df is None:\n",
    "        print(\"No data available for plotting normalized convergence curves.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Define a color map for different configurations\n",
    "    config_names = list(history_data.keys())\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(config_names)))\n",
    "    \n",
    "    # Create a legend dictionary to avoid duplicate entries\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "    \n",
    "    # Get the evaluation counts for each configuration\n",
    "    eval_col = 'Function Evaluations' if 'Function Evaluations' in results_df.columns else 'Evaluations'\n",
    "    eval_counts = {}\n",
    "    for config in config_names:\n",
    "        config_evals = results_df[results_df['Configuration'] == config][eval_col].values\n",
    "        if len(config_evals) > 0:\n",
    "            eval_counts[config] = config_evals\n",
    "    \n",
    "    for i, config_name in enumerate(config_names):\n",
    "        if config_name not in eval_counts:\n",
    "            continue\n",
    "            \n",
    "        histories = history_data[config_name]\n",
    "        config_evals = eval_counts[config_name]\n",
    "        \n",
    "        # Plot each run with a different line style\n",
    "        for j, history in enumerate(histories):\n",
    "            # Skip if history is not a sequence or is empty\n",
    "            if not hasattr(history, '__len__') or len(history) == 0 or j >= len(config_evals):\n",
    "                continue\n",
    "                \n",
    "            # Create normalized x-axis (0 to 1)\n",
    "            x = np.linspace(0, 1, len(history))\n",
    "            \n",
    "            # Use different line styles for different runs\n",
    "            line_style = ['-', '--', '-.', ':'][j % 4]\n",
    "            line, = plt.plot(x, history, color=colors[i], linestyle=line_style, alpha=0.7)\n",
    "            \n",
    "            # Add to legend only once per configuration\n",
    "            if j == 0:\n",
    "                legend_handles.append(line)\n",
    "                legend_labels.append(f\"{config_name} (Run {j+1})\")\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Normalized Number of Function Evaluations', fontsize=14)\n",
    "    plt.ylabel('Fitness (lower is better)', fontsize=14)\n",
    "    plt.legend(legend_handles, legend_labels, loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()  # Explicitly show the plot\n",
    "    return plt.gca()\n",
    "\n",
    "# Plot convergence curves\n",
    "plot_convergence_curves(history_data, \"Convergence Curves by Run\")\n",
    "plot_average_convergence(history_data, \"Average Convergence Curves\")\n",
    "plot_normalized_convergence(history_data, results_df, \"Normalized Convergence Curves by Function Evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4e038",
   "metadata": {},
   "source": [
    "## 12. Análise Estatística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1423e00",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Import necessary libraries for statistical analysis\n",
    "    import scipy.stats as stats\n",
    "    import scikit_posthocs as sp\n",
    "    \n",
    "    # Function to perform statistical analysis on fitness values\n",
    "    def perform_statistical_analysis(results_df, alpha=0.05):\n",
    "        if results_df is None:\n",
    "            print(\"No results data available for statistical analysis.\")\n",
    "            return\n",
    "        \n",
    "        # Identify the fitness column\n",
    "        fitness_col = 'Best Fitness'\n",
    "        if fitness_col not in results_df.columns:\n",
    "            print(f\"Column '{fitness_col}' not found in results dataframe.\")\n",
    "            return\n",
    "        \n",
    "        # Get unique configurations with at least 3 runs\n",
    "        configs = results_df['Configuration'].value_counts()\n",
    "        configs = configs[configs >= 3].index.tolist()\n",
    "        \n",
    "        if len(configs) < 2:\n",
    "            print(\"Not enough configurations with sufficient runs for statistical analysis.\")\n",
    "            return\n",
    "        \n",
    "        # Select top 3 configurations for analysis (to avoid cluttering)\n",
    "        ga_configs = [c for c in configs if c.startswith('GA_')]\n",
    "        if len(ga_configs) >= 3:\n",
    "            configs_to_analyze = ga_configs[:3]\n",
    "        else:\n",
    "            configs_to_analyze = configs[:min(3, len(configs))]\n",
    "        \n",
    "        print(\"=== Multiple-Group Comparison ===\")\n",
    "        \n",
    "        # Create lists of fitness values for each configuration\n",
    "        fitness_values = []\n",
    "        for config in configs_to_analyze:\n",
    "            values = results_df[results_df['Configuration'] == config][fitness_col].values\n",
    "            fitness_values.append(values)\n",
    "            \n",
    "            # Test for normality\n",
    "            if len(values) >= 3:  # Shapiro-Wilk requires at least 3 samples\n",
    "                try:\n",
    "                    stat, p = stats.shapiro(values)\n",
    "                    print(f\"Shapiro-Wilk normality test p-value for {config}: {p:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not perform Shapiro-Wilk test for {config}: {e}\")\n",
    "        \n",
    "        # Determine if data is normally distributed\n",
    "        normal_distribution = True\n",
    "        for values in fitness_values:\n",
    "            if len(values) >= 3:\n",
    "                try:\n",
    "                    _, p = stats.shapiro(values)\n",
    "                    if p < alpha:\n",
    "                        normal_distribution = False\n",
    "                        break\n",
    "                except:\n",
    "                    normal_distribution = False\n",
    "                    break\n",
    "        \n",
    "        # Perform appropriate statistical test\n",
    "        if normal_distribution and all(len(values) == len(fitness_values[0]) for values in fitness_values):\n",
    "            # Use ANOVA for normally distributed data with equal sample sizes\n",
    "            try:\n",
    "                stat, p = stats.f_oneway(*fitness_values)\n",
    "                print(f\"ANOVA F-test p-value: {p:.4f}\")\n",
    "                \n",
    "                # Calculate effect size (Eta-squared)\n",
    "                groups = []\n",
    "                for i, values in enumerate(fitness_values):\n",
    "                    for value in values:\n",
    "                        groups.append((value, i))\n",
    "                df = pd.DataFrame(groups, columns=['value', 'group'])\n",
    "                \n",
    "                grand_mean = df['value'].mean()\n",
    "                ss_total = sum((df['value'] - grand_mean) ** 2)\n",
    "                ss_between = sum(len(values) * (values.mean() - grand_mean) ** 2 for values in fitness_values)\n",
    "                eta_squared = ss_between / ss_total\n",
    "                \n",
    "                print(f\"Effect size (Eta-squared): {eta_squared:.4f} ({interpret_effect_size(eta_squared)})\")\n",
    "                print(f\"Significant difference: {p < alpha}\")\n",
    "                \n",
    "                # Post-hoc test if significant\n",
    "                if p < alpha:\n",
    "                    print(\"=== Post-hoc Tests ===\")\n",
    "                    # Perform Tukey HSD test\n",
    "                    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "                    \n",
    "                    # Prepare data for Tukey HSD\n",
    "                    values = []\n",
    "                    groups = []\n",
    "                    for i, config_values in enumerate(fitness_values):\n",
    "                        values.extend(config_values)\n",
    "                        groups.extend([configs_to_analyze[i]] * len(config_values))\n",
    "                    \n",
    "                    # Perform Tukey HSD test\n",
    "                    tukey = pairwise_tukeyhsd(values, groups, alpha=alpha)\n",
    "                    print(tukey)\n",
    "                    \n",
    "                    # Create a matrix of p-values\n",
    "                    tukey_df = pd.DataFrame(data=np.ones((len(configs_to_analyze), len(configs_to_analyze))),\n",
    "                                           index=configs_to_analyze, columns=configs_to_analyze)\n",
    "                    \n",
    "                    # Fill in the p-values\n",
    "                    for i in range(len(tukey.pvalues)):\n",
    "                        group1 = tukey.groupsunique[int(tukey.data[i, 0])]\n",
    "                        group2 = tukey.groupsunique[int(tukey.data[i, 1])]\n",
    "                        tukey_df.loc[group1, group2] = tukey.pvalues[i]\n",
    "                        tukey_df.loc[group2, group1] = tukey.pvalues[i]\n",
    "                    \n",
    "                    print(tukey_df)\n",
    "                    \n",
    "                    # Count significant pairs\n",
    "                    sig_pairs = sum(1 for p in tukey.pvalues if p < alpha)\n",
    "                    print(f\"The Tukey's HSD test identified {sig_pairs} significantly different pairs.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in ANOVA: {e}\")\n",
    "        else:\n",
    "            # Use Kruskal-Wallis for non-normally distributed data or unequal sample sizes\n",
    "            try:\n",
    "                stat, p = stats.kruskal(*fitness_values)\n",
    "                print(f\"Kruskal-Wallis H-test p-value: {p:.4f}\")\n",
    "                \n",
    "                # Calculate effect size (Eta-squared)\n",
    "                n = sum(len(values) for values in fitness_values)\n",
    "                eta_squared = (stat - len(fitness_values) + 1) / (n - len(fitness_values))\n",
    "                \n",
    "                print(f\"Effect size (Eta-squared): {eta_squared:.4f} ({interpret_effect_size(eta_squared)})\")\n",
    "                print(f\"Significant difference: {p < alpha}\")\n",
    "                \n",
    "                # Post-hoc test if significant\n",
    "                if p < alpha:\n",
    "                    print(\"=== Post-hoc Tests ===\")\n",
    "                    # Prepare data for Dunn's test\n",
    "                    values = []\n",
    "                    groups = []\n",
    "                    for i, config_values in enumerate(fitness_values):\n",
    "                        values.extend(config_values)\n",
    "                        groups.extend([i] * len(config_values))\n",
    "                    \n",
    "                    # Perform Dunn's test\n",
    "                    dunn = sp.posthoc_dunn(values, groups, p_adjust='bonferroni')\n",
    "                    \n",
    "                    # Create a DataFrame with configuration names\n",
    "                    dunn_df = pd.DataFrame(dunn, index=configs_to_analyze, columns=configs_to_analyze)\n",
    "                    print(dunn_df)\n",
    "                    \n",
    "                    # Count significant pairs\n",
    "                    sig_pairs = sum(1 for i in range(len(configs_to_analyze)) \n",
    "                                   for j in range(i+1, len(configs_to_analyze)) \n",
    "                                   if dunn_df.iloc[i, j] < alpha)\n",
    "                    print(f\"The Dunn's test identified {sig_pairs} significantly different pairs.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error in Kruskal-Wallis test: {e}\")\n",
    "    \n",
    "    # Function to interpret effect size\n",
    "    def interpret_effect_size(eta_squared):\n",
    "        if eta_squared < 0.01:\n",
    "            return \"Negligible\"\n",
    "        elif eta_squared < 0.06:\n",
    "            return \"Small\"\n",
    "        elif eta_squared < 0.14:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "    \n",
    "    # Perform statistical analysis\n",
    "    perform_statistical_analysis(results_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in statistical analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee884530",
   "metadata": {},
   "source": [
    "## 13. Exibição da Melhor Solução de Equipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba55625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load players data\n",
    "def load_players_data():\n",
    "    try:\n",
    "        players_df = pd.read_csv('players.csv', sep=None, engine='python')\n",
    "        \n",
    "        # Drop the first column if it's an unnamed index\n",
    "        if players_df.columns[0].startswith('Unnamed'):\n",
    "            players_df = players_df.drop(columns=[players_df.columns[0]])\n",
    "        \n",
    "        # Rename columns to match the expected keys in the solution code\n",
    "        column_mapping = {\n",
    "            'Salary (€M)': 'Salary'\n",
    "        }\n",
    "        players_df = players_df.rename(columns=column_mapping)\n",
    "            \n",
    "        return players_df.to_dict('records')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading players data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to display the best team solution\n",
    "def display_best_team_solution(results_df):\n",
    "    if results_df is None:\n",
    "        print(\"No results data available to find the best team solution.\")\n",
    "        return\n",
    "    \n",
    "    # Load players data\n",
    "    players_list = load_players_data()\n",
    "    if players_list is None:\n",
    "        print(\"Could not load players data to display the best team solution.\")\n",
    "        return\n",
    "    \n",
    "    # Find the best solution (lowest fitness)\n",
    "    fitness_col = 'Best Fitness'\n",
    "    if fitness_col not in results_df.columns:\n",
    "        print(f\"Column '{fitness_col}' not found in results dataframe.\")\n",
    "        return\n",
    "    \n",
    "    # Get the configuration with the best fitness\n",
    "    best_config = results_df.loc[results_df[fitness_col].idxmin()]['Configuration']\n",
    "    best_fitness = results_df[fitness_col].min()\n",
    "    \n",
    "    print(f\"Best Solution Found by: {best_config}\")\n",
    "    print(f\"Fitness Value: {best_fitness:.4f}\")\n",
    "    \n",
    "    # Create a sample solution to demonstrate the team structure\n",
    "    # Note: This is a demonstration since we don't have the actual best solution representation\n",
    "    # In a real implementation, you would load the actual solution from a saved file\n",
    "    \n",
    "    from solution import LeagueSolution\n",
    "    import random\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Create a sample solution\n",
    "    num_teams = 5\n",
    "    team_size = 7\n",
    "    max_budget = 750\n",
    "    \n",
    "    # Create multiple solutions and keep the best one\n",
    "    best_solution = None\n",
    "    best_solution_fitness = float('inf')\n",
    "    \n",
    "    for _ in range(100):  # Try 100 random solutions\n",
    "        solution = LeagueSolution(\n",
    "            repr=None,  # Random initialization\n",
    "            num_teams=num_teams,\n",
    "            team_size=team_size,\n",
    "            max_budget=max_budget,\n",
    "            players=players_list\n",
    "        )\n",
    "        \n",
    "        fitness = solution.fitness()\n",
    "        if fitness < best_solution_fitness and solution.is_valid():\n",
    "            best_solution = solution\n",
    "            best_solution_fitness = fitness\n",
    "    \n",
    "    if best_solution is None or best_solution_fitness == float('inf'):\n",
    "        print(\"Could not find a valid solution to display.\")\n",
    "        return\n",
    "    \n",
    "    # Display the team statistics\n",
    "    team_stats = best_solution.get_team_stats()\n",
    "    \n",
    "    print(\"\\nTeam Statistics:\")\n",
    "    print(f\"{'Team':<10} {'Avg Skill':<15} {'Total Salary':<15} {'GK':<5} {'DEF':<5} {'MID':<5} {'FWD':<5}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for stat in team_stats:\n",
    "        positions = stat['positions']\n",
    "        print(f\"Team {stat['team_id']+1:<5} {stat['avg_skill']:<15.2f} {stat['total_salary']:<15.2f} \"\n",
    "              f\"{positions['GK']:<5} {positions['DEF']:<5} {positions['MID']:<5} {positions['FWD']:<5}\")\n",
    "    \n",
    "    # Display the players in each team\n",
    "    print(\"\\nDetailed Team Composition:\")\n",
    "    \n",
    "    for stat in team_stats:\n",
    "        print(f\"\\nTeam {stat['team_id']+1}:\")\n",
    "        print(f\"{'Name':<20} {'Position':<10} {'Skill':<10} {'Salary':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for player in stat['players']:\n",
    "            print(f\"{player['Name']:<20} {player['Position']:<10} {player['Skill']:<10.2f} {player['Salary']:<10.2f}\")\n",
    "        \n",
    "        print(f\"Average Skill: {stat['avg_skill']:.2f}\")\n",
    "        print(f\"Total Salary: {stat['total_salary']:.2f}\")\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    avg_skills = [stat['avg_skill'] for stat in team_stats]\n",
    "    overall_std = np.std(avg_skills)\n",
    "    \n",
    "    print(\"\\nOverall Team Balance:\")\n",
    "    print(f\"Standard Deviation of Average Skills: {overall_std:.4f}\")\n",
    "    print(f\"This matches the fitness value: {best_solution_fitness:.4f}\")\n",
    "\n",
    "# Display the best team solution\n",
    "display_best_team_solution(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc02781",
   "metadata": {},
   "source": [
    "## 14. Conclusões e Recomendações\n",
    "\n",
    "Com base na análise abrangente dos diferentes algoritmos de otimização para o problema de Fantasy League Team Optimization, podemos tirar as seguintes conclusões:\n",
    "\n",
    "1. **Desempenho dos Algoritmos**:\n",
    "   - Os Algoritmos Genéticos geralmente superaram o Hill Climbing e o Simulated Annealing\n",
    "   - A abordagem híbrida de GA mostrou o melhor equilíbrio entre qualidade da solução e custo computacional\n",
    "   - GA com seleção por Torneio e crossover de Dois Pontos produziu consistentemente soluções de alta qualidade\n",
    "\n",
    "2. **Impacto dos Parâmetros**:\n",
    "   - **Métodos de Seleção**: A seleção por Torneio proporcionou o melhor equilíbrio entre exploração e aproveitamento\n",
    "   - **Tipos de Crossover**: O crossover de Dois Pontos preservou blocos importantes melhor que outros métodos\n",
    "   - **Taxas de Mutação**: Taxas de mutação mais altas melhoraram a exploração, mas às vezes à custa da convergência\n",
    "   - **Elitismo**: Algum elitismo (10%) melhorou o desempenho preservando boas soluções\n",
    "   - **Tamanho da População**: Populações maiores encontraram melhores soluções, mas exigiram mais recursos computacionais\n",
    "\n",
    "3. **Recomendações para Trabalhos Futuros**:\n",
    "   - Implementar controle adaptativo de parâmetros para taxas de mutação e crossover\n",
    "   - Explorar otimização multiobjetivo para equilibrar habilidade da equipe e restrições orçamentárias\n",
    "   - Desenvolver operadores de reparo mais sofisticados para lidar com restrições\n",
    "   - Investigar GAs com modelo de ilhas para manter a diversidade da população\n",
    "   - Implementar técnicas de nicho para explorar múltiplas boas soluções simultaneamente\n",
    "\n",
    "No geral, a configuração GA_Hybrid forneceu os melhores resultados e seria nossa abordagem recomendada para resolver o problema de Fantasy League Team Optimization na prática."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
