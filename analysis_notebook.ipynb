{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports League Optimization: Comparative Analysis of Algorithms\n",
    "\n",
    "This notebook presents a comprehensive analysis of different optimization algorithms applied to the Sports League problem. We compare Hill Climbing, Simulated Annealing, and Genetic Algorithm approaches, analyzing their performance across multiple metrics.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Problem Definition](#1-problem-definition)\n",
    "2. [Experimental Setup](#2-experimental-setup)\n",
    "3. [Algorithm Implementations](#3-algorithm-implementations)\n",
    "4. [Performance Comparison](#4-performance-comparison)\n",
    "5. [Statistical Analysis](#5-statistical-analysis)\n",
    "6. [Conclusion](#6-conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy import stats\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "\n",
    "# Import our custom modules\n",
    "from solution import LeagueSolution, LeagueHillClimbingSolution, LeagueSASolution\n",
    "from evolution import (\n",
    "    hill_climbing, \n",
    "    simulated_annealing, \n",
    "    genetic_algorithm,\n",
    "    # Mutation operators\n",
    "    mutate_swap,\n",
    "    mutate_swap_constrained,\n",
    "    mutate_team_shift,\n",
    "    mutate_targeted_player_exchange,\n",
    "    mutate_shuffle_within_team_constrained,\n",
    "    # Crossover operators\n",
    "    crossover_one_point,\n",
    "    crossover_one_point_prefer_valid,\n",
    "    crossover_uniform,\n",
    "    crossover_uniform_prefer_valid,\n",
    "    # Selection operators\n",
    "    selection_tournament,\n",
    "    selection_tournament_variable_k,\n",
    "    selection_ranking,\n",
    "    selection_boltzmann\n",
    ")\n",
    "# Import the new fitness counter module\n",
    "from fitness_counter import fitness_counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Definition\n",
    "\n",
    "### 1.1 Sports League Problem\n",
    "\n",
    "The Sports League problem involves assigning players to teams while satisfying specific constraints and optimizing for team balance. The goal is to create teams with similar average skill levels.\n",
    "\n",
    "**Formal Definition:**\n",
    "- We have 35 players with different positions (GK, DEF, MID, FWD) and skill levels\n",
    "- We need to assign these players to 5 teams (7 players per team)\n",
    "- Each team must have exactly 1 GK, 2 DEF, 2 MID, and 2 FWD\n",
    "- Each team's total salary must not exceed 750M €\n",
    "- The objective is to minimize the standard deviation of average team skills\n",
    "\n",
    "### 1.2 Solution Representation\n",
    "\n",
    "We represent a solution as a list of team assignments for each player. For example, if `solution.repr[0] = 2`, it means player 0 is assigned to team 2.\n",
    "\n",
    "**Search Space Size:**\n",
    "- For 35 players and 5 teams, the theoretical search space is 5^35\n",
    "- With constraints, the actual feasible search space is much smaller, but still extremely large\n",
    "\n",
    "### 1.3 Fitness Function\n",
    "\n",
    "The fitness function calculates the standard deviation of the average skill levels across all teams. A lower value indicates more balanced teams, which is our optimization goal.\n",
    "\n",
    "For invalid solutions (those violating constraints), we return infinity to ensure they are never selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load player data\n",
    "players_df = pd.read_csv(\"players.csv\", sep=\";\")\n",
    "# Rename the salary column to match the code expectations\n",
    "players_df = players_df.rename(columns={'Salary (€M)': 'Salary'})\n",
    "players_data = players_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Display the player data\n",
    "players_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Analysis\n",
    "\n",
    "Let's analyze the player data to understand the distribution of skills, positions, and salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze player positions\n",
    "position_counts = players_df['Position'].value_counts()\n",
    "print(\"Position distribution:\")\n",
    "print(position_counts)\n",
    "\n",
    "# Analyze skill distribution by position\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Position', y='Skill', data=players_df)\n",
    "plt.title('Skill Distribution by Position')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Analyze salary distribution by position\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Position', y='Salary', data=players_df)\n",
    "plt.title('Salary Distribution by Position')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Correlation between skill and salary\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Skill', y='Salary', hue='Position', data=players_df)\n",
    "plt.title('Correlation between Skill and Salary')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experimental Setup\n",
    "\n",
    "### 2.1 Metrics for Comparison\n",
    "\n",
    "To ensure a fair comparison between different algorithms, we'll track the following metrics:\n",
    "\n",
    "1. **Solution Quality**: The fitness value (standard deviation of average team skills)\n",
    "2. **Function Evaluations**: Number of fitness function calls\n",
    "3. **Iterations**: Number of algorithm iterations\n",
    "4. **Runtime**: Actual execution time in seconds\n",
    "\n",
    "### 2.2 Algorithm Configurations\n",
    "\n",
    "We'll test the following algorithm configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate valid initial solutions using a heuristic approach\n",
    "def generate_valid_solution(solution_class, players_data, max_attempts=100000):\n",
    "    \"\"\"\n",
    "    Generate a valid initial solution for any solution class using a heuristic approach.\n",
    "    \n",
    "    Args:\n",
    "        solution_class: The solution class to instantiate\n",
    "        players_data: List of player dictionaries\n",
    "        max_attempts: Maximum number of attempts to generate a valid solution\n",
    "        \n",
    "    Returns:\n",
    "        A valid solution instance or None if no valid solution could be found\n",
    "    \"\"\"\n",
    "    # First try the standard random approach with increased attempts\n",
    "    for _ in range(1000):  # Try random solutions first\n",
    "        solution = solution_class(players=players_data)\n",
    "        if solution.is_valid():\n",
    "            return solution\n",
    "    \n",
    "    # If random approach fails, use a heuristic construction approach\n",
    "    print(\"Using heuristic construction approach for valid solution generation...\")\n",
    "    \n",
    "    # Group players by position\n",
    "    players_by_position = {}\n",
    "    for i, player in enumerate(players_data):\n",
    "        pos = player['Position']\n",
    "        if pos not in players_by_position:\n",
    "            players_by_position[pos] = []\n",
    "        players_by_position[pos].append((i, player))\n",
    "    \n",
    "    # Sort players by skill within each position (descending)\n",
    "    for pos in players_by_position:\n",
    "        players_by_position[pos].sort(key=lambda x: x[1]['Skill'], reverse=True)\n",
    "    \n",
    "    # Create a new solution instance\n",
    "    solution = solution_class(players=players_data)\n",
    "    \n",
    "    # Reset all assignments\n",
    "    for i in range(len(solution.repr)):\n",
    "        solution.repr[i] = -1  # -1 means unassigned\n",
    "    \n",
    "    # Number of teams\n",
    "    num_teams = 5\n",
    "    \n",
    "    # Assign players to teams using a balanced approach\n",
    "    # First, assign goalkeepers (1 per team)\n",
    "    for team_id in range(num_teams):\n",
    "        if team_id < len(players_by_position['GK']):\n",
    "            player_idx = players_by_position['GK'][team_id][0]\n",
    "            solution.repr[player_idx] = team_id\n",
    "    \n",
    "    # Assign defenders (2 per team)\n",
    "    for team_id in range(num_teams):\n",
    "        defenders_assigned = 0\n",
    "        for player_tuple in players_by_position['DEF']:\n",
    "            player_idx = player_tuple[0]\n",
    "            if solution.repr[player_idx] == -1:  # If player not assigned yet\n",
    "                solution.repr[player_idx] = team_id\n",
    "                defenders_assigned += 1\n",
    "                if defenders_assigned == 2:  # 2 defenders per team\n",
    "                    break\n",
    "    \n",
    "    # Assign midfielders (2 per team)\n",
    "    for team_id in range(num_teams):\n",
    "        midfielders_assigned = 0\n",
    "        for player_tuple in players_by_position['MID']:\n",
    "            player_idx = player_tuple[0]\n",
    "            if solution.repr[player_idx] == -1:  # If player not assigned yet\n",
    "                solution.repr[player_idx] = team_id\n",
    "                midfielders_assigned += 1\n",
    "                if midfielders_assigned == 2:  # 2 midfielders per team\n",
    "                    break\n",
    "    \n",
    "    # Assign forwards (2 per team)\n",
    "    for team_id in range(num_teams):\n",
    "        forwards_assigned = 0\n",
    "        for player_tuple in players_by_position['FWD']:\n",
    "            player_idx = player_tuple[0]\n",
    "            if solution.repr[player_idx] == -1:  # If player not assigned yet\n",
    "                solution.repr[player_idx] = team_id\n",
    "                forwards_assigned += 1\n",
    "                if forwards_assigned == 2:  # 2 forwards per team\n",
    "                    break\n",
    "    \n",
    "    # Check if all players are assigned\n",
    "    if -1 in solution.repr:\n",
    "        # Assign remaining players to balance teams\n",
    "        for i in range(len(solution.repr)):\n",
    "            if solution.repr[i] == -1:\n",
    "                # Find team with fewest players\n",
    "                team_counts = [0] * num_teams\n",
    "                for team_id in solution.repr:\n",
    "                    if team_id != -1:\n",
    "                        team_counts[team_id] += 1\n",
    "                solution.repr[i] = team_counts.index(min(team_counts))\n",
    "    \n",
    "    # Check if solution is valid\n",
    "    if solution.is_valid():\n",
    "        return solution\n",
    "    \n",
    "    # If heuristic approach fails, try to fix salary constraints\n",
    "    # Calculate team salaries\n",
    "    team_salaries = [0] * num_teams\n",
    "    for i, team_id in enumerate(solution.repr):\n",
    "        team_salaries[team_id] += players_data[i]['Salary']\n",
    "    \n",
    "    # Try to swap players to satisfy salary constraints\n",
    "    max_salary = 750  # Maximum salary per team\n",
    "    for attempt in range(max_attempts):\n",
    "        # Find teams that exceed salary cap\n",
    "        over_budget_teams = [i for i, salary in enumerate(team_salaries) if salary > max_salary]\n",
    "        if not over_budget_teams:  # All teams within budget\n",
    "            break\n",
    "            \n",
    "        # Find teams under budget\n",
    "        under_budget_teams = [i for i, salary in enumerate(team_salaries) if salary <= max_salary]\n",
    "        if not under_budget_teams:  # All teams over budget\n",
    "            break\n",
    "            \n",
    "        # Select a team over budget and a team under budget\n",
    "        over_team = random.choice(over_budget_teams)\n",
    "        under_team = random.choice(under_budget_teams)\n",
    "        \n",
    "        # Find players in these teams\n",
    "        over_team_players = [(i, players_data[i]) for i in range(len(solution.repr)) if solution.repr[i] == over_team]\n",
    "        under_team_players = [(i, players_data[i]) for i in range(len(solution.repr)) if solution.repr[i] == under_team]\n",
    "        \n",
    "        # Try to find a pair of players to swap that would improve salary balance\n",
    "        for over_idx, over_player in over_team_players:\n",
    "            over_pos = over_player['Position']\n",
    "            over_salary = over_player['Salary']\n",
    "            \n",
    "            for under_idx, under_player in under_team_players:\n",
    "                under_pos = under_player['Position']\n",
    "                under_salary = under_player['Salary']\n",
    "                \n",
    "                # Only swap players of same position\n",
    "                if over_pos == under_pos and over_salary > under_salary:\n",
    "                    # Calculate new salaries after swap\n",
    "                    new_over_salary = team_salaries[over_team] - over_salary + under_salary\n",
    "                    new_under_salary = team_salaries[under_team] - under_salary + over_salary\n",
    "                    \n",
    "                    # If swap improves situation, do it\n",
    "                    if new_over_salary <= max_salary or new_over_salary < team_salaries[over_team]:\n",
    "                        # Swap players\n",
    "                        solution.repr[over_idx] = under_team\n",
    "                        solution.repr[under_idx] = over_team\n",
    "                        \n",
    "                        # Update team salaries\n",
    "                        team_salaries[over_team] = new_over_salary\n",
    "                        team_salaries[under_team] = new_under_salary\n",
    "                        break\n",
    "            \n",
    "            # Check if we've made a swap\n",
    "            if solution.is_valid():\n",
    "                return solution\n",
    "    \n",
    "    # If we still don't have a valid solution, try one more random approach\n",
    "    for _ in range(max_attempts - 1000):  # Use remaining attempts\n",
    "        solution = solution_class(players=players_data)\n",
    "        if solution.is_valid():\n",
    "            return solution\n",
    "    \n",
    "    # If all attempts fail, return None\n",
    "    return None\n",
    "\n",
    "# Define algorithm configurations\n",
    "configs = {\n",
    "    # Hill Climbing configurations\n",
    "    'HC_Standard': {\n",
    "        'algorithm': 'Hill Climbing',\n",
    "        'params': {\n",
    "            'max_iterations': 500,\n",
    "            'max_no_improvement': 100,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Hill Climbing with valid initial solution\n",
    "    'HC_Valid_Initial': {\n",
    "        'algorithm': 'Hill Climbing Valid',\n",
    "        'params': {\n",
    "            'max_iterations': 500,\n",
    "            'max_no_improvement': 100,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Simulated Annealing configurations\n",
    "    'SA_Standard': {\n",
    "        'algorithm': 'Simulated Annealing',\n",
    "        'params': {\n",
    "            'initial_temperature': 200.0,\n",
    "            'cooling_rate': 0.95,\n",
    "            'min_temperature': 1e-5,\n",
    "            'iterations_per_temp': 20,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Genetic Algorithm configurations\n",
    "    'GA_Tournament_OnePoint': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'params': {\n",
    "            'population_size': 100,\n",
    "            'max_generations': 50,\n",
    "            'selection_operator': selection_tournament,\n",
    "            'crossover_operator': crossover_one_point_prefer_valid,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_operator': mutate_swap_constrained,\n",
    "            'mutation_rate': 0.1,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 2,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    'GA_Ranking_Uniform': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'params': {\n",
    "            'population_size': 100,\n",
    "            'max_generations': 50,\n",
    "            'selection_operator': selection_ranking,\n",
    "            'crossover_operator': crossover_uniform_prefer_valid,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_operator': mutate_targeted_player_exchange,\n",
    "            'mutation_rate': 0.1,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 2,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    'GA_Boltzmann_TeamShift': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'params': {\n",
    "            'population_size': 100,\n",
    "            'max_generations': 50,\n",
    "            'selection_operator': selection_boltzmann,\n",
    "            'selection_params': {'temperature': 1.0},\n",
    "            'crossover_operator': crossover_one_point_prefer_valid,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_operator': mutate_team_shift,\n",
    "            'mutation_rate': 0.1,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 2,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    'GA_Hybrid': {\n",
    "        'algorithm': 'Hybrid GA',\n",
    "        'params': {\n",
    "            'population_size': 75,\n",
    "            'max_generations': 40,\n",
    "            'selection_operator': selection_tournament_variable_k,\n",
    "            'selection_params': {'k': 3},\n",
    "            'crossover_operator': crossover_uniform_prefer_valid,\n",
    "            'crossover_rate': 0.85,\n",
    "            'mutation_operator': mutate_shuffle_within_team_constrained,\n",
    "            'mutation_rate': 0.15,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 1,\n",
    "            'local_search': {\n",
    "                'algorithm': 'hill_climbing',\n",
    "                'frequency': 5,  # Apply HC every 5 generations\n",
    "                'iterations': 50  # HC iterations per application\n",
    "            },\n",
    "            'verbose': False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display the configurations\n",
    "for name, config in configs.items():\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"Algorithm: {config['algorithm']}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param, value in config['params'].items():\n",
    "        if param not in ['selection_operator', 'crossover_operator', 'mutation_operator', 'verbose']:\n",
    "            print(f\"  {param}: {value}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tracking Function Evaluations\n",
    "\n",
    "To ensure fair comparison between algorithms, we'll use our improved fitness counter module to track function evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fitness_counter is imported from fitness_counter.py\n",
    "# We'll use it to track fitness evaluations across all algorithms\n",
    "print(\"Fitness counter initialized and ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Experiment Runner\n",
    "\n",
    "We'll create a function to run a single experiment with a specific configuration and run number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(config_name, config, players_data, run):\n",
    "    \"\"\"\n",
    "    Run a single experiment with a specific configuration and run number.\n",
    "    \n",
    "    Args:\n",
    "        config_name (str): Name of the configuration\n",
    "        config (dict): Configuration dictionary\n",
    "        players_data (list): List of player dictionaries\n",
    "        run (int): Run number (0-based)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results of the experiment\n",
    "    \"\"\"\n",
    "    # Reset random seed for this run to ensure reproducibility\n",
    "    random.seed(42 + run)\n",
    "    np.random.seed(42 + run)\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run the appropriate algorithm\n",
    "        if config['algorithm'] == 'Hill Climbing':\n",
    "            # Create initial solution (may be invalid)\n",
    "            initial_solution = LeagueHillClimbingSolution(players=players_data)\n",
    "            \n",
    "            # Start counting fitness evaluations\n",
    "            fitness_counter.start_counting(LeagueHillClimbingSolution)\n",
    "            \n",
    "            # Run Hill Climbing\n",
    "            best_solution, best_fitness, history = hill_climbing(\n",
    "                initial_solution,\n",
    "                **config['params']\n",
    "            )\n",
    "            \n",
    "            # Get number of fitness evaluations\n",
    "            evaluations = fitness_counter.stop_counting(LeagueHillClimbingSolution)\n",
    "            \n",
    "            iterations = len(history)\n",
    "            \n",
    "        elif config['algorithm'] == 'Hill Climbing Valid':\n",
    "            # Create valid initial solution\n",
    "            initial_solution = generate_valid_solution(LeagueHillClimbingSolution, players_data)\n",
    "            \n",
    "            # If no valid solution could be found, return error result\n",
    "            if initial_solution is None:\n",
    "                return {\n",
    "                    'Configuration': config_name,\n",
    "                    'Algorithm': config['algorithm'],\n",
    "                    'Run': run + 1,\n",
    "                    'Best Fitness': float('inf'),\n",
    "                    'Iterations': 0,\n",
    "                    'Function Evaluations': 0,\n",
    "                    'Runtime (s)': 0,\n",
    "                    'History': [float('inf')]\n",
    "                }\n",
    "            \n",
    "            # Start counting fitness evaluations\n",
    "            fitness_counter.start_counting(LeagueHillClimbingSolution)\n",
    "            \n",
    "            # Run Hill Climbing\n",
    "            best_solution, best_fitness, history = hill_climbing(\n",
    "                initial_solution,\n",
    "                **config['params']\n",
    "            )\n",
    "            \n",
    "            # Get number of fitness evaluations\n",
    "            evaluations = fitness_counter.stop_counting(LeagueHillClimbingSolution)\n",
    "            \n",
    "            iterations = len(history)\n",
    "            \n",
    "        elif config['algorithm'] == 'Simulated Annealing':\n",
    "            # Create initial solution\n",
    "            initial_solution = LeagueSASolution(players=players_data)\n",
    "            \n",
    "            # Start counting fitness evaluations\n",
    "            fitness_counter.start_counting(LeagueSASolution)\n",
    "            \n",
    "            # Run Simulated Annealing\n",
    "            best_solution, best_fitness, history = simulated_annealing(\n",
    "                initial_solution,\n",
    "                **config['params']\n",
    "            )\n",
    "            \n",
    "            # Get number of fitness evaluations\n",
    "            evaluations = fitness_counter.stop_counting(LeagueSASolution)\n",
    "            \n",
    "            iterations = len(history)\n",
    "            \n",
    "        elif config['algorithm'] in ['Genetic Algorithm', 'Hybrid GA']:\n",
    "            # Start counting fitness evaluations\n",
    "            fitness_counter.start_counting(LeagueSolution)\n",
    "            \n",
    "            # For GA_Hybrid, ensure LeagueHillClimbingSolution is imported\n",
    "            if config_name == 'GA_Hybrid':\n",
    "                # Import LeagueHillClimbingSolution for local search\n",
    "                from solution import LeagueHillClimbingSolution\n",
    "                \n",
    "                # Make sure the local search algorithm is properly configured\n",
    "                if 'local_search' in config['params']:\n",
    "                    local_search = config['params']['local_search']\n",
    "                    if local_search['algorithm'] == 'hill_climbing':\n",
    "                        # Ensure hill_climbing is imported\n",
    "                        from evolution import hill_climbing\n",
    "            \n",
    "            # Run Genetic Algorithm\n",
    "            best_solution, best_fitness, history = genetic_algorithm(\n",
    "                players_data,\n",
    "                **config['params']\n",
    "            )\n",
    "            \n",
    "            # Get number of fitness evaluations\n",
    "            evaluations = fitness_counter.stop_counting(LeagueSolution)\n",
    "            \n",
    "            iterations = len(history)\n",
    "        \n",
    "        # Record end time and calculate runtime\n",
    "        runtime = time.time() - start_time\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            'Configuration': config_name,\n",
    "            'Algorithm': config['algorithm'],\n",
    "            'Run': run + 1,\n",
    "            'Best Fitness': best_fitness,\n",
    "            'Iterations': iterations,\n",
    "            'Function Evaluations': evaluations,\n",
    "            'Runtime (s)': runtime,\n",
    "            'History': history\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Return error information\n",
    "        return {\n",
    "            'Configuration': config_name,\n",
    "            'Algorithm': config['algorithm'],\n",
    "            'Run': run + 1,\n",
    "            'Error': str(e),\n",
    "            'Best Fitness': float('inf'),\n",
    "            'Iterations': 0,\n",
    "            'Function Evaluations': 0,\n",
    "            'Runtime (s)': 0,\n",
    "            'History': [float('inf')]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a function to run all experiments, with options for parallel or sequential execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(configs, players_data, num_runs=5, parallel=True, max_workers=None, save_csv=True):\n",
    "    \"\"\"\n",
    "    Run experiments with all configurations and collect results.\n",
    "    \n",
    "    Args:\n",
    "        configs (dict): Dictionary of configurations\n",
    "        players_data (list): List of player dictionaries\n",
    "        num_runs (int): Number of runs per configuration\n",
    "        parallel (bool): Whether to run experiments in parallel\n",
    "        max_workers (int): Maximum number of worker processes (None = auto)\n",
    "        save_csv (bool): Whether to save results to CSV\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Results of all experiments\n",
    "    \"\"\"\n",
    "    all_tasks = []\n",
    "    \n",
    "    # Prepare all tasks\n",
    "    for config_name, config in configs.items():\n",
    "        for run in range(num_runs):\n",
    "            all_tasks.append((config_name, config, players_data, run))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if parallel:\n",
    "        print(f\"Running {len(all_tasks)} experiments in parallel mode...\")\n",
    "        \n",
    "        # Run experiments in parallel\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all tasks\n",
    "            future_to_task = {executor.submit(run_single_experiment, *task): task for task in all_tasks}\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for i, future in enumerate(concurrent.futures.as_completed(future_to_task)):\n",
    "                task = future_to_task[future]\n",
    "                config_name, _, _, run = task\n",
    "                \n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Check if there was an error\n",
    "                    if 'Error' in result:\n",
    "                        print(f\"  Error in {config_name} - Run {run + 1}: {result['Error']}\")\n",
    "                    else:\n",
    "                        print(f\"  Completed {config_name} - Run {run + 1}: Fitness = {result['Best Fitness']:.6f}, \"\n",
    "                              f\"Evaluations = {result['Function Evaluations']}, Runtime = {result['Runtime (s)']:.2f}s\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error in {config_name} - Run {run + 1}: {e}\")\n",
    "    else:\n",
    "        print(f\"Running {len(all_tasks)} experiments in sequential mode...\")\n",
    "        \n",
    "        # Run experiments sequentially\n",
    "        for task in all_tasks:\n",
    "            config_name, config, players_data, run = task\n",
    "            print(f\"Running {config_name} - Run {run + 1}...\")\n",
    "            \n",
    "            try:\n",
    "                result = run_single_experiment(config_name, config, players_data, run)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Check if there was an error\n",
    "                if 'Error' in result:\n",
    "                    print(f\"  Error: {result['Error']}\")\n",
    "                else:\n",
    "                    print(f\"  Completed: Fitness = {result['Best Fitness']:.6f}, \"\n",
    "                          f\"Evaluations = {result['Function Evaluations']}, Runtime = {result['Runtime (s)']:.2f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results to CSV if requested\n",
    "    if save_csv:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_filename = f\"experiment_results_{timestamp}.csv\"\n",
    "        \n",
    "        # Create a copy without the history column for CSV export\n",
    "        # Use errors='ignore' to avoid KeyError if 'History' column doesn't exist\n",
    "        export_df = results_df.drop(columns=['History'], errors='ignore')\n",
    "        export_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"Results saved to {csv_filename}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithm Implementations\n",
    "\n",
    "### 3.1 Hill Climbing\n",
    "\n",
    "Hill Climbing is a local search algorithm that starts with an initial solution and iteratively moves to better neighboring solutions until no improvement is possible.\n",
    "\n",
    "**Key Components:**\n",
    "- **Neighborhood Generation**: Defined in `LeagueHillClimbingSolution.get_neighbors()`, which generates valid neighboring solutions by swapping players between teams.\n",
    "- **Selection Strategy**: We use steepest ascent, selecting the best neighbor at each iteration.\n",
    "- **Termination Criteria**: The algorithm stops when no better neighbor is found or after a maximum number of iterations.\n",
    "\n",
    "### 3.2 Simulated Annealing\n",
    "\n",
    "Simulated Annealing is inspired by the annealing process in metallurgy. It allows accepting worse solutions with a probability that decreases over time, helping to escape local optima.\n",
    "\n",
    "**Key Components:**\n",
    "- **Random Neighbor Generation**: Defined in `LeagueSASolution.get_random_neighbor()`, which generates a random valid neighboring solution.\n",
    "- **Acceptance Probability**: Based on the temperature and the fitness difference between the current and new solutions.\n",
    "- **Cooling Schedule**: The temperature decreases over time, reducing the probability of accepting worse solutions.\n",
    "\n",
    "### 3.3 Genetic Algorithm\n",
    "\n",
    "Genetic Algorithm is a population-based search algorithm inspired by natural selection and genetics.\n",
    "\n",
    "**Key Components:**\n",
    "- **Selection Operators**: We've implemented three selection mechanisms:\n",
    "  - Tournament Selection: Selects the best solution from k random candidates.\n",
    "  - Ranking Selection: Selects solutions with probability proportional to their rank.\n",
    "  - Boltzmann Selection: Uses Boltzmann distribution to select solutions.\n",
    "\n",
    "- **Crossover Operators**: We've implemented three crossover operators:\n",
    "  - One-Point Crossover: Creates a child by taking a portion from each parent.\n",
    "  - One-Point Prefer Valid: Tries multiple cut points to find a valid solution.\n",
    "  - Uniform Crossover: Creates a child by randomly selecting genes from either parent.\n",
    "\n",
    "- **Mutation Operators**: We've implemented four mutation operators:\n",
    "  - Swap: Randomly swaps two players between teams.\n",
    "  - Swap Constrained: Swaps players of the same position.\n",
    "  - Team Shift: Shifts all player assignments by a random number.\n",
    "  - Targeted Player Exchange: Swaps players between teams to improve balance.\n",
    "  - Shuffle Within Team: Shuffles players within a team with other teams.\n",
    "\n",
    "- **Elitism**: Preserves the best solutions from one generation to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Comparison\n",
    "\n",
    "Let's run the experiments and compare the performance of different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments with all configurations\n",
    "# You can choose between parallel (parallel=True) and sequential (parallel=False) execution\n",
    "# Set max_workers to control the number of parallel processes (None = auto)\n",
    "# Set save_csv=True to save results to CSV file\n",
    "\n",
    "results_df = run_experiments(\n",
    "    configs=configs, \n",
    "    players_data=players_data, \n",
    "    num_runs=5,\n",
    "    parallel=True,  # Set to False for sequential execution\n",
    "    max_workers=None,  # None = auto, or specify a number\n",
    "    save_csv=True  # Save results to CSV\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Solution Quality Comparison\n",
    "\n",
    "Let's compare the quality of solutions found by different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for each configuration\n",
    "summary = results_df.groupby('Configuration')['Best Fitness'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "summary = summary.sort_values('mean')\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Solution Quality Summary (lower is better):\")\n",
    "print(summary)\n",
    "\n",
    "# Plot solution quality comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Configuration', y='Best Fitness', data=results_df, order=summary['Configuration'])\n",
    "plt.title('Solution Quality Comparison (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Computational Efficiency Comparison\n",
    "\n",
    "Let's compare the computational efficiency of different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for runtime and function evaluations\n",
    "runtime_summary = results_df.groupby('Configuration')['Runtime (s)'].mean().reset_index()\n",
    "runtime_summary = runtime_summary.sort_values('Runtime (s)')\n",
    "\n",
    "evaluations_summary = results_df.groupby('Configuration')['Function Evaluations'].mean().reset_index()\n",
    "evaluations_summary = evaluations_summary.sort_values('Function Evaluations')\n",
    "\n",
    "# Plot runtime comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Configuration', y='Runtime (s)', data=runtime_summary)\n",
    "plt.title('Runtime Comparison (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot function evaluations comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Configuration', y='Function Evaluations', data=evaluations_summary)\n",
    "plt.title('Function Evaluations Comparison (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Convergence Analysis\n",
    "\n",
    "Let's analyze the convergence behavior of different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curves for each algorithm (using the first run)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for config_name in configs.keys():\n",
    "    # Try to get the first run for this configuration\n",
    "    run_data = results_df[(results_df['Configuration'] == config_name) & (results_df['Run'] == 1)]\n",
    "    \n",
    "    # Check if we have data for this configuration\n",
    "    if not run_data.empty:\n",
    "        run_data = run_data.iloc[0]\n",
    "        \n",
    "        # Check if we have history data\n",
    "        if 'History' in run_data and run_data['History'] is not None:\n",
    "            # Plot the convergence curve\n",
    "            plt.plot(run_data['History'], label=config_name)\n",
    "        else:\n",
    "            print(f\"No history data available for {config_name}\")\n",
    "    else:\n",
    "        print(f\"No data available for {config_name} - Run 1\")\n",
    "\n",
    "plt.title('Convergence Curves (first run)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Fitness (lower is better)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized convergence curves (by function evaluations)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for config_name in configs.keys():\n",
    "    # Try to get the first run for this configuration\n",
    "    run_data = results_df[(results_df['Configuration'] == config_name) & (results_df['Run'] == 1)]\n",
    "    \n",
    "    # Check if we have data for this configuration\n",
    "    if not run_data.empty:\n",
    "        run_data = run_data.iloc[0]\n",
    "        \n",
    "        # Check if we have history data and function evaluations\n",
    "        if ('History' in run_data and run_data['History'] is not None and \n",
    "            'Function Evaluations' in run_data and run_data['Function Evaluations'] > 0):\n",
    "            \n",
    "            # Calculate evaluations per iteration\n",
    "            history_length = len(run_data['History'])\n",
    "            if history_length > 0:\n",
    "                evals_per_iter = run_data['Function Evaluations'] / history_length\n",
    "                \n",
    "                # Create x-axis values (cumulative evaluations)\n",
    "                x_values = [i * evals_per_iter for i in range(history_length)]\n",
    "                \n",
    "                # Plot the normalized convergence curve\n",
    "                plt.plot(x_values, run_data['History'], label=config_name)\n",
    "            else:\n",
    "                print(f\"Empty history for {config_name}\")\n",
    "        else:\n",
    "            print(f\"No history data or function evaluations available for {config_name}\")\n",
    "    else:\n",
    "        print(f\"No data available for {config_name} - Run 1\")\n",
    "\n",
    "plt.title('Normalized Convergence Curves (by function evaluations)')\n",
    "plt.xlabel('Function Evaluations')\n",
    "plt.ylabel('Fitness (lower is better)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n",
    "\n",
    "Let's perform statistical tests to determine if the differences between algorithms are significant. We'll follow a structured decision flow to select the appropriate statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(results_df):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis on experiment results following a structured decision flow.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pandas.DataFrame): DataFrame containing experiment results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing statistical test results\n",
    "    \"\"\"\n",
    "    # Filter out rows with errors or infinite fitness\n",
    "    valid_results = results_df[~results_df['Best Fitness'].isin([float('inf'), float('-inf')])].copy()\n",
    "    \n",
    "    # Check if we have enough valid data\n",
    "    if len(valid_results) < 2:\n",
    "        print(\"Not enough valid data to perform statistical analysis.\")\n",
    "        return {}\n",
    "    \n",
    "    # Get unique configurations with valid results\n",
    "    configurations = valid_results['Configuration'].unique()\n",
    "    \n",
    "    # Step 1: Check if we have at least 2 configurations to compare\n",
    "    if len(configurations) < 2:\n",
    "        print(\"Not enough configurations with valid results to perform statistical analysis.\")\n",
    "        return {}\n",
    "    \n",
    "    # Step 2: Determine if we have 2 or more configurations\n",
    "    if len(configurations) == 2:\n",
    "        print(\"\\n=== Two-Group Comparison ===\")\n",
    "        return two_group_comparison(valid_results, configurations)\n",
    "    else:\n",
    "        print(\"\\n=== Multiple-Group Comparison ===\")\n",
    "        return multiple_group_comparison(valid_results, configurations)\n",
    "\n",
    "def two_group_comparison(results_df, configurations):\n",
    "    \"\"\"\n",
    "    Perform statistical comparison between two groups.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pandas.DataFrame): DataFrame containing experiment results\n",
    "        configurations (array): Array of configuration names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing statistical test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract data for each configuration\n",
    "        group1 = results_df[results_df['Configuration'] == configurations[0]]['Best Fitness'].values\n",
    "        group2 = results_df[results_df['Configuration'] == configurations[1]]['Best Fitness'].values\n",
    "        \n",
    "        # Check if we have enough data in each group\n",
    "        if len(group1) < 2 or len(group2) < 2:\n",
    "            print(f\"Not enough data in groups: {configurations[0]}={len(group1)}, {configurations[1]}={len(group2)}\")\n",
    "            return {}\n",
    "        \n",
    "        # Step 3: Test for normality using Shapiro-Wilk test\n",
    "        print(\"Testing for normality (Shapiro-Wilk):\")\n",
    "        _, p_value1 = stats.shapiro(group1)\n",
    "        _, p_value2 = stats.shapiro(group2)\n",
    "        print(f\"  {configurations[0]}: p-value = {p_value1:.4f} ({'Normal' if p_value1 > 0.05 else 'Non-normal'})\")\n",
    "        print(f\"  {configurations[1]}: p-value = {p_value2:.4f} ({'Normal' if p_value2 > 0.05 else 'Non-normal'})\")\n",
    "        \n",
    "        # Both groups must be normal to use parametric tests\n",
    "        is_normal = p_value1 > 0.05 and p_value2 > 0.05\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if is_normal:\n",
    "            # Step 4a: Test for equal variances using Levene's test\n",
    "            print(\"\\nTesting for equal variances (Levene's test):\")\n",
    "            _, p_value_var = stats.levene(group1, group2)\n",
    "            equal_var = p_value_var > 0.05\n",
    "            print(f\"  p-value = {p_value_var:.4f} ({'Equal variances' if equal_var else 'Unequal variances'})\")\n",
    "            \n",
    "            # Step 5a: Perform t-test (either with equal or unequal variances)\n",
    "            if equal_var:\n",
    "                print(\"\\nPerforming Independent t-test (equal variances):\")\n",
    "                t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=True)\n",
    "                test_name = \"Independent t-test\"\n",
    "            else:\n",
    "                print(\"\\nPerforming Welch's t-test (unequal variances):\")\n",
    "                t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)\n",
    "                test_name = \"Welch's t-test\"\n",
    "                \n",
    "            print(f\"  {test_name}: t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "            print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            mean1, mean2 = np.mean(group1), np.mean(group2)\n",
    "            std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
    "            n1, n2 = len(group1), len(group2)\n",
    "            \n",
    "            # Pooled standard deviation\n",
    "            pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "            cohen_d = abs(mean1 - mean2) / pooled_std\n",
    "            \n",
    "            print(f\"  Effect size (Cohen's d): {cohen_d:.4f}\")\n",
    "            \n",
    "            # Interpret Cohen's d\n",
    "            if cohen_d < 0.2:\n",
    "                effect_interpretation = \"Negligible effect\"\n",
    "            elif cohen_d < 0.5:\n",
    "                effect_interpretation = \"Small effect\"\n",
    "            elif cohen_d < 0.8:\n",
    "                effect_interpretation = \"Medium effect\"\n",
    "            else:\n",
    "                effect_interpretation = \"Large effect\"\n",
    "                \n",
    "            print(f\"  Interpretation: {effect_interpretation}\")\n",
    "            \n",
    "            results = {\n",
    "                'test': test_name,\n",
    "                'statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'effect_size': cohen_d,\n",
    "                'effect_interpretation': effect_interpretation\n",
    "            }\n",
    "        else:\n",
    "            # Step 4b: Perform Mann-Whitney U test (non-parametric)\n",
    "            print(\"\\nPerforming Mann-Whitney U test (non-parametric):\")\n",
    "            u_stat, p_value = stats.mannwhitneyu(group1, group2)\n",
    "            print(f\"  Mann-Whitney U: U-statistic = {u_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "            print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            \n",
    "            # Calculate effect size (r = Z / sqrt(N))\n",
    "            n1, n2 = len(group1), len(group2)\n",
    "            n_total = n1 + n2\n",
    "            \n",
    "            # Convert U to Z\n",
    "            mean_u = n1 * n2 / 2\n",
    "            std_u = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)\n",
    "            z = (u_stat - mean_u) / std_u\n",
    "            \n",
    "            # Calculate effect size r\n",
    "            r = abs(z) / np.sqrt(n_total)\n",
    "            \n",
    "            print(f\"  Effect size (r): {r:.4f}\")\n",
    "            \n",
    "            # Interpret r\n",
    "            if r < 0.1:\n",
    "                effect_interpretation = \"Negligible effect\"\n",
    "            elif r < 0.3:\n",
    "                effect_interpretation = \"Small effect\"\n",
    "            elif r < 0.5:\n",
    "                effect_interpretation = \"Medium effect\"\n",
    "            else:\n",
    "                effect_interpretation = \"Large effect\"\n",
    "                \n",
    "            print(f\"  Interpretation: {effect_interpretation}\")\n",
    "            \n",
    "            results = {\n",
    "                'test': 'Mann-Whitney U',\n",
    "                'statistic': u_stat,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'effect_size': r,\n",
    "                'effect_interpretation': effect_interpretation\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in two-group comparison: {e}\")\n",
    "        return {}\n",
    "\n",
    "def multiple_group_comparison(results_df, configurations):\n",
    "    \"\"\"\n",
    "    Perform statistical comparison between multiple groups.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pandas.DataFrame): DataFrame containing experiment results\n",
    "        configurations (array): Array of configuration names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing statistical test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract data for each configuration\n",
    "        groups = []\n",
    "        valid_configs = []\n",
    "        \n",
    "        for config in configurations:\n",
    "            group_data = results_df[results_df['Configuration'] == config]['Best Fitness'].values\n",
    "            if len(group_data) >= 2:  # Need at least 2 samples for statistical tests\n",
    "                groups.append(group_data)\n",
    "                valid_configs.append(config)\n",
    "            else:\n",
    "                print(f\"Skipping {config} due to insufficient data (n={len(group_data)})\")\n",
    "        \n",
    "        # Check if we have enough valid groups\n",
    "        if len(groups) < 2:\n",
    "            print(\"Not enough valid groups for comparison\")\n",
    "            return {}\n",
    "        \n",
    "        # Update configurations list to only include valid ones\n",
    "        configurations = valid_configs\n",
    "        \n",
    "        # Step 3: Test for normality using Shapiro-Wilk test\n",
    "        print(\"Testing for normality (Shapiro-Wilk):\")\n",
    "        normality_results = []\n",
    "        for i, config in enumerate(configurations):\n",
    "            try:\n",
    "                _, p_value = stats.shapiro(groups[i])\n",
    "                is_normal = p_value > 0.05\n",
    "                normality_results.append(is_normal)\n",
    "                print(f\"  {config}: p-value = {p_value:.4f} ({'Normal' if is_normal else 'Non-normal'})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {config}: Error in normality test - {e}\")\n",
    "                normality_results.append(False)  # Assume non-normal if test fails\n",
    "        \n",
    "        # All groups must be normal to use parametric tests\n",
    "        all_normal = all(normality_results)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if all_normal:\n",
    "            # Step 4a: Perform ANOVA (parametric)\n",
    "            print(\"\\nPerforming One-way ANOVA (parametric):\")\n",
    "            f_stat, p_value = stats.f_oneway(*groups)\n",
    "            print(f\"  ANOVA: F-statistic = {f_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "            print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            \n",
    "            # Calculate effect size (Eta-squared)\n",
    "            # Flatten all groups into a single array\n",
    "            all_values = np.concatenate(groups)\n",
    "            grand_mean = np.mean(all_values)\n",
    "            \n",
    "            # Calculate sum of squares between groups (SSB)\n",
    "            ssb = sum(len(group) * (np.mean(group) - grand_mean)**2 for group in groups)\n",
    "            \n",
    "            # Calculate sum of squares total (SST)\n",
    "            sst = sum((x - grand_mean)**2 for x in all_values)\n",
    "            \n",
    "            # Calculate Eta-squared\n",
    "            eta_squared = ssb / sst\n",
    "            \n",
    "            print(f\"  Effect size (Eta-squared): {eta_squared:.4f}\")\n",
    "            \n",
    "            # Interpret Eta-squared\n",
    "            if eta_squared < 0.01:\n",
    "                effect_interpretation = \"Negligible effect\"\n",
    "            elif eta_squared < 0.06:\n",
    "                effect_interpretation = \"Small effect\"\n",
    "            elif eta_squared < 0.14:\n",
    "                effect_interpretation = \"Medium effect\"\n",
    "            else:\n",
    "                effect_interpretation = \"Large effect\"\n",
    "                \n",
    "            print(f\"  Interpretation: {effect_interpretation}\")\n",
    "            \n",
    "            results = {\n",
    "                'test': 'One-way ANOVA',\n",
    "                'statistic': f_stat,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'effect_size': eta_squared,\n",
    "                'effect_interpretation': effect_interpretation\n",
    "            }\n",
    "            \n",
    "            # Step 5a: Perform post-hoc tests if ANOVA is significant\n",
    "            if p_value < 0.05:\n",
    "                try:\n",
    "                    print(\"\\nPerforming Tukey HSD post-hoc test:\")\n",
    "                    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "                    \n",
    "                    # Prepare data for Tukey HSD\n",
    "                    fitness_values = []\n",
    "                    config_labels = []\n",
    "                    \n",
    "                    for i, config in enumerate(configurations):\n",
    "                        for value in groups[i]:\n",
    "                            fitness_values.append(value)\n",
    "                            config_labels.append(config)\n",
    "                    \n",
    "                    # Perform Tukey HSD test\n",
    "                    tukey_results = pairwise_tukeyhsd(fitness_values, config_labels, alpha=0.05)\n",
    "                    print(tukey_results)\n",
    "                    \n",
    "                    # Store significant pairs\n",
    "                    significant_pairs = []\n",
    "                    for i, row in enumerate(tukey_results.summary().data[1:]):\n",
    "                        group1, group2, _, _, _, reject = row\n",
    "                        if reject:\n",
    "                            significant_pairs.append((group1, group2))\n",
    "                    \n",
    "                    results['post_hoc'] = {\n",
    "                        'test': 'Tukey HSD',\n",
    "                        'significant_pairs': significant_pairs\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in Tukey HSD test: {e}\")\n",
    "        else:\n",
    "            # Step 4b: Perform Kruskal-Wallis test (non-parametric)\n",
    "            print(\"\\nPerforming Kruskal-Wallis test (non-parametric):\")\n",
    "            try:\n",
    "                h_stat, p_value = stats.kruskal(*groups)\n",
    "                print(f\"  Kruskal-Wallis: H-statistic = {h_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "                print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "                \n",
    "                # Calculate effect size (Eta-squared for Kruskal-Wallis)\n",
    "                n_total = sum(len(group) for group in groups)\n",
    "                eta_squared_h = (h_stat - len(groups) + 1) / (n_total - len(groups))\n",
    "                eta_squared_h = max(0, eta_squared_h)  # Ensure non-negative\n",
    "                \n",
    "                print(f\"  Effect size (Eta-squared H): {eta_squared_h:.4f}\")\n",
    "                \n",
    "                # Interpret Eta-squared H (same thresholds as Eta-squared)\n",
    "                if eta_squared_h < 0.01:\n",
    "                    effect_interpretation = \"Negligible effect\"\n",
    "                elif eta_squared_h < 0.06:\n",
    "                    effect_interpretation = \"Small effect\"\n",
    "                elif eta_squared_h < 0.14:\n",
    "                    effect_interpretation = \"Medium effect\"\n",
    "                else:\n",
    "                    effect_interpretation = \"Large effect\"\n",
    "                    \n",
    "                print(f\"  Interpretation: {effect_interpretation}\")\n",
    "                \n",
    "                results = {\n",
    "                    'test': 'Kruskal-Wallis',\n",
    "                    'statistic': h_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'significant': p_value < 0.05,\n",
    "                    'effect_size': eta_squared_h,\n",
    "                    'effect_interpretation': effect_interpretation\n",
    "                }\n",
    "                \n",
    "                # Step 5b: Perform post-hoc tests if Kruskal-Wallis is significant\n",
    "                if p_value < 0.05:\n",
    "                    try:\n",
    "                        print(\"\\nPerforming Dunn's test with Bonferroni correction:\")\n",
    "                        from scikit_posthocs import posthoc_dunn\n",
    "                        \n",
    "                        # Prepare data for Dunn's test\n",
    "                        # Convert to DataFrame format for scikit_posthocs\n",
    "                        import pandas as pd\n",
    "                        dunn_data = pd.DataFrame()\n",
    "                        for i, config in enumerate(configurations):\n",
    "                            dunn_data[config] = pd.Series(groups[i])\n",
    "                        \n",
    "                        # Perform Dunn's test\n",
    "                        dunn_results = posthoc_dunn(dunn_data, p_adjust='bonferroni')\n",
    "                        print(dunn_results)\n",
    "                        \n",
    "                        # Store significant pairs\n",
    "                        significant_pairs = []\n",
    "                        for i in range(len(configurations)):\n",
    "                            for j in range(i+1, len(configurations)):\n",
    "                                if dunn_results.iloc[i, j] < 0.05:\n",
    "                                    significant_pairs.append((configurations[i], configurations[j]))\n",
    "                        \n",
    "                        results['post_hoc'] = {\n",
    "                            'test': \"Dunn's test with Bonferroni correction\",\n",
    "                            'significant_pairs': significant_pairs\n",
    "                        }\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in Dunn's test: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Kruskal-Wallis test: {e}\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in multiple-group comparison: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Perform statistical analysis\n",
    "try:\n",
    "    statistical_results = perform_statistical_analysis(results_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error in statistical analysis: {e}\")\n",
    "    statistical_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualizing Statistical Results\n",
    "\n",
    "Let's create visualizations that incorporate the statistical significance information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_significance(results_df, statistical_results):\n",
    "    \"\"\"\n",
    "    Create boxplot with statistical significance annotations.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pandas.DataFrame): DataFrame containing experiment results\n",
    "        statistical_results (dict): Dictionary containing statistical test results\n",
    "    \"\"\"\n",
    "    # Filter out rows with errors or infinite fitness\n",
    "    valid_results = results_df[~results_df['Best Fitness'].isin([float('inf'), float('-inf')])].copy()\n",
    "    \n",
    "    # Check if we have enough valid data\n",
    "    if len(valid_results) < 2:\n",
    "        print(\"Not enough valid data to create visualization.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate summary statistics for each configuration\n",
    "    summary = valid_results.groupby('Configuration')['Best Fitness'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    summary = summary.sort_values('mean')\n",
    "    \n",
    "    # Create boxplot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.boxplot(x='Configuration', y='Best Fitness', data=valid_results, order=summary['Configuration'])\n",
    "    \n",
    "    # Add statistical significance annotations if available\n",
    "    if statistical_results and 'significant' in statistical_results and statistical_results['significant']:\n",
    "        if 'post_hoc' in statistical_results and 'significant_pairs' in statistical_results['post_hoc']:\n",
    "            # Get the ordered configurations\n",
    "            ordered_configs = summary['Configuration'].tolist()\n",
    "            \n",
    "            # Get significant pairs from post-hoc tests\n",
    "            significant_pairs = statistical_results['post_hoc']['significant_pairs']\n",
    "            \n",
    "            # Add significance bars\n",
    "            y_max = valid_results['Best Fitness'].max()\n",
    "            y_range = valid_results['Best Fitness'].max() - valid_results['Best Fitness'].min()\n",
    "            bar_height = y_range * 0.05\n",
    "            \n",
    "            for i, (config1, config2) in enumerate(significant_pairs):\n",
    "                # Check if both configs are in the ordered list\n",
    "                if config1 in ordered_configs and config2 in ordered_configs:\n",
    "                    # Get indices in the ordered list\n",
    "                    idx1 = ordered_configs.index(config1)\n",
    "                    idx2 = ordered_configs.index(config2)\n",
    "                    \n",
    "                    # Ensure idx1 < idx2\n",
    "                    if idx1 > idx2:\n",
    "                        idx1, idx2 = idx2, idx1\n",
    "                        config1, config2 = config2, config1\n",
    "                    \n",
    "                    # Calculate bar position\n",
    "                    y_pos = y_max + bar_height * (i + 1)\n",
    "                    \n",
    "                    # Draw the bar\n",
    "                    plt.plot([idx1, idx2], [y_pos, y_pos], 'k-', linewidth=1.5)\n",
    "                    plt.plot([idx1, idx1], [y_pos - bar_height/2, y_pos], 'k-', linewidth=1.5)\n",
    "                    plt.plot([idx2, idx2], [y_pos - bar_height/2, y_pos], 'k-', linewidth=1.5)\n",
    "                    \n",
    "                    # Add asterisk\n",
    "                    plt.text((idx1 + idx2) / 2, y_pos + bar_height/4, '*', ha='center', va='center', fontsize=14)\n",
    "        elif 'p_value' in statistical_results:\n",
    "            # For two-group comparison, add a single significance indicator\n",
    "            plt.title(f\"Solution Quality Comparison (p = {statistical_results['p_value']:.4f}, {statistical_results['effect_interpretation']})\")\n",
    "    \n",
    "    plt.title('Solution Quality Comparison with Statistical Significance')\n",
    "    plt.xlabel('Configuration')\n",
    "    plt.ylabel('Best Fitness (lower is better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot with significance annotations\n",
    "try:\n",
    "    plot_with_significance(results_df, statistical_results)\n",
    "except Exception as e:\n",
    "    print(f\"Error in significance visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Based on our experiments and analysis, we can draw the following conclusions:\n",
    "\n",
    "1. **Solution Quality**: [To be filled after running experiments]\n",
    "2. **Computational Efficiency**: [To be filled after running experiments]\n",
    "3. **Convergence Behavior**: [To be filled after running experiments]\n",
    "4. **Statistical Significance**: [To be filled after running experiments]\n",
    "\n",
    "### 6.1 Best Algorithm for the Sports League Problem\n",
    "\n",
    "[To be filled after running experiments]\n",
    "\n",
    "### 6.2 Trade-offs and Recommendations\n",
    "\n",
    "[To be filled after running experiments]\n",
    "\n",
    "### 6.3 Future Work\n",
    "\n",
    "1. Explore more advanced hybrid approaches\n",
    "2. Implement adaptive parameter tuning\n",
    "3. Test with larger problem instances\n",
    "4. Develop more specialized operators for the Sports League problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
