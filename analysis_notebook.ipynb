{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports League Optimization: Comparative Analysis of Algorithms\n",
    "\n",
    "This notebook presents a comprehensive analysis of different optimization algorithms applied to the Sports League problem. We compare Hill Climbing, Simulated Annealing, and Genetic Algorithm approaches, analyzing their performance across multiple metrics.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Problem Definition](#1-problem-definition)\n",
    "2. [Experimental Setup](#2-experimental-setup)\n",
    "3. [Algorithm Implementations](#3-algorithm-implementations)\n",
    "4. [Performance Comparison](#4-performance-comparison)\n",
    "5. [Statistical Analysis](#5-statistical-analysis)\n",
    "6. [Conclusion](#6-conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy import stats\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "\n",
    "# Import our custom modules\n",
    "from solution import LeagueSolution, LeagueHillClimbingSolution, LeagueSASolution\n",
    "from evolution import (\n",
    "    hill_climbing, \n",
    "    simulated_annealing, \n",
    "    genetic_algorithm,\n",
    "    # Mutation operators\n",
    "    mutate_swap,\n",
    "    mutate_swap_constrained,\n",
    "    mutate_team_shift,\n",
    "    mutate_targeted_player_exchange,\n",
    "    mutate_shuffle_within_team_constrained,\n",
    "    # Crossover operators\n",
    "    crossover_one_point,\n",
    "    crossover_one_point_prefer_valid,\n",
    "    crossover_uniform,\n",
    "    crossover_uniform_prefer_valid,\n",
    "    # Selection operators\n",
    "    selection_tournament,\n",
    "    selection_tournament_variable_k,\n",
    "    selection_ranking,\n",
    "    selection_boltzmann\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Definition\n",
    "\n",
    "### 1.1 Sports League Problem\n",
    "\n",
    "The Sports League problem involves assigning players to teams while satisfying specific constraints and optimizing for team balance. The goal is to create teams with similar average skill levels.\n",
    "\n",
    "**Formal Definition:**\n",
    "- We have 35 players with different positions (GK, DEF, MID, FWD) and skill levels\n",
    "- We need to assign these players to 5 teams (7 players per team)\n",
    "- Each team must have exactly 1 GK, 2 DEF, 2 MID, and 2 FWD\n",
    "- Each team's total salary must not exceed 750M â‚¬\n",
    "- The objective is to minimize the standard deviation of average team skills\n",
    "\n",
    "### 1.2 Solution Representation\n",
    "\n",
    "We represent a solution as a list of team assignments for each player. For example, if `solution.repr[0] = 2`, it means player 0 is assigned to team 2.\n",
    "\n",
    "**Search Space Size:**\n",
    "- For 35 players and 5 teams, the theoretical search space is 5^35\n",
    "- With constraints, the actual feasible search space is much smaller, but still extremely large\n",
    "\n",
    "### 1.3 Fitness Function\n",
    "\n",
    "The fitness function calculates the standard deviation of the average skill levels across all teams. A lower value indicates more balanced teams, which is our optimization goal.\n",
    "\n",
    "For invalid solutions (those violating constraints), we return infinity to ensure they are never selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load player data\n",
    "players_df = pd.read_csv(\"players.csv\", sep=\";\")\n",
    "players_data = players_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Display the player data\n",
    "players_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Analysis\n",
    "\n",
    "Let's analyze the player data to understand the distribution of skills, positions, and salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze player positions\n",
    "position_counts = players_df['Position'].value_counts()\n",
    "print(\"Position distribution:\")\n",
    "print(position_counts)\n",
    "\n",
    "# Analyze skill distribution by position\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Position', y='Skill', data=players_df)\n",
    "plt.title('Skill Distribution by Position')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Analyze salary distribution by position\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Position', y='Salary', data=players_df)\n",
    "plt.title('Salary Distribution by Position')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Correlation between skill and salary\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Skill', y='Salary', hue='Position', data=players_df)\n",
    "plt.title('Correlation between Skill and Salary')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experimental Setup\n",
    "\n",
    "### 2.1 Metrics for Comparison\n",
    "\n",
    "To ensure a fair comparison between different algorithms, we'll track the following metrics:\n",
    "\n",
    "1. **Solution Quality**: The fitness value (standard deviation of average team skills)\n",
    "2. **Function Evaluations**: Number of fitness function calls\n",
    "3. **Iterations**: Number of algorithm iterations\n",
    "4. **Runtime**: Actual execution time in seconds\n",
    "\n",
    "### 2.2 Algorithm Configurations\n",
    "\n",
    "We'll test the following algorithm configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define algorithm configurations\n",
    "configs = {\n",
    "    # Hill Climbing configurations\n",
    "    'HC_Standard': {\n",
    "        'algorithm': 'Hill Climbing',\n",
    "        'params': {\n",
    "            'max_iterations': 500,\n",
    "            'max_no_improvement': 100,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Simulated Annealing configurations\n",
    "    'SA_Standard': {\n",
    "        'algorithm': 'Simulated Annealing',\n",
    "        'params': {\n",
    "            'initial_temperature': 200.0,\n",
    "            'cooling_rate': 0.95,\n",
    "            'min_temperature': 1e-5,\n",
    "            'iterations_per_temp': 20,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Genetic Algorithm configurations\n",
    "    'GA_Tournament_OnePoint': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'params': {\n",
    "            'population_size': 100,\n",
    "            'max_generations': 50,\n",
    "            'selection_operator': selection_tournament,\n",
    "            'crossover_operator': crossover_one_point_prefer_valid,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_operator': mutate_swap_constrained,\n",
    "            'mutation_rate': 0.1,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 2,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    'GA_Ranking_Uniform': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'params': {\n",
    "            'population_size': 100,\n",
    "            'max_generations': 50,\n",
    "            'selection_operator': selection_ranking,\n",
    "            'crossover_operator': crossover_uniform_prefer_valid,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_operator': mutate_targeted_player_exchange,\n",
    "            'mutation_rate': 0.1,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 2,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    'GA_Boltzmann_TeamShift': {\n",
    "        'algorithm': 'Genetic Algorithm',\n",
    "        'params': {\n",
    "            'population_size': 100,\n",
    "            'max_generations': 50,\n",
    "            'selection_operator': selection_boltzmann,\n",
    "            'selection_params': {'temperature': 1.0},\n",
    "            'crossover_operator': crossover_one_point_prefer_valid,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_operator': mutate_team_shift,\n",
    "            'mutation_rate': 0.1,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 2,\n",
    "            'verbose': False\n",
    "        }\n",
    "    },\n",
    "    'GA_Hybrid': {\n",
    "        'algorithm': 'Hybrid GA',\n",
    "        'params': {\n",
    "            'population_size': 75,\n",
    "            'max_generations': 40,\n",
    "            'selection_operator': selection_tournament_variable_k,\n",
    "            'selection_params': {'k': 3},\n",
    "            'crossover_operator': crossover_uniform_prefer_valid,\n",
    "            'crossover_rate': 0.85,\n",
    "            'mutation_operator': mutate_shuffle_within_team_constrained,\n",
    "            'mutation_rate': 0.15,\n",
    "            'elitism': True,\n",
    "            'elitism_size': 1,\n",
    "            'local_search': {\n",
    "                'algorithm': 'hill_climbing',\n",
    "                'frequency': 5,  # Apply HC every 5 generations\n",
    "                'iterations': 50  # HC iterations per application\n",
    "            },\n",
    "            'verbose': False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display the configurations\n",
    "for name, config in configs.items():\n",
    "    print(f\"Configuration: {name}\")\n",
    "    print(f\"Algorithm: {config['algorithm']}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param, value in config['params'].items():\n",
    "        if param not in ['selection_operator', 'crossover_operator', 'mutation_operator', 'verbose']:\n",
    "            print(f\"  {param}: {value}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tracking Function Evaluations\n",
    "\n",
    "To ensure fair comparison between algorithms, we'll implement a counter for fitness function evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a wrapper for the fitness function to count evaluations\n",
    "class FitnessCounter:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.original_fitness = LeagueSolution.fitness\n",
    "        \n",
    "    def start_counting(self):\n",
    "        self.count = 0\n",
    "        LeagueSolution.fitness = self.counting_fitness\n",
    "        \n",
    "    def stop_counting(self):\n",
    "        LeagueSolution.fitness = self.original_fitness\n",
    "        return self.count\n",
    "    \n",
    "    def counting_fitness(self, solution):\n",
    "        self.count += 1\n",
    "        return self.original_fitness(solution)\n",
    "\n",
    "# Initialize the counter\n",
    "fitness_counter = FitnessCounter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Experiment Runner\n",
    "\n",
    "We'll create a function to run a single experiment with a specific configuration and run number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_single_experiment(config_name, config, players_data, run):\n",
    "    \"\"\"\n",
    "    Run a single experiment with a specific configuration and run number.\n",
    "    \n",
    "    Args:\n",
    "        config_name (str): Name of the configuration\n",
    "        config (dict): Configuration dictionary\n",
    "        players_data (list): List of player dictionaries\n",
    "        run (int): Run number (0-based)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results of the experiment\n",
    "    \"\"\"\n",
    "    # Reset random seed for this run to ensure reproducibility\n",
    "    random.seed(42 + run)\n",
    "    np.random.seed(42 + run)\n",
    "    \n",
    "    # Create a local fitness counter for this process\n",
    "    local_counter = FitnessCounter()\n",
    "    local_counter.start_counting()\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the appropriate algorithm\n",
    "    if config['algorithm'] == 'Hill Climbing':\n",
    "        # Create initial solution\n",
    "        initial_solution = LeagueHillClimbingSolution(players=players_data)\n",
    "        \n",
    "        # Run Hill Climbing\n",
    "        best_solution, best_fitness, history = hill_climbing(\n",
    "            initial_solution,\n",
    "            **config['params']\n",
    "        )\n",
    "        \n",
    "        iterations = len(history)\n",
    "        \n",
    "    elif config['algorithm'] == 'Simulated Annealing':\n",
    "        # Create initial solution\n",
    "        initial_solution = LeagueSASolution(players=players_data)\n",
    "        \n",
    "        # Run Simulated Annealing\n",
    "        best_solution, best_fitness, history = simulated_annealing(\n",
    "            initial_solution,\n",
    "            **config['params']\n",
    "        )\n",
    "        \n",
    "        iterations = len(history)\n",
    "        \n",
    "    elif config['algorithm'] in ['Genetic Algorithm', 'Hybrid GA']:\n",
    "        # Run Genetic Algorithm\n",
    "        best_solution, best_fitness, history = genetic_algorithm(\n",
    "            players_data,\n",
    "            **config['params']\n",
    "        )\n",
    "        \n",
    "        iterations = len(history)\n",
    "    \n",
    "    # Record end time and calculate runtime\n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    # Get number of fitness evaluations\n",
    "    evaluations = local_counter.stop_counting()\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'Configuration': config_name,\n",
    "        'Algorithm': config['algorithm'],\n",
    "        'Run': run + 1,\n",
    "        'Best Fitness': best_fitness,\n",
    "        'Iterations': iterations,\n",
    "        'Function Evaluations': evaluations,\n",
    "        'Runtime (s)': runtime,\n",
    "        'History': history\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a function to run all experiments, with options for parallel or sequential execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_experiments(configs, players_data, num_runs=5, parallel=True, max_workers=None, save_csv=True):\n",
    "    \"\"\"\n",
    "    Run experiments with all configurations and collect results.\n",
    "    \n",
    "    Args:\n",
    "        configs (dict): Dictionary of configurations\n",
    "        players_data (list): List of player dictionaries\n",
    "        num_runs (int): Number of runs per configuration\n",
    "        parallel (bool): Whether to run experiments in parallel\n",
    "        max_workers (int): Maximum number of worker processes (None = auto)\n",
    "        save_csv (bool): Whether to save results to CSV\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Results of all experiments\n",
    "    \"\"\"\n",
    "    all_tasks = []\n",
    "    \n",
    "    # Prepare all tasks\n",
    "    for config_name, config in configs.items():\n",
    "        for run in range(num_runs):\n",
    "            all_tasks.append((config_name, config, players_data, run))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if parallel:\n",
    "        print(f\"Running {len(all_tasks)} experiments in parallel mode...\")\n",
    "        \n",
    "        # Run experiments in parallel\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all tasks\n",
    "            future_to_task = {executor.submit(run_single_experiment, *task): task for task in all_tasks}\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for i, future in enumerate(concurrent.futures.as_completed(future_to_task)):\n",
    "                task = future_to_task[future]\n",
    "                config_name, _, _, run = task\n",
    "                \n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    print(f\"  Completed {config_name} - Run {run + 1}: Fitness = {result['Best Fitness']:.6f}, \"\n",
    "                          f\"Evaluations = {result['Function Evaluations']}, Runtime = {result['Runtime (s)']:.2f}s\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error in {config_name} - Run {run + 1}: {e}\")\n",
    "    else:\n",
    "        print(f\"Running {len(all_tasks)} experiments in sequential mode...\")\n",
    "        \n",
    "        # Run experiments sequentially\n",
    "        for task in all_tasks:\n",
    "            config_name, config, players_data, run = task\n",
    "            print(f\"Running {config_name} - Run {run + 1}...\")\n",
    "            \n",
    "            try:\n",
    "                result = run_single_experiment(config_name, config, players_data, run)\n",
    "                results.append(result)\n",
    "                print(f\"  Completed: Fitness = {result['Best Fitness']:.6f}, \"\n",
    "                      f\"Evaluations = {result['Function Evaluations']}, Runtime = {result['Runtime (s)']:.2f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results to CSV if requested\n",
    "    if save_csv:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_filename = f\"experiment_results_{timestamp}.csv\"\n",
    "        \n",
    "        # Create a copy without the history column for CSV export\n",
    "        export_df = results_df.drop(columns=['History'])\n",
    "        export_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"Results saved to {csv_filename}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithm Implementations\n",
    "\n",
    "### 3.1 Hill Climbing\n",
    "\n",
    "Hill Climbing is a local search algorithm that starts with an initial solution and iteratively moves to better neighboring solutions until no improvement is possible.\n",
    "\n",
    "**Key Components:**\n",
    "- **Neighborhood Generation**: Defined in `LeagueHillClimbingSolution.get_neighbors()`, which generates valid neighboring solutions by swapping players between teams.\n",
    "- **Selection Strategy**: We use steepest ascent, selecting the best neighbor at each iteration.\n",
    "- **Termination Criteria**: The algorithm stops when no better neighbor is found or after a maximum number of iterations.\n",
    "\n",
    "### 3.2 Simulated Annealing\n",
    "\n",
    "Simulated Annealing is inspired by the annealing process in metallurgy. It allows accepting worse solutions with a probability that decreases over time, helping to escape local optima.\n",
    "\n",
    "**Key Components:**\n",
    "- **Random Neighbor Generation**: Defined in `LeagueSASolution.get_random_neighbor()`, which generates a random valid neighboring solution.\n",
    "- **Acceptance Probability**: Based on the temperature and the fitness difference between the current and new solutions.\n",
    "- **Cooling Schedule**: The temperature decreases over time, reducing the probability of accepting worse solutions.\n",
    "\n",
    "### 3.3 Genetic Algorithm\n",
    "\n",
    "Genetic Algorithm is a population-based search algorithm inspired by natural selection and genetics.\n",
    "\n",
    "**Key Components:**\n",
    "- **Selection Operators**: We've implemented three selection mechanisms:\n",
    "  - Tournament Selection: Selects the best solution from k random candidates.\n",
    "  - Ranking Selection: Selects solutions with probability proportional to their rank.\n",
    "  - Boltzmann Selection: Uses Boltzmann distribution to select solutions.\n",
    "\n",
    "- **Crossover Operators**: We've implemented three crossover operators:\n",
    "  - One-Point Crossover: Creates a child by taking a portion from each parent.\n",
    "  - One-Point Prefer Valid: Tries multiple cut points to find a valid solution.\n",
    "  - Uniform Crossover: Creates a child by randomly selecting genes from either parent.\n",
    "\n",
    "- **Mutation Operators**: We've implemented four mutation operators:\n",
    "  - Swap: Randomly swaps two players between teams.\n",
    "  - Swap Constrained: Swaps players of the same position.\n",
    "  - Team Shift: Shifts all player assignments by a random number.\n",
    "  - Targeted Player Exchange: Swaps players between teams to improve balance.\n",
    "  - Shuffle Within Team: Shuffles players within a team with other teams.\n",
    "\n",
    "- **Elitism**: Preserves the best solutions from one generation to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Comparison\n",
    "\n",
    "Let's run the experiments and compare the performance of different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run experiments with all configurations\n",
    "# You can choose between parallel (parallel=True) and sequential (parallel=False) execution\n",
    "# Set max_workers to control the number of parallel processes (None = auto)\n",
    "# Set save_csv=True to save results to CSV file\n",
    "\n",
    "results_df = run_experiments(\n",
    "    configs=configs, \n",
    "    players_data=players_data, \n",
    "    num_runs=5,\n",
    "    parallel=True,  # Set to False for sequential execution\n",
    "    max_workers=None,  # None = auto, or specify a number\n",
    "    save_csv=True  # Save results to CSV\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Solution Quality Comparison\n",
    "\n",
    "Let's compare the quality of solutions found by different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate summary statistics for each configuration\n",
    "summary = results_df.groupby('Configuration')['Best Fitness'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "summary = summary.sort_values('mean')\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Solution Quality Summary (lower is better):\")\n",
    "print(summary)\n",
    "\n",
    "# Plot solution quality comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Configuration', y='Best Fitness', data=results_df, order=summary['Configuration'])\n",
    "plt.title('Solution Quality Comparison (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Computational Efficiency Comparison\n",
    "\n",
    "Let's compare the computational efficiency of different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate summary statistics for runtime and function evaluations\n",
    "runtime_summary = results_df.groupby('Configuration')['Runtime (s)'].mean().reset_index()\n",
    "runtime_summary = runtime_summary.sort_values('Runtime (s)')\n",
    "\n",
    "evaluations_summary = results_df.groupby('Configuration')['Function Evaluations'].mean().reset_index()\n",
    "evaluations_summary = evaluations_summary.sort_values('Function Evaluations')\n",
    "\n",
    "# Plot runtime comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Configuration', y='Runtime (s)', data=runtime_summary)\n",
    "plt.title('Runtime Comparison (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot function evaluations comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Configuration', y='Function Evaluations', data=evaluations_summary)\n",
    "plt.title('Function Evaluations Comparison (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Convergence Analysis\n",
    "\n",
    "Let's analyze the convergence behavior of different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot convergence curves for each algorithm (using the first run)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for config_name in configs.keys():\n",
    "    # Get the first run for this configuration\n",
    "    run_data = results_df[(results_df['Configuration'] == config_name) & (results_df['Run'] == 1)].iloc[0]\n",
    "    \n",
    "    # Plot the convergence curve\n",
    "    plt.plot(run_data['History'], label=config_name)\n",
    "\n",
    "plt.title('Convergence Curves (first run)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Fitness (lower is better)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized convergence curves (by function evaluations)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for config_name in configs.keys():\n",
    "    # Get the first run for this configuration\n",
    "    run_data = results_df[(results_df['Configuration'] == config_name) & (results_df['Run'] == 1)].iloc[0]\n",
    "    \n",
    "    # Calculate evaluations per iteration\n",
    "    evals_per_iter = run_data['Function Evaluations'] / len(run_data['History'])\n",
    "    \n",
    "    # Create x-axis values (cumulative evaluations)\n",
    "    x_values = [i * evals_per_iter for i in range(len(run_data['History']))]\n",
    "    \n",
    "    # Plot the normalized convergence curve\n",
    "    plt.plot(x_values, run_data['History'], label=config_name)\n",
    "\n",
    "plt.title('Normalized Convergence Curves (by function evaluations)')\n",
    "plt.xlabel('Function Evaluations')\n",
    "plt.ylabel('Fitness (lower is better)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n",
    "\n",
    "Let's perform statistical tests to determine if the differences between algorithms are significant. We'll follow a structured decision flow to select the appropriate statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def perform_statistical_analysis(results_df):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis on experiment results following a structured decision flow.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pandas.DataFrame): DataFrame containing experiment results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing statistical test results\n",
    "    \"\"\"\n",
    "    # Step 1: Check if we have at least 2 configurations to compare\n",
    "    configurations = results_df['Configuration'].unique()\n",
    "    if len(configurations) < 2:\n",
    "        print(\"Not enough configurations to perform statistical analysis.\")\n",
    "        return {}\n",
    "    \n",
    "    # Step 2: Determine if we have 2 or more configurations\n",
    "    if len(configurations) == 2:\n",
    "        print(\"\\n=== Two-Group Comparison ===\")\n",
    "        return two_group_comparison(results_df, configurations)\n",
    "    else:\n",
    "        print(\"\\n=== Multiple-Group Comparison ===\")\n",
    "        return multiple_group_comparison(results_df, configurations)\n",
    "\n",
    "def two_group_comparison(results_df, configurations):\n",
    "    \"\"\"\n",
    "    Perform statistical comparison between two groups.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pandas.DataFrame): DataFrame containing experiment results\n",
    "        configurations (array): Array of configuration names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing statistical test results\n",
    "    \"\"\"\n",
    "    # Extract data for each configuration\n",
    "    group1 = results_df[results_df['Configuration'] == configurations[0]]['Best Fitness'].values\n",
    "    group2 = results_df[results_df['Configuration'] == configurations[1]]['Best Fitness'].values\n",
    "    \n",
    "    # Step 3: Test for normality using Shapiro-Wilk test\n",
    "    print(\"Testing for normality (Shapiro-Wilk):\")\n",
    "    _, p_value1 = stats.shapiro(group1)\n",
    "    _, p_value2 = stats.shapiro(group2)\n",
    "    print(f\"  {configurations[0]}: p-value = {p_value1:.4f} ({'Normal' if p_value1 > 0.05 else 'Non-normal'})\")\n",
    "    print(f\"  {configurations[1]}: p-value = {p_value2:.4f} ({'Normal' if p_value2 > 0.05 else 'Non-normal'})\")\n",
    "    \n",
    "    # Both groups must be normal to use parametric tests\n",
    "    is_normal = p_value1 > 0.05 and p_value2 > 0.05\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if is_normal:\n",
    "        # Step 4a: Test for equal variances using Levene's test\n",
    "        print(\"\\nTesting for equal variances (Levene's test):\")\n",
    "        _, p_value_var = stats.levene(group1, group2)\n",
    "        equal_var = p_value_var > 0.05\n",
    "        print(f\"  p-value = {p_value_var:.4f} ({'Equal variances' if equal_var else 'Unequal variances'})\")\n",
    "        \n",
    "        # Step 5a: Perform t-test (either with equal or unequal variances)\n",
    "        if equal_var:\n",
    "            print(\"\\nPerforming Independent t-test (equal variances):\")\n",
    "            t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=True)\n",
    "            test_name = \"Independent t-test\"\n",
    "        else:\n",
    "            print(\"\\nPerforming Welch's t-test (unequal variances):\")\n",
    "            t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)\n",
    "            test_name = \"Welch's t-test\"\n",
    "            \n",
    "        print(f\"  {test_name}: t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "        print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "        \n",
    "        # Calculate effect size (Cohen's d)\n",
    "        mean1, mean2 = np.mean(group1), np.mean(group2)\n",
    "        std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        \n",
    "        # Pooled standard deviation\n",
    "        pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "        cohen_d = abs(mean1 - mean2) / pooled_std\n",
    "        \n",
    "        print(f\"  Effect size (Cohen's d): {cohen_d:.4f}\")\n",
    "        \n",
    "        # Interpret Cohen's d\n",
    "        if cohen_d < 0.2:\n",
    "            effect_interpretation = \"Negligible effect\"\n",
    "        elif cohen_d < 0.5:\n",
    "            effect_interpretation = \"Small effect\"\n",
    "        elif cohen_d < 0.8:\n",
    "            effect_interpretation = \"Medium effect\"\n",
    "        else:\n",
    "            effect_interpretation = \"Large effect\"\n",
    "            \n",
    "        print(f\"  Interpretation: {effect_interpretation}\")\n",
    "        \n",
    "        results = {\n",
    "            'test': test_name,\n",
    "            'statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'effect_size': cohen_d,\n",
    "            'effect_interpretation': effect_interpretation\n",
    "        }\n",
    "    else:\n",
    "        # Step 4b: Perform Mann-Whitney U test (non-parametric)\n",
    "        print(\"\\nPerforming Mann-Whitney U test (non-parametric):\")\n",
    "        u_stat, p_value = stats.mannwhitneyu(group1, group2)\n",
    "        print(f\"  Mann-Whitney U: U-statistic = {u_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "        print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "        \n",
    "        # Calculate effect size (r = Z / sqrt(N))\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        n_total = n1 + n2\n",
    "        \n",
    "        # Convert U to Z\n",
    "        mean_u = n1 * n2 / 2\n",
    "        std_u = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)\n",
    "        z = (u_stat - mean_u) / std_u\n",
    "        \n",
    "        # Calculate effect size r\n",
    "        r = abs(z) / np.sqrt(n_total)\n",
    "        \n",
    "        print(f\"  Effect size (r): {r:.4f}\")\n",
    "        \n",
    "        # Interpret r\n",
    "        if r < 0.1:\n",
    "            effect_interpretation = \"Negligible effect\"\n",
    "        elif r < 0.3:\n",
    "            effect_interpretation = \"Small effect\"\n",
    "        elif r < 0.5:\n",
    "            effect_interpretation = \"Medium effect\"\n",
    "        else:\n",
    "            effect_interpretation = \"Large effect\"\n",
    "            \n",
    "        print(f\"  Interpretation: {effect_interpretation}\")\n",
    "        \n",
    "        results = {\n",
    "            'test': 'Mann-Whitney U',\n",
    "            'statistic': u_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'effect_size': r,\n",
    "            'effect_interpretation': effect_interpretation\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def multiple_group_comparison(results_df, configurations):\n",
    "    \"\"\"\n",
    "    Perform statistical comparison between multiple groups.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pandas.DataFrame): DataFrame containing experiment results\n",
    "        configurations (array): Array of configuration names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing statistical test results\n",
    "    \"\"\"\n",
    "    # Extract data for each configuration\n",
    "    groups = [results_df[results_df['Configuration'] == config]['Best Fitness'].values \n",
    "              for config in configurations]\n",
    "    \n",
    "    # Step 3: Test for normality using Shapiro-Wilk test\n",
    "    print(\"Testing for normality (Shapiro-Wilk):\")\n",
    "    normality_results = []\n",
    "    for i, config in enumerate(configurations):\n",
    "        _, p_value = stats.shapiro(groups[i])\n",
    "        is_normal = p_value > 0.05\n",
    "        normality_results.append(is_normal)\n",
    "        print(f\"  {config}: p-value = {p_value:.4f} ({'Normal' if is_normal else 'Non-normal'})\")\n",
    "    \n",
    "    # All groups must be normal to use parametric tests\n",
    "    all_normal = all(normality_results)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if all_normal:\n",
    "        # Step 4a: Perform ANOVA (parametric)\n",
    "        print(\"\\nPerforming One-way ANOVA (parametric):\")\n",
    "        f_stat, p_value = stats.f_oneway(*groups)\n",
    "        print(f\"  ANOVA: F-statistic = {f_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "        print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "        \n",
    "        # Calculate effect size (Eta-squared)\n",
    "        # Flatten all groups into a single array\n",
    "        all_values = np.concatenate(groups)\n",
    "        grand_mean = np.mean(all_values)\n",
    "        \n",
    "        # Calculate sum of squares between groups (SSB)\n",
    "        ssb = sum(len(group) * (np.mean(group) - grand_mean)**2 for group in groups)\n",
    "        \n",
    "        # Calculate sum of squares total (SST)\n",
    "        sst = sum((x - grand_mean)**2 for x in all_values)\n",
    "        \n",
    "        # Calculate Eta-squared\n",
    "        eta_squared = ssb / sst\n",
    "        \n",
    "        print(f\"  Effect size (Eta-squared): {eta_squared:.4f}\")\n",
    "        \n",
    "        # Interpret Eta-squared\n",
    "        if eta_squared < 0.01:\n",
    "            effect_interpretation = \"Negligible effect\"\n",
    "        elif eta_squared < 0.06:\n",
    "            effect_interpretation = \"Small effect\"\n",
    "        elif eta_squared < 0.14:\n",
    "            effect_interpretation = \"Medium effect\"\n",
    "        else:\n",
    "            effect_interpretation = \"Large effect\"\n",
    "            \n",
    "        print(f\"  Interpretation: {effect_interpretation}\")\n",
    "        \n",
    "        results = {\n",
    "            'test': 'One-way ANOVA',\n",
    "            'statistic': f_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'effect_size': eta_squared,\n",
    "            'effect_interpretation': effect_interpretation\n",
    "        }\n",
    "        \n",
    "        # Step 5a: Perform post-hoc tests if ANOVA is significant\n",
    "        if p_value < 0.05:\n",
    "            print(\"\\nPerforming Tukey HSD post-hoc test:\")\n",
    "            from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "            \n",
    "            # Prepare data for Tukey HSD\n",
    "            fitness_values = results_df['Best Fitness'].values\n",
    "            config_labels = results_df['Configuration'].values\n",
    "            \n",
    "            # Perform Tukey HSD test\n",
    "            tukey_results = pairwise_tukeyhsd(fitness_values, config_labels, alpha=0.05)\n",
    "            print(tukey_results)\n",
    "            \n",
    "            # Store significant pairs\n",
    "            significant_pairs = []\n",
    "            for i, row in enumerate(tukey_results.summary().data[1:]):\n",
    "                group1, group2, _, _, _, reject = row\n",
    "                if reject:\n",
    "                    significant_pairs.append((group1, group2))\n",
    "            \n",
    "            results['post_hoc'] = {\n",
    "                'test': 'Tukey HSD',\n",
    "                'significant_pairs': significant_pairs\n",
    "            }\n",
    "    else:\n",
    "        # Step 4b: Perform Kruskal-Wallis test (non-parametric)\n",
    "        print(\"\\nPerforming Kruskal-Wallis test (non-parametric):\")\n",
    "        h_stat, p_value = stats.kruskal(*groups)\n",
    "        print(f\"  Kruskal-Wallis: H-statistic = {h_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "        print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "        \n",
    "        # Calculate effect size (Eta-squared for Kruskal-Wallis)\n",
    "        n_total = sum(len(group) for group in groups)\n",
    "        eta_squared_h = (h_stat - len(groups) + 1) / (n_total - len(groups))\n",
    "        eta_squared_h = max(0, eta_squared_h)  # Ensure non-negative\n",
    "        \n",
    "        print(f\"  Effect size (Eta-squared H): {eta_squared_h:.4f}\")\n",
    "        \n",
    "        # Interpret Eta-squared H (same thresholds as Eta-squared)\n",
    "        if eta_squared_h < 0.01:\n",
    "            effect_interpretation = \"Negligible effect\"\n",
    "        elif eta_squared_h < 0.06:\n",
    "            effect_interpretation = \"Small effect\"\n",
    "        elif eta_squared_h < 0.14:\n",
    "            effect_interpretation = \"Medium effect\"\n",
    "        else:\n",
    "            effect_interpretation = \"Large effect\"\n",
    "            \n",
    "        print(f\"  Interpretation: {effect_interpretation}\")\n",
    "        \n",
    "        results = {\n",
    "            'test': 'Kruskal-Wallis',\n",
    "            'statistic': h_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'effect_size': eta_squared_h,\n",
    "            'effect_interpretation': effect_interpretation\n",
    "        }\n",
    "        \n",
    "        # Step 5b: Perform post-hoc tests if Kruskal-Wallis is significant\n",
    "        if p_value < 0.05:\n",
    "            print(\"\\nPerforming Dunn's test with Bonferroni correction:\")\n",
    "            from scikit_posthocs import posthoc_dunn\n",
    "            \n",
    "            # Prepare data for Dunn's test\n",
    "            dunn_data = {}\n",
    "            for i, config in enumerate(configurations):\n",
    "                dunn_data[config] = groups[i]\n",
    "            \n",
    "            # Perform Dunn's test\n",
    "            dunn_results = posthoc_dunn(dunn_data, p_adjust='bonferroni')\n",
    "            print(dunn_results)\n",
    "            \n",
    "            # Store significant pairs\n",
    "            significant_pairs = []\n",
    "            for i in range(len(configurations)):\n",
    "                for j in range(i+1, len(configurations)):\n",
    "                    if dunn_results.iloc[i, j] < 0.05:\n",
    "                        significant_pairs.append((configurations[i], configurations[j]))\n",
    "            \n",
    "            results['post_hoc'] = {\n",
    "                'test': \"Dunn's test with Bonferroni correction\",\n",
    "                'significant_pairs': significant_pairs\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform statistical analysis\n",
    "statistical_results = perform_statistical_analysis(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualizing Statistical Results\n",
    "\n",
    "Let's create visualizations that incorporate the statistical significance information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_with_significance(results_df, statistical_results):\n",
    "    \"\"\"\n",
    "    Create boxplot with statistical significance annotations.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pandas.DataFrame): DataFrame containing experiment results\n",
    "        statistical_results (dict): Dictionary containing statistical test results\n",
    "    \"\"\"\n",
    "    # Calculate summary statistics for each configuration\n",
    "    summary = results_df.groupby('Configuration')['Best Fitness'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    summary = summary.sort_values('mean')\n",
    "    \n",
    "    # Create boxplot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.boxplot(x='Configuration', y='Best Fitness', data=results_df, order=summary['Configuration'])\n",
    "    \n",
    "    # Add statistical significance annotations if available\n",
    "    if 'significant' in statistical_results and statistical_results['significant']:\n",
    "        if 'post_hoc' in statistical_results:\n",
    "            # Get the ordered configurations\n",
    "            ordered_configs = summary['Configuration'].tolist()\n",
    "            \n",
    "            # Get significant pairs from post-hoc tests\n",
    "            significant_pairs = statistical_results['post_hoc']['significant_pairs']\n",
    "            \n",
    "            # Add significance bars\n",
    "            y_max = results_df['Best Fitness'].max()\n",
    "            y_range = results_df['Best Fitness'].max() - results_df['Best Fitness'].min()\n",
    "            bar_height = y_range * 0.05\n",
    "            \n",
    "            for i, (config1, config2) in enumerate(significant_pairs):\n",
    "                # Get indices in the ordered list\n",
    "                idx1 = ordered_configs.index(config1)\n",
    "                idx2 = ordered_configs.index(config2)\n",
    "                \n",
    "                # Ensure idx1 < idx2\n",
    "                if idx1 > idx2:\n",
    "                    idx1, idx2 = idx2, idx1\n",
    "                    config1, config2 = config2, config1\n",
    "                \n",
    "                # Calculate bar position\n",
    "                y_pos = y_max + bar_height * (i + 1)\n",
    "                \n",
    "                # Draw the bar\n",
    "                plt.plot([idx1, idx2], [y_pos, y_pos], 'k-', linewidth=1.5)\n",
    "                plt.plot([idx1, idx1], [y_pos - bar_height/2, y_pos], 'k-', linewidth=1.5)\n",
    "                plt.plot([idx2, idx2], [y_pos - bar_height/2, y_pos], 'k-', linewidth=1.5)\n",
    "                \n",
    "                # Add asterisk\n",
    "                plt.text((idx1 + idx2) / 2, y_pos + bar_height/4, '*', ha='center', va='center', fontsize=14)\n",
    "        else:\n",
    "            # For two-group comparison, add a single significance indicator\n",
    "            plt.title(f\"Solution Quality Comparison (p = {statistical_results['p_value']:.4f}, {statistical_results['effect_interpretation']})\")\n",
    "    \n",
    "    plt.title('Solution Quality Comparison with Statistical Significance')\n",
    "    plt.xlabel('Configuration')\n",
    "    plt.ylabel('Best Fitness (lower is better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot with significance annotations\n",
    "plot_with_significance(results_df, statistical_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Based on our experiments and analysis, we can draw the following conclusions:\n",
    "\n",
    "1. **Solution Quality**: [To be filled after running experiments]\n",
    "2. **Computational Efficiency**: [To be filled after running experiments]\n",
    "3. **Convergence Behavior**: [To be filled after running experiments]\n",
    "4. **Statistical Significance**: [To be filled after running experiments]\n",
    "\n",
    "### 6.1 Best Algorithm for the Sports League Problem\n",
    "\n",
    "[To be filled after running experiments]\n",
    "\n",
    "### 6.2 Trade-offs and Recommendations\n",
    "\n",
    "[To be filled after running experiments]\n",
    "\n",
    "### 6.3 Future Work\n",
    "\n",
    "1. Explore more advanced hybrid approaches\n",
    "2. Implement adaptive parameter tuning\n",
    "3. Test with larger problem instances\n",
    "4. Develop more specialized operators for the Sports League problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
